./data/pc_pv_20151124.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151125095515_22e28889-4f13-4fd6-813b-bfda3d218497
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298557, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298557/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298557
Hadoop job information for Stage-1: number of mappers: 16; number of reducers: 3
2015-11-25 09:55:58,629 Stage-1 map = 0%,  reduce = 0%
2015-11-25 09:56:09,617 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 169.97 sec
2015-11-25 09:56:11,762 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 322.21 sec
2015-11-25 09:56:12,822 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 388.19 sec
2015-11-25 09:56:13,885 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 391.13 sec
2015-11-25 09:56:15,055 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 426.49 sec
2015-11-25 09:56:16,125 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 436.94 sec
2015-11-25 09:56:17,173 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 446.09 sec
2015-11-25 09:56:18,238 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 464.49 sec
2015-11-25 09:56:19,304 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 469.3 sec
2015-11-25 09:56:20,385 Stage-1 map = 92%,  reduce = 0%, Cumulative CPU 476.64 sec
2015-11-25 09:56:21,449 Stage-1 map = 97%,  reduce = 0%, Cumulative CPU 483.57 sec
2015-11-25 09:56:22,508 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 485.6 sec
2015-11-25 09:56:23,575 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 487.66 sec
2015-11-25 09:56:24,649 Stage-1 map = 100%,  reduce = 60%, Cumulative CPU 491.15 sec
2015-11-25 09:56:25,710 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 491.69 sec
2015-11-25 09:56:26,772 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 496.84 sec
2015-11-25 09:56:28,930 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 500.14 sec
MapReduce Total cumulative CPU time: 8 minutes 20 seconds 140 msec
Ended Job = job_1445566335713_298557
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 16  Reduce: 3   Cumulative CPU: 500.14 sec   HDFS Read: 2100506500 HDFS Write: 118 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 20 seconds 140 msec
OK
Time taken: 74.269 seconds, Fetched: 4 row(s)
./data/mz_pv_20151124.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151125095650_5d86089b-3618-42f4-801f-04a36523dc23
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298573, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298573/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298573
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 2
2015-11-25 09:57:35,161 Stage-1 map = 0%,  reduce = 0%
2015-11-25 09:57:45,871 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 36.25 sec
2015-11-25 09:57:46,917 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 51.45 sec
2015-11-25 09:57:47,971 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 54.69 sec
2015-11-25 09:57:49,024 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 57.82 sec
2015-11-25 09:57:50,089 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 114.17 sec
2015-11-25 09:57:51,145 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 134.55 sec
2015-11-25 09:57:52,197 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 137.41 sec
2015-11-25 09:57:53,245 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 143.51 sec
2015-11-25 09:57:54,301 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 146.68 sec
2015-11-25 09:57:55,355 Stage-1 map = 77%,  reduce = 0%, Cumulative CPU 150.89 sec
2015-11-25 09:57:56,412 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 153.99 sec
2015-11-25 09:57:59,591 Stage-1 map = 90%,  reduce = 0%, Cumulative CPU 162.15 sec
2015-11-25 09:58:00,749 Stage-1 map = 90%,  reduce = 11%, Cumulative CPU 162.46 sec
2015-11-25 09:58:01,839 Stage-1 map = 100%,  reduce = 11%, Cumulative CPU 164.97 sec
2015-11-25 09:58:02,903 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 166.26 sec
2015-11-25 09:58:03,961 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 167.9 sec
2015-11-25 09:58:08,269 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 173.7 sec
2015-11-25 09:58:12,574 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 169.18 sec
MapReduce Total cumulative CPU time: 2 minutes 49 seconds 180 msec
Ended Job = job_1445566335713_298573
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 2   Cumulative CPU: 169.18 sec   HDFS Read: 791640704 HDFS Write: 68 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 49 seconds 180 msec
OK
Time taken: 83.626 seconds, Fetched: 3 row(s)
./data/tv_pv_20151124.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151125095832_302a321c-2878-4bde-9e75-c298492f7c59
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298595, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298595/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298595
Hadoop job information for Stage-1: number of mappers: 9; number of reducers: 2
2015-11-25 09:58:57,457 Stage-1 map = 0%,  reduce = 0%
2015-11-25 09:59:07,165 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 54.88 sec
2015-11-25 09:59:08,231 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 87.01 sec
2015-11-25 09:59:09,290 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 134.52 sec
2015-11-25 09:59:10,346 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 146.47 sec
2015-11-25 09:59:11,409 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 152.0 sec
2015-11-25 09:59:12,467 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 158.39 sec
2015-11-25 09:59:13,519 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 164.86 sec
2015-11-25 09:59:19,844 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 169.2 sec
2015-11-25 09:59:23,031 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 175.6 sec
MapReduce Total cumulative CPU time: 2 minutes 55 seconds 600 msec
Ended Job = job_1445566335713_298595
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 9  Reduce: 2   Cumulative CPU: 175.6 sec   HDFS Read: 440384579 HDFS Write: 83 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 55 seconds 600 msec
OK
Time taken: 52.358 seconds, Fetched: 3 row(s)
./data/mobile_pv_20151124.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151125095940_42b52da9-d0a3-465a-9b14-58c316a21fa6
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 60
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298612, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298612/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298612
Hadoop job information for Stage-1: number of mappers: 244; number of reducers: 60
2015-11-25 10:00:15,130 Stage-1 map = 0%,  reduce = 0%
2015-11-25 10:00:30,039 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 392.08 sec
2015-11-25 10:00:31,099 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 499.33 sec
2015-11-25 10:00:33,233 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 753.37 sec
2015-11-25 10:00:34,290 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 873.07 sec
2015-11-25 10:00:35,348 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 971.49 sec
2015-11-25 10:00:37,459 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 1275.34 sec
2015-11-25 10:00:38,520 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 1479.93 sec
2015-11-25 10:00:39,566 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 1544.23 sec
2015-11-25 10:00:40,625 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 1675.82 sec
2015-11-25 10:00:41,697 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 1777.41 sec
2015-11-25 10:00:42,964 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 1815.67 sec
2015-11-25 10:00:45,095 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 1921.34 sec
2015-11-25 10:00:47,223 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 2014.24 sec
2015-11-25 10:00:48,287 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 2050.55 sec
2015-11-25 10:00:50,417 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 2169.0 sec
2015-11-25 10:00:51,476 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 2228.79 sec
2015-11-25 10:00:52,529 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 2285.86 sec
2015-11-25 10:00:53,580 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 2318.81 sec
2015-11-25 10:00:55,695 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 2374.81 sec
2015-11-25 10:00:56,765 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 2400.34 sec
2015-11-25 10:00:59,920 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 2582.19 sec
2015-11-25 10:01:02,038 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 2687.66 sec
2015-11-25 10:01:04,149 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 2910.07 sec
2015-11-25 10:01:07,307 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 3112.0 sec
2015-11-25 10:01:08,361 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 3130.72 sec
2015-11-25 10:01:10,471 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 3334.98 sec
2015-11-25 10:01:12,583 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 3473.73 sec
2015-11-25 10:01:14,696 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 3534.53 sec
2015-11-25 10:01:16,807 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 3636.26 sec
2015-11-25 10:01:18,922 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 3727.29 sec
2015-11-25 10:01:19,976 Stage-1 map = 35%,  reduce = 1%, Cumulative CPU 3776.33 sec
2015-11-25 10:01:21,044 Stage-1 map = 36%,  reduce = 1%, Cumulative CPU 3809.82 sec
2015-11-25 10:01:22,104 Stage-1 map = 37%,  reduce = 1%, Cumulative CPU 3830.56 sec
2015-11-25 10:01:23,153 Stage-1 map = 38%,  reduce = 1%, Cumulative CPU 3889.81 sec
2015-11-25 10:01:26,322 Stage-1 map = 39%,  reduce = 1%, Cumulative CPU 4033.43 sec
2015-11-25 10:01:27,373 Stage-1 map = 40%,  reduce = 1%, Cumulative CPU 4068.4 sec
2015-11-25 10:01:28,421 Stage-1 map = 41%,  reduce = 1%, Cumulative CPU 4145.63 sec
2015-11-25 10:01:29,472 Stage-1 map = 42%,  reduce = 1%, Cumulative CPU 4181.31 sec
2015-11-25 10:01:30,521 Stage-1 map = 43%,  reduce = 1%, Cumulative CPU 4237.7 sec
2015-11-25 10:01:33,676 Stage-1 map = 44%,  reduce = 1%, Cumulative CPU 4391.43 sec
2015-11-25 10:01:37,884 Stage-1 map = 45%,  reduce = 1%, Cumulative CPU 4534.07 sec
2015-11-25 10:01:38,932 Stage-1 map = 46%,  reduce = 1%, Cumulative CPU 4605.46 sec
2015-11-25 10:01:39,981 Stage-1 map = 47%,  reduce = 1%, Cumulative CPU 4669.23 sec
2015-11-25 10:01:41,026 Stage-1 map = 48%,  reduce = 1%, Cumulative CPU 4732.55 sec
2015-11-25 10:01:42,078 Stage-1 map = 49%,  reduce = 1%, Cumulative CPU 4767.77 sec
2015-11-25 10:01:43,129 Stage-1 map = 50%,  reduce = 1%, Cumulative CPU 4794.78 sec
2015-11-25 10:01:46,307 Stage-1 map = 51%,  reduce = 1%, Cumulative CPU 4934.93 sec
2015-11-25 10:01:48,415 Stage-1 map = 52%,  reduce = 1%, Cumulative CPU 5014.82 sec
2015-11-25 10:01:49,463 Stage-1 map = 53%,  reduce = 1%, Cumulative CPU 5073.62 sec
2015-11-25 10:01:51,563 Stage-1 map = 54%,  reduce = 1%, Cumulative CPU 5096.64 sec
2015-11-25 10:01:54,711 Stage-1 map = 55%,  reduce = 1%, Cumulative CPU 5221.54 sec
2015-11-25 10:01:56,800 Stage-1 map = 55%,  reduce = 2%, Cumulative CPU 5284.93 sec
2015-11-25 10:01:57,850 Stage-1 map = 56%,  reduce = 2%, Cumulative CPU 5330.29 sec
2015-11-25 10:01:59,949 Stage-1 map = 57%,  reduce = 2%, Cumulative CPU 5387.53 sec
2015-11-25 10:02:05,215 Stage-1 map = 59%,  reduce = 2%, Cumulative CPU 5616.2 sec
2015-11-25 10:02:07,311 Stage-1 map = 60%,  reduce = 2%, Cumulative CPU 5699.8 sec
2015-11-25 10:02:08,363 Stage-1 map = 61%,  reduce = 2%, Cumulative CPU 5733.48 sec
2015-11-25 10:02:11,526 Stage-1 map = 62%,  reduce = 2%, Cumulative CPU 5842.52 sec
2015-11-25 10:02:12,579 Stage-1 map = 63%,  reduce = 2%, Cumulative CPU 5867.91 sec
2015-11-25 10:02:14,675 Stage-1 map = 64%,  reduce = 2%, Cumulative CPU 5977.49 sec
2015-11-25 10:02:18,882 Stage-1 map = 65%,  reduce = 2%, Cumulative CPU 6115.52 sec
2015-11-25 10:02:19,937 Stage-1 map = 66%,  reduce = 2%, Cumulative CPU 6165.97 sec
2015-11-25 10:02:23,111 Stage-1 map = 68%,  reduce = 2%, Cumulative CPU 6294.22 sec
2015-11-25 10:02:24,167 Stage-1 map = 69%,  reduce = 2%, Cumulative CPU 6316.21 sec
2015-11-25 10:02:25,231 Stage-1 map = 70%,  reduce = 2%, Cumulative CPU 6334.26 sec
2015-11-25 10:02:28,402 Stage-1 map = 72%,  reduce = 2%, Cumulative CPU 6410.37 sec
2015-11-25 10:02:29,460 Stage-1 map = 73%,  reduce = 2%, Cumulative CPU 6435.63 sec
2015-11-25 10:02:30,516 Stage-1 map = 74%,  reduce = 2%, Cumulative CPU 6458.63 sec
2015-11-25 10:02:33,688 Stage-1 map = 75%,  reduce = 2%, Cumulative CPU 6586.88 sec
2015-11-25 10:02:36,841 Stage-1 map = 76%,  reduce = 2%, Cumulative CPU 6717.76 sec
2015-11-25 10:02:37,895 Stage-1 map = 77%,  reduce = 2%, Cumulative CPU 6810.92 sec
2015-11-25 10:02:38,956 Stage-1 map = 78%,  reduce = 2%, Cumulative CPU 6985.71 sec
2015-11-25 10:02:40,017 Stage-1 map = 79%,  reduce = 2%, Cumulative CPU 7004.5 sec
2015-11-25 10:02:41,084 Stage-1 map = 80%,  reduce = 2%, Cumulative CPU 7083.84 sec
2015-11-25 10:02:42,139 Stage-1 map = 81%,  reduce = 2%, Cumulative CPU 7101.74 sec
2015-11-25 10:02:43,189 Stage-1 map = 83%,  reduce = 2%, Cumulative CPU 7120.16 sec
2015-11-25 10:02:45,282 Stage-1 map = 84%,  reduce = 2%, Cumulative CPU 7162.92 sec
2015-11-25 10:02:47,386 Stage-1 map = 85%,  reduce = 2%, Cumulative CPU 7209.47 sec
2015-11-25 10:02:49,486 Stage-1 map = 86%,  reduce = 2%, Cumulative CPU 7264.96 sec
2015-11-25 10:02:50,758 Stage-1 map = 87%,  reduce = 2%, Cumulative CPU 7318.21 sec
2015-11-25 10:02:51,814 Stage-1 map = 88%,  reduce = 2%, Cumulative CPU 7385.02 sec
2015-11-25 10:02:52,865 Stage-1 map = 89%,  reduce = 2%, Cumulative CPU 7445.44 sec
2015-11-25 10:02:56,040 Stage-1 map = 91%,  reduce = 2%, Cumulative CPU 7668.96 sec
2015-11-25 10:02:57,088 Stage-1 map = 91%,  reduce = 3%, Cumulative CPU 7684.59 sec
2015-11-25 10:02:58,144 Stage-1 map = 93%,  reduce = 3%, Cumulative CPU 7720.3 sec
2015-11-25 10:02:59,196 Stage-1 map = 94%,  reduce = 3%, Cumulative CPU 7776.01 sec
2015-11-25 10:03:00,237 Stage-1 map = 95%,  reduce = 3%, Cumulative CPU 7794.6 sec
2015-11-25 10:03:01,290 Stage-1 map = 96%,  reduce = 3%, Cumulative CPU 7823.26 sec
2015-11-25 10:03:06,518 Stage-1 map = 97%,  reduce = 4%, Cumulative CPU 7930.85 sec
2015-11-25 10:03:08,626 Stage-1 map = 98%,  reduce = 4%, Cumulative CPU 7968.2 sec
2015-11-25 10:03:10,717 Stage-1 map = 99%,  reduce = 5%, Cumulative CPU 7985.82 sec
2015-11-25 10:03:13,865 Stage-1 map = 100%,  reduce = 7%, Cumulative CPU 7995.41 sec
2015-11-25 10:03:14,911 Stage-1 map = 100%,  reduce = 16%, Cumulative CPU 8004.83 sec
2015-11-25 10:03:15,956 Stage-1 map = 100%,  reduce = 18%, Cumulative CPU 8012.05 sec
2015-11-25 10:03:18,050 Stage-1 map = 100%,  reduce = 20%, Cumulative CPU 8013.92 sec
2015-11-25 10:03:19,099 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 8027.5 sec
2015-11-25 10:03:20,148 Stage-1 map = 100%,  reduce = 35%, Cumulative CPU 8031.17 sec
2015-11-25 10:03:22,249 Stage-1 map = 100%,  reduce = 40%, Cumulative CPU 8037.18 sec
2015-11-25 10:03:23,305 Stage-1 map = 100%,  reduce = 48%, Cumulative CPU 8046.59 sec
2015-11-25 10:03:24,355 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 8054.8 sec
2015-11-25 10:03:26,461 Stage-1 map = 100%,  reduce = 60%, Cumulative CPU 8062.11 sec
2015-11-25 10:03:27,510 Stage-1 map = 100%,  reduce = 63%, Cumulative CPU 8066.05 sec
2015-11-25 10:03:28,562 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 8069.66 sec
2015-11-25 10:03:29,615 Stage-1 map = 100%,  reduce = 70%, Cumulative CPU 8074.21 sec
2015-11-25 10:03:30,662 Stage-1 map = 100%,  reduce = 78%, Cumulative CPU 8090.04 sec
2015-11-25 10:03:31,715 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 8095.07 sec
2015-11-25 10:03:32,762 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 8096.85 sec
2015-11-25 10:03:33,820 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 8103.43 sec
2015-11-25 10:03:34,877 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 8109.72 sec
2015-11-25 10:03:35,933 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 8114.08 sec
2015-11-25 10:03:36,989 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 8116.91 sec
2015-11-25 10:03:38,035 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8120.48 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 15 minutes 20 seconds 480 msec
Ended Job = job_1445566335713_298612
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 244  Reduce: 60   Cumulative CPU: 8120.48 sec   HDFS Read: 58465190456 HDFS Write: 534 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 15 minutes 20 seconds 480 msec
OK
Time taken: 238.986 seconds, Fetched: 2 row(s)
2015-11-25 10:03:40 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2015-11-25 10:03:40 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20151124
2015-11-25 10:03:40 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-25 10:03:40 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-25 10:03:40 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2015-11-25 10:03:40 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2015-11-25 10:03:40 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2015-11-25 10:03:40 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2015-11-25 10:03:41 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:0
2015-11-25 10:03:41 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2015-11-25 10:03:41 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:0
2015-11-25 10:03:41 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:964millis
2015-11-25 10:03:41 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2015-11-25 10:03:41 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2015-11-25 10:03:41 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2015-11-25 10:03:41 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2015-11-25 10:03:41 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160309.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310172257_c6e4c9df-6609-4d0d-a7af-31d3e04855c0
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 16
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024396, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024396/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024396
Hadoop job information for Stage-1: number of mappers: 19; number of reducers: 16
2016-03-10 17:23:17,632 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:23:28,278 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 52.88 sec
2016-03-10 17:23:29,637 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 118.3 sec
2016-03-10 17:23:31,760 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 184.22 sec
2016-03-10 17:23:32,829 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 189.33 sec
2016-03-10 17:23:35,197 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 282.69 sec
2016-03-10 17:23:38,640 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 358.43 sec
2016-03-10 17:23:39,743 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 364.49 sec
2016-03-10 17:23:40,799 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 376.64 sec
2016-03-10 17:23:41,856 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 413.69 sec
2016-03-10 17:23:42,918 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 423.95 sec
2016-03-10 17:23:44,965 Stage-1 map = 45%,  reduce = 5%, Cumulative CPU 440.46 sec
2016-03-10 17:23:46,054 Stage-1 map = 64%,  reduce = 5%, Cumulative CPU 479.36 sec
2016-03-10 17:23:47,124 Stage-1 map = 67%,  reduce = 9%, Cumulative CPU 492.67 sec
2016-03-10 17:23:48,749 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 501.09 sec
2016-03-10 17:23:49,817 Stage-1 map = 67%,  reduce = 16%, Cumulative CPU 513.66 sec
2016-03-10 17:23:51,946 Stage-1 map = 71%,  reduce = 18%, Cumulative CPU 539.74 sec
2016-03-10 17:23:53,041 Stage-1 map = 75%,  reduce = 18%, Cumulative CPU 547.91 sec
2016-03-10 17:23:54,557 Stage-1 map = 77%,  reduce = 19%, Cumulative CPU 552.22 sec
2016-03-10 17:23:55,615 Stage-1 map = 85%,  reduce = 21%, Cumulative CPU 569.19 sec
2016-03-10 17:23:56,688 Stage-1 map = 85%,  reduce = 23%, Cumulative CPU 571.11 sec
2016-03-10 17:23:57,758 Stage-1 map = 86%,  reduce = 25%, Cumulative CPU 575.3 sec
2016-03-10 17:23:58,815 Stage-1 map = 88%,  reduce = 26%, Cumulative CPU 583.54 sec
2016-03-10 17:24:00,147 Stage-1 map = 88%,  reduce = 27%, Cumulative CPU 583.95 sec
2016-03-10 17:24:01,404 Stage-1 map = 89%,  reduce = 28%, Cumulative CPU 595.43 sec
2016-03-10 17:24:04,572 Stage-1 map = 90%,  reduce = 28%, Cumulative CPU 603.41 sec
2016-03-10 17:24:18,395 Stage-1 map = 91%,  reduce = 28%, Cumulative CPU 661.6 sec
2016-03-10 17:24:22,623 Stage-1 map = 93%,  reduce = 28%, Cumulative CPU 673.12 sec
2016-03-10 17:24:28,963 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 693.79 sec
2016-03-10 17:24:30,012 Stage-1 map = 98%,  reduce = 29%, Cumulative CPU 696.3 sec
2016-03-10 17:24:31,070 Stage-1 map = 98%,  reduce = 30%, Cumulative CPU 696.75 sec
2016-03-10 17:24:32,126 Stage-1 map = 98%,  reduce = 31%, Cumulative CPU 697.69 sec
2016-03-10 17:24:33,175 Stage-1 map = 98%,  reduce = 32%, Cumulative CPU 699.26 sec
2016-03-10 17:24:35,273 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 695.9 sec
2016-03-10 17:24:36,320 Stage-1 map = 100%,  reduce = 34%, Cumulative CPU 696.43 sec
2016-03-10 17:24:37,373 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 715.44 sec
2016-03-10 17:24:38,419 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 723.51 sec
2016-03-10 17:24:39,459 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 724.54 sec
2016-03-10 17:24:41,563 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 727.22 sec
2016-03-10 17:24:44,699 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 731.23 sec
MapReduce Total cumulative CPU time: 12 minutes 11 seconds 230 msec
Ended Job = job_1453192496319_1024396
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 19  Reduce: 16   Cumulative CPU: 732.17 sec   HDFS Read: 2800771719 HDFS Write: 237 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 12 seconds 170 msec
OK
Time taken: 108.934 seconds, Fetched: 4 row(s)
./data/mz_pv_20160309.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310172458_5331e3f7-c7a8-43e9-af97-346563334d3d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024419, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024419/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024419
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 17:25:15,182 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:25:25,748 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 7.28 sec
2016-03-10 17:25:26,800 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 29.75 sec
2016-03-10 17:25:28,911 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 39.26 sec
2016-03-10 17:25:32,070 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 84.1 sec
2016-03-10 17:25:35,219 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 107.39 sec
2016-03-10 17:25:36,282 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 111.03 sec
2016-03-10 17:25:38,374 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 123.7 sec
2016-03-10 17:25:39,425 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 126.87 sec
2016-03-10 17:25:41,533 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 138.01 sec
2016-03-10 17:25:42,585 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 140.97 sec
2016-03-10 17:25:43,634 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 143.94 sec
2016-03-10 17:25:44,689 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 149.55 sec
2016-03-10 17:25:45,741 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 156.33 sec
2016-03-10 17:25:46,797 Stage-1 map = 84%,  reduce = 0%, Cumulative CPU 160.61 sec
2016-03-10 17:25:47,844 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 164.68 sec
2016-03-10 17:25:48,890 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 166.26 sec
2016-03-10 17:25:52,084 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 168.69 sec
2016-03-10 17:25:53,141 Stage-1 map = 100%,  reduce = 17%, Cumulative CPU 171.03 sec
2016-03-10 17:25:54,200 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 175.73 sec
2016-03-10 17:25:55,264 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 179.14 sec
2016-03-10 17:25:57,372 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 183.64 sec
2016-03-10 17:26:05,758 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 188.76 sec
MapReduce Total cumulative CPU time: 3 minutes 8 seconds 760 msec
Ended Job = job_1453192496319_1024419
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 188.76 sec   HDFS Read: 856996277 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 8 seconds 760 msec
OK
Time taken: 68.631 seconds, Fetched: 3 row(s)
./data/tv_pv_20160309.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310172618_e2922d94-a253-4026-9f20-61aa57d60b69
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024443, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024443/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024443
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 11
2016-03-10 17:26:35,993 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:26:46,593 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 36.02 sec
2016-03-10 17:26:47,650 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 66.05 sec
2016-03-10 17:26:49,757 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 85.82 sec
2016-03-10 17:26:50,810 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 94.38 sec
2016-03-10 17:26:51,862 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 99.93 sec
2016-03-10 17:26:52,921 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 105.82 sec
2016-03-10 17:26:53,972 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 107.37 sec
2016-03-10 17:26:55,023 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 112.24 sec
2016-03-10 17:26:56,077 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 121.66 sec
2016-03-10 17:26:57,136 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 127.02 sec
2016-03-10 17:26:58,192 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 131.97 sec
2016-03-10 17:26:59,248 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 135.6 sec
2016-03-10 17:27:01,352 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 139.75 sec
2016-03-10 17:27:02,406 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 142.5 sec
2016-03-10 17:27:03,463 Stage-1 map = 64%,  reduce = 8%, Cumulative CPU 145.37 sec
2016-03-10 17:27:04,524 Stage-1 map = 66%,  reduce = 12%, Cumulative CPU 150.52 sec
2016-03-10 17:27:05,581 Stage-1 map = 71%,  reduce = 12%, Cumulative CPU 155.03 sec
2016-03-10 17:27:06,630 Stage-1 map = 71%,  reduce = 13%, Cumulative CPU 155.71 sec
2016-03-10 17:27:07,686 Stage-1 map = 73%,  reduce = 14%, Cumulative CPU 161.28 sec
2016-03-10 17:27:08,732 Stage-1 map = 74%,  reduce = 15%, Cumulative CPU 163.36 sec
2016-03-10 17:27:09,788 Stage-1 map = 78%,  reduce = 16%, Cumulative CPU 166.75 sec
2016-03-10 17:27:10,843 Stage-1 map = 83%,  reduce = 18%, Cumulative CPU 169.37 sec
2016-03-10 17:27:11,899 Stage-1 map = 83%,  reduce = 20%, Cumulative CPU 172.06 sec
2016-03-10 17:27:12,953 Stage-1 map = 87%,  reduce = 23%, Cumulative CPU 173.57 sec
2016-03-10 17:27:14,003 Stage-1 map = 87%,  reduce = 24%, Cumulative CPU 174.15 sec
2016-03-10 17:27:15,053 Stage-1 map = 93%,  reduce = 26%, Cumulative CPU 175.77 sec
2016-03-10 17:27:16,102 Stage-1 map = 93%,  reduce = 30%, Cumulative CPU 177.18 sec
2016-03-10 17:27:23,450 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 183.89 sec
2016-03-10 17:27:32,897 Stage-1 map = 100%,  reduce = 37%, Cumulative CPU 196.17 sec
2016-03-10 17:27:33,958 Stage-1 map = 100%,  reduce = 85%, Cumulative CPU 208.99 sec
2016-03-10 17:27:35,010 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 219.52 sec
MapReduce Total cumulative CPU time: 3 minutes 39 seconds 520 msec
Ended Job = job_1453192496319_1024443
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 11   Cumulative CPU: 219.52 sec   HDFS Read: 639513190 HDFS Write: 171 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 39 seconds 520 msec
OK
Time taken: 78.991 seconds, Fetched: 3 row(s)
./data/mobile_pv_20160309.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310172748_a01fe983-eaec-41ca-8d4d-dc847ac8c91d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 598
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024470, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024470/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024470
Hadoop job information for Stage-1: number of mappers: 472; number of reducers: 598
2016-03-10 17:28:15,915 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:28:33,069 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 2150.02 sec
2016-03-10 17:28:34,184 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 2766.62 sec
2016-03-10 17:28:35,497 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 3611.91 sec
2016-03-10 17:28:36,580 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 4671.49 sec
2016-03-10 17:28:37,664 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 5087.26 sec
2016-03-10 17:28:38,737 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 5897.66 sec
2016-03-10 17:28:39,798 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 6585.11 sec
2016-03-10 17:28:40,853 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 7030.3 sec
2016-03-10 17:28:41,915 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 7785.83 sec
2016-03-10 17:28:43,048 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 8434.72 sec
2016-03-10 17:28:44,132 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 9023.58 sec
2016-03-10 17:28:45,188 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 9669.92 sec
2016-03-10 17:28:46,247 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 10059.26 sec
2016-03-10 17:28:47,304 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 10466.11 sec
2016-03-10 17:28:48,359 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 10949.68 sec
2016-03-10 17:28:49,413 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 11235.02 sec
2016-03-10 17:28:50,482 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 11661.45 sec
2016-03-10 17:28:51,540 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 11929.99 sec
2016-03-10 17:28:52,596 Stage-1 map = 76%,  reduce = 5%, Cumulative CPU 12290.9 sec
2016-03-10 17:28:53,650 Stage-1 map = 80%,  reduce = 9%, Cumulative CPU 12659.14 sec
2016-03-10 17:28:54,703 Stage-1 map = 84%,  reduce = 11%, Cumulative CPU 12887.76 sec
2016-03-10 17:28:55,757 Stage-1 map = 87%,  reduce = 13%, Cumulative CPU 13112.56 sec
2016-03-10 17:28:56,814 Stage-1 map = 89%,  reduce = 15%, Cumulative CPU 13321.39 sec
2016-03-10 17:28:57,867 Stage-1 map = 91%,  reduce = 17%, Cumulative CPU 13487.14 sec
2016-03-10 17:28:58,917 Stage-1 map = 91%,  reduce = 18%, Cumulative CPU 13600.07 sec
2016-03-10 17:28:59,972 Stage-1 map = 92%,  reduce = 19%, Cumulative CPU 13727.95 sec
2016-03-10 17:29:01,027 Stage-1 map = 94%,  reduce = 20%, Cumulative CPU 13846.64 sec
2016-03-10 17:29:02,080 Stage-1 map = 96%,  reduce = 21%, Cumulative CPU 13942.01 sec
2016-03-10 17:29:03,130 Stage-1 map = 97%,  reduce = 22%, Cumulative CPU 14033.28 sec
2016-03-10 17:29:04,185 Stage-1 map = 98%,  reduce = 23%, Cumulative CPU 14105.43 sec
2016-03-10 17:29:05,238 Stage-1 map = 98%,  reduce = 24%, Cumulative CPU 14160.51 sec
2016-03-10 17:29:07,339 Stage-1 map = 99%,  reduce = 25%, Cumulative CPU 14246.86 sec
2016-03-10 17:29:09,448 Stage-1 map = 99%,  reduce = 26%, Cumulative CPU 14304.21 sec
2016-03-10 17:29:13,656 Stage-1 map = 99%,  reduce = 27%, Cumulative CPU 14439.64 sec
2016-03-10 17:29:23,144 Stage-1 map = 100%,  reduce = 41%, Cumulative CPU 14875.34 sec
2016-03-10 17:29:24,199 Stage-1 map = 100%,  reduce = 79%, Cumulative CPU 15493.86 sec
2016-03-10 17:29:25,251 Stage-1 map = 100%,  reduce = 81%, Cumulative CPU 15594.78 sec
2016-03-10 17:29:26,305 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 15606.13 sec
2016-03-10 17:29:28,416 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 15686.26 sec
2016-03-10 17:29:29,466 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 15941.49 sec
2016-03-10 17:29:32,625 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15973.28 sec
MapReduce Total cumulative CPU time: 0 days 4 hours 26 minutes 13 seconds 280 msec
Ended Job = job_1453192496319_1024470
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 472  Reduce: 598   Cumulative CPU: 15973.28 sec   HDFS Read: 116655644861 HDFS Write: 4841 SUCCESS
Total MapReduce CPU Time Spent: 0 days 4 hours 26 minutes 13 seconds 280 msec
OK
Time taken: 105.369 seconds, Fetched: 2 row(s)
./data/pc_pv_20160201.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310173547_9b10eaaa-5bb4-411d-9b95-bbd1ef28ed61
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024706, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024706/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024706
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 11
2016-03-10 17:36:05,061 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:36:12,446 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 3.72 sec
2016-03-10 17:36:15,602 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 91.33 sec
2016-03-10 17:36:18,785 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 129.57 sec
2016-03-10 17:36:21,955 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 167.62 sec
2016-03-10 17:36:23,016 Stage-1 map = 51%,  reduce = 3%, Cumulative CPU 174.56 sec
2016-03-10 17:36:24,083 Stage-1 map = 56%,  reduce = 3%, Cumulative CPU 176.89 sec
2016-03-10 17:36:25,137 Stage-1 map = 61%,  reduce = 3%, Cumulative CPU 201.5 sec
2016-03-10 17:36:26,182 Stage-1 map = 61%,  reduce = 13%, Cumulative CPU 203.01 sec
2016-03-10 17:36:27,232 Stage-1 map = 64%,  reduce = 13%, Cumulative CPU 205.6 sec
2016-03-10 17:36:28,276 Stage-1 map = 73%,  reduce = 13%, Cumulative CPU 227.47 sec
2016-03-10 17:36:29,345 Stage-1 map = 77%,  reduce = 18%, Cumulative CPU 230.1 sec
2016-03-10 17:36:30,410 Stage-1 map = 78%,  reduce = 18%, Cumulative CPU 233.67 sec
2016-03-10 17:36:31,461 Stage-1 map = 85%,  reduce = 18%, Cumulative CPU 248.22 sec
2016-03-10 17:36:32,520 Stage-1 map = 85%,  reduce = 23%, Cumulative CPU 249.22 sec
2016-03-10 17:36:33,574 Stage-1 map = 90%,  reduce = 23%, Cumulative CPU 258.48 sec
2016-03-10 17:36:34,629 Stage-1 map = 92%,  reduce = 26%, Cumulative CPU 262.68 sec
2016-03-10 17:36:35,684 Stage-1 map = 96%,  reduce = 28%, Cumulative CPU 265.21 sec
2016-03-10 17:36:36,733 Stage-1 map = 97%,  reduce = 28%, Cumulative CPU 265.21 sec
2016-03-10 17:36:37,783 Stage-1 map = 97%,  reduce = 30%, Cumulative CPU 268.62 sec
2016-03-10 17:36:38,832 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 268.8 sec
2016-03-10 17:36:39,887 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 271.85 sec
2016-03-10 17:36:40,944 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 292.35 sec
2016-03-10 17:36:41,997 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 294.49 sec
MapReduce Total cumulative CPU time: 4 minutes 54 seconds 490 msec
Ended Job = job_1453192496319_1024706
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 13  Reduce: 11   Cumulative CPU: 294.49 sec   HDFS Read: 1856033252 HDFS Write: 196 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 54 seconds 490 msec
OK
Time taken: 55.399 seconds, Fetched: 4 row(s)
./data/mz_pv_20160201.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310173654_34218b97-ff96-478c-be3f-2007d8cb251c
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024747, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024747/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024747
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 6
2016-03-10 17:37:10,876 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:37:21,441 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 43.67 sec
2016-03-10 17:37:24,623 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 69.07 sec
2016-03-10 17:37:26,728 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 73.98 sec
2016-03-10 17:37:27,790 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 89.49 sec
2016-03-10 17:37:29,896 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 94.73 sec
2016-03-10 17:37:30,943 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 104.21 sec
2016-03-10 17:37:33,048 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 109.74 sec
2016-03-10 17:37:34,096 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 113.06 sec
2016-03-10 17:37:36,220 Stage-1 map = 93%,  reduce = 20%, Cumulative CPU 120.27 sec
2016-03-10 17:37:37,279 Stage-1 map = 93%,  reduce = 24%, Cumulative CPU 120.61 sec
2016-03-10 17:37:39,384 Stage-1 map = 94%,  reduce = 28%, Cumulative CPU 124.22 sec
2016-03-10 17:37:40,442 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 125.76 sec
2016-03-10 17:37:41,496 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 132.99 sec
2016-03-10 17:37:42,545 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 138.95 sec
MapReduce Total cumulative CPU time: 2 minutes 18 seconds 950 msec
Ended Job = job_1453192496319_1024747
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 6   Cumulative CPU: 138.95 sec   HDFS Read: 896584083 HDFS Write: 126 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 18 seconds 950 msec
OK
Time taken: 49.215 seconds, Fetched: 3 row(s)
./data/tv_pv_20160201.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310173755_e937d9a5-bde3-420b-9d52-52f840da509c
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 12
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024772, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024772/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024772
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 12
2016-03-10 17:38:10,924 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:38:20,414 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 100.86 sec
2016-03-10 17:38:21,464 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 103.78 sec
2016-03-10 17:38:22,515 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 107.51 sec
2016-03-10 17:38:23,570 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 136.18 sec
2016-03-10 17:38:24,628 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 140.52 sec
2016-03-10 17:38:25,681 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 144.68 sec
2016-03-10 17:38:26,729 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 153.4 sec
2016-03-10 17:38:27,780 Stage-1 map = 96%,  reduce = 0%, Cumulative CPU 154.62 sec
2016-03-10 17:38:28,834 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 156.63 sec
2016-03-10 17:38:29,889 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 176.97 sec
2016-03-10 17:38:30,944 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 180.78 sec
2016-03-10 17:38:31,997 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 182.73 sec
MapReduce Total cumulative CPU time: 3 minutes 2 seconds 730 msec
Ended Job = job_1453192496319_1024772
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 14  Reduce: 12   Cumulative CPU: 182.73 sec   HDFS Read: 768261778 HDFS Write: 177 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 2 seconds 730 msec
OK
Time taken: 38.166 seconds, Fetched: 3 row(s)
./data/mobile_pv_20160201.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310173844_d054a407-2efd-44ae-b0ce-d76aa0436460
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 659
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024793, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024793/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024793
Hadoop job information for Stage-1: number of mappers: 501; number of reducers: 659
2016-03-10 17:39:00,828 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:39:14,824 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 1387.49 sec
2016-03-10 17:39:15,888 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 3754.81 sec
2016-03-10 17:39:16,958 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 3876.92 sec
2016-03-10 17:39:18,013 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 5039.59 sec
2016-03-10 17:39:19,064 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 5682.59 sec
2016-03-10 17:39:20,113 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 5814.84 sec
2016-03-10 17:39:21,162 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 6901.99 sec
2016-03-10 17:39:22,215 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 7520.07 sec
2016-03-10 17:39:23,267 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 7696.16 sec
2016-03-10 17:39:24,419 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 7984.1 sec
2016-03-10 17:39:25,479 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 8833.27 sec
2016-03-10 17:39:26,530 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 8970.07 sec
2016-03-10 17:39:27,581 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 9524.31 sec
./data/pc_pv_20160202.log
2016-03-10 17:39:28,625 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 9624.6 sec
2016-03-10 17:39:29,667 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 9785.85 sec
2016-03-10 17:39:30,711 Stage-1 map = 90%,  reduce = 0%, Cumulative CPU 10062.97 sec
2016-03-10 17:39:31,760 Stage-1 map = 92%,  reduce = 0%, Cumulative CPU 10165.16 sec

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
2016-03-10 17:39:32,820 Stage-1 map = 95%,  reduce = 14%, Cumulative CPU 10653.27 sec
2016-03-10 17:39:33,894 Stage-1 map = 96%,  reduce = 16%, Cumulative CPU 10758.34 sec
2016-03-10 17:39:34,939 Stage-1 map = 97%,  reduce = 17%, Cumulative CPU 10858.61 sec
2016-03-10 17:39:35,993 Stage-1 map = 98%,  reduce = 19%, Cumulative CPU 10999.23 sec
2016-03-10 17:39:37,049 Stage-1 map = 98%,  reduce = 20%, Cumulative CPU 11036.87 sec
2016-03-10 17:39:38,111 Stage-1 map = 98%,  reduce = 21%, Cumulative CPU 11086.21 sec
2016-03-10 17:39:39,159 Stage-1 map = 99%,  reduce = 22%, Cumulative CPU 11179.36 sec
2016-03-10 17:39:40,205 Stage-1 map = 99%,  reduce = 23%, Cumulative CPU 11207.29 sec
2016-03-10 17:39:42,307 Stage-1 map = 99%,  reduce = 24%, Cumulative CPU 11279.58 sec
Query ID = zhaochunlong_20160310173939_cda0b0e9-289b-48c8-8707-86e024e51e67
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024811, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024811/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024811
2016-03-10 17:40:08,027 Stage-1 map = 99%,  reduce = 25%, Cumulative CPU 11828.38 sec
2016-03-10 17:40:14,304 Stage-1 map = 100%,  reduce = 25%, Cumulative CPU 11954.83 sec
2016-03-10 17:40:15,357 Stage-1 map = 100%,  reduce = 61%, Cumulative CPU 12469.55 sec
2016-03-10 17:40:16,404 Stage-1 map = 100%,  reduce = 73%, Cumulative CPU 12678.47 sec
2016-03-10 17:40:17,458 Stage-1 map = 100%,  reduce = 74%, Cumulative CPU 12695.52 sec
2016-03-10 17:40:18,537 Stage-1 map = 100%,  reduce = 76%, Cumulative CPU 12761.34 sec
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 11
2016-03-10 17:40:19,462 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:40:19,609 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 13133.54 sec
2016-03-10 17:40:20,672 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 13201.67 sec
2016-03-10 17:40:26,961 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13230.94 sec
MapReduce Total cumulative CPU time: 0 days 3 hours 40 minutes 30 seconds 940 msec
Ended Job = job_1453192496319_1024793
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 501  Reduce: 659   Cumulative CPU: 13230.94 sec   HDFS Read: 129269500078 HDFS Write: 5329 SUCCESS
Total MapReduce CPU Time Spent: 0 days 3 hours 40 minutes 30 seconds 940 msec
OK
2016-03-10 17:40:30,031 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 77.48 sec
Time taken: 103.631 seconds, Fetched: 2 row(s)
2016-03-10 17:40:31 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:40:31 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160201
2016-03-10 17:40:31 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:40:31 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:40:32 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:40:32 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:40:32 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:40:32 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:40:33,206 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 108.75 sec
2016-03-10 17:40:33 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:40:33 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:40:33 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:40:33 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1248millis
2016-03-10 17:40:33 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:40:33 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:40:33 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:40:33 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:40:33 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:40:36,371 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 139.67 sec
2016-03-10 17:40:37,421 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 142.89 sec
2016-03-10 17:40:38,470 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 145.95 sec
2016-03-10 17:40:39,545 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 169.56 sec
2016-03-10 17:40:41,641 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 175.35 sec
2016-03-10 17:40:42,688 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 190.12 sec
2016-03-10 17:40:44,791 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 202.88 sec
2016-03-10 17:40:45,835 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 212.2 sec
2016-03-10 17:40:46,877 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 215.73 sec
2016-03-10 17:40:47,940 Stage-1 map = 91%,  reduce = 19%, Cumulative CPU 227.31 sec
2016-03-10 17:40:48,989 Stage-1 map = 100%,  reduce = 19%, Cumulative CPU 233.83 sec
2016-03-10 17:40:51,087 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 252.36 sec
2016-03-10 17:40:52,133 Stage-1 map = 100%,  reduce = 87%, Cumulative CPU 253.96 sec
2016-03-10 17:40:53,185 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 257.23 sec
2016-03-10 17:40:57,397 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 260.03 sec
MapReduce Total cumulative CPU time: 4 minutes 20 seconds 30 msec
Ended Job = job_1453192496319_1024811
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 11  Reduce: 11   Cumulative CPU: 260.03 sec   HDFS Read: 1764983379 HDFS Write: 198 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 20 seconds 30 msec
OK
Time taken: 79.52 seconds, Fetched: 4 row(s)
./data/mz_pv_20160202.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174109_12aff746-2147-4e1e-a3dc-0a635b8f7993
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024833, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024833/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024833
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 6
2016-03-10 17:41:25,953 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:41:35,458 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 28.63 sec
2016-03-10 17:41:38,616 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 39.46 sec
2016-03-10 17:41:40,720 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 45.84 sec
2016-03-10 17:41:41,764 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 57.69 sec
2016-03-10 17:41:43,866 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 58.13 sec
2016-03-10 17:41:44,913 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 68.7 sec
2016-03-10 17:41:45,961 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 70.31 sec
2016-03-10 17:41:48,083 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 79.88 sec
2016-03-10 17:41:49,130 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 83.7 sec
2016-03-10 17:41:50,179 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 86.84 sec
2016-03-10 17:41:51,239 Stage-1 map = 77%,  reduce = 0%, Cumulative CPU 89.68 sec
2016-03-10 17:41:52,291 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 94.33 sec
2016-03-10 17:41:55,473 Stage-1 map = 88%,  reduce = 0%, Cumulative CPU 97.42 sec
2016-03-10 17:41:56,528 Stage-1 map = 88%,  reduce = 27%, Cumulative CPU 99.35 sec
2016-03-10 17:41:58,627 Stage-1 map = 90%,  reduce = 27%, Cumulative CPU 102.3 sec
2016-03-10 17:42:01,780 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 106.11 sec
2016-03-10 17:42:02,829 Stage-1 map = 100%,  reduce = 63%, Cumulative CPU 109.75 sec
2016-03-10 17:42:03,878 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 118.75 sec
MapReduce Total cumulative CPU time: 1 minutes 58 seconds 750 msec
Ended Job = job_1453192496319_1024833
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 5  Reduce: 6   Cumulative CPU: 118.75 sec   HDFS Read: 842267923 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 58 seconds 750 msec
OK
Time taken: 55.28 seconds, Fetched: 3 row(s)
2016-03-10 17:42:05 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:42:05 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160202
2016-03-10 17:42:05 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:42:05 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:42:06 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:42:06 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:42:06 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:42:06 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:42:07 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:42:07 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:42:07 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:42:07 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1219millis
2016-03-10 17:42:07 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:42:07 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:42:07 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:42:07 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:42:07 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160203.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174218_05d76c8f-273e-4574-a0a5-9d3327982cae
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024852, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024852/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024852
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 10
2016-03-10 17:42:34,975 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:42:45,532 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 65.12 sec
2016-03-10 17:42:48,696 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 102.71 sec
2016-03-10 17:42:50,802 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 105.76 sec
2016-03-10 17:42:51,855 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 129.89 sec
2016-03-10 17:42:53,956 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 148.87 sec
2016-03-10 17:42:55,010 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 172.3 sec
2016-03-10 17:42:56,066 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 173.77 sec
2016-03-10 17:42:57,115 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 195.46 sec
2016-03-10 17:42:58,174 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 198.46 sec
2016-03-10 17:43:00,274 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 214.48 sec
2016-03-10 17:43:01,338 Stage-1 map = 78%,  reduce = 12%, Cumulative CPU 218.23 sec
2016-03-10 17:43:02,397 Stage-1 map = 87%,  reduce = 17%, Cumulative CPU 224.43 sec
2016-03-10 17:43:03,443 Stage-1 map = 93%,  reduce = 17%, Cumulative CPU 230.86 sec
2016-03-10 17:43:04,489 Stage-1 map = 93%,  reduce = 27%, Cumulative CPU 232.42 sec
2016-03-10 17:43:06,596 Stage-1 map = 94%,  reduce = 27%, Cumulative CPU 235.47 sec
2016-03-10 17:43:07,649 Stage-1 map = 94%,  reduce = 30%, Cumulative CPU 236.31 sec
2016-03-10 17:43:08,691 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 239.26 sec
2016-03-10 17:43:11,856 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 242.89 sec
2016-03-10 17:43:16,072 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 248.01 sec
2016-03-10 17:43:17,126 Stage-1 map = 100%,  reduce = 66%, Cumulative CPU 252.57 sec
2016-03-10 17:43:18,181 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 266.15 sec
2016-03-10 17:43:19,249 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 269.42 sec
MapReduce Total cumulative CPU time: 4 minutes 29 seconds 420 msec
Ended Job = job_1453192496319_1024852
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 10   Cumulative CPU: 269.42 sec   HDFS Read: 1730850776 HDFS Write: 183 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 29 seconds 420 msec
OK
Time taken: 61.898 seconds, Fetched: 4 row(s)
./data/mz_pv_20160203.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174331_f2803e8d-c976-4b1a-9cad-7ae15539cf3b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024874, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024874/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024874
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 17:43:51,698 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:44:00,505 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 6.3 sec
2016-03-10 17:44:02,625 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 34.14 sec
2016-03-10 17:44:04,752 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 38.47 sec
2016-03-10 17:44:05,827 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 47.76 sec
2016-03-10 17:44:07,946 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 58.63 sec
2016-03-10 17:44:11,122 Stage-1 map = 37%,  reduce = 1%, Cumulative CPU 68.34 sec
2016-03-10 17:44:12,184 Stage-1 map = 38%,  reduce = 6%, Cumulative CPU 70.6 sec
2016-03-10 17:44:14,307 Stage-1 map = 43%,  reduce = 6%, Cumulative CPU 79.39 sec
2016-03-10 17:44:15,360 Stage-1 map = 44%,  reduce = 6%, Cumulative CPU 82.05 sec
2016-03-10 17:44:17,477 Stage-1 map = 50%,  reduce = 6%, Cumulative CPU 91.7 sec
2016-03-10 17:44:18,535 Stage-1 map = 51%,  reduce = 6%, Cumulative CPU 95.08 sec
2016-03-10 17:44:19,589 Stage-1 map = 66%,  reduce = 6%, Cumulative CPU 100.88 sec
2016-03-10 17:44:20,644 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 104.13 sec
2016-03-10 17:44:21,700 Stage-1 map = 68%,  reduce = 17%, Cumulative CPU 107.67 sec
2016-03-10 17:44:22,761 Stage-1 map = 75%,  reduce = 17%, Cumulative CPU 110.4 sec
2016-03-10 17:44:23,829 Stage-1 map = 77%,  reduce = 20%, Cumulative CPU 113.75 sec
2016-03-10 17:44:24,876 Stage-1 map = 77%,  reduce = 22%, Cumulative CPU 113.99 sec
2016-03-10 17:44:26,981 Stage-1 map = 78%,  reduce = 22%, Cumulative CPU 117.59 sec
2016-03-10 17:44:30,146 Stage-1 map = 85%,  reduce = 22%, Cumulative CPU 121.65 sec
2016-03-10 17:44:33,294 Stage-1 map = 85%,  reduce = 28%, Cumulative CPU 122.79 sec
2016-03-10 17:44:44,862 Stage-1 map = 87%,  reduce = 28%, Cumulative CPU 128.6 sec
2016-03-10 17:44:48,013 Stage-1 map = 88%,  reduce = 28%, Cumulative CPU 132.38 sec
2016-03-10 17:44:51,185 Stage-1 map = 89%,  reduce = 28%, Cumulative CPU 136.26 sec
2016-03-10 17:44:54,333 Stage-1 map = 91%,  reduce = 28%, Cumulative CPU 139.38 sec
2016-03-10 17:44:57,469 Stage-1 map = 93%,  reduce = 28%, Cumulative CPU 142.18 sec
2016-03-10 17:44:59,559 Stage-1 map = 100%,  reduce = 28%, Cumulative CPU 145.18 sec
2016-03-10 17:45:00,598 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 148.48 sec
2016-03-10 17:45:01,654 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 154.02 sec
2016-03-10 17:45:02,706 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 156.64 sec
MapReduce Total cumulative CPU time: 2 minutes 36 seconds 640 msec
Ended Job = job_1453192496319_1024874
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 156.64 sec   HDFS Read: 927299483 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 36 seconds 640 msec
OK
Time taken: 91.956 seconds, Fetched: 3 row(s)
2016-03-10 17:45:04 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:45:04 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160203
2016-03-10 17:45:04 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:45:04 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:45:04 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:45:05 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:45:05 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:45:05 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:45:06 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:45:06 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:45:06 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:45:06 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1065millis
2016-03-10 17:45:06 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:45:06 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:45:06 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:45:06 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:45:06 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160204.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174517_a0e3ca06-f820-4941-b596-77f513082e76
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024900, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024900/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024900
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 11
2016-03-10 17:45:34,010 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:45:44,563 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 43.0 sec
2016-03-10 17:45:45,621 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 78.78 sec
2016-03-10 17:45:47,736 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 100.95 sec
2016-03-10 17:45:48,789 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 154.0 sec
2016-03-10 17:45:50,885 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 176.52 sec
2016-03-10 17:45:51,936 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 184.24 sec
2016-03-10 17:45:54,032 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 204.73 sec
2016-03-10 17:45:55,081 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 217.87 sec
2016-03-10 17:45:57,184 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 239.71 sec
2016-03-10 17:45:58,237 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 246.15 sec
2016-03-10 17:46:00,353 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 262.21 sec
2016-03-10 17:46:01,396 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 271.56 sec
2016-03-10 17:46:02,443 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 274.63 sec
2016-03-10 17:46:03,503 Stage-1 map = 69%,  reduce = 5%, Cumulative CPU 298.56 sec
2016-03-10 17:46:04,563 Stage-1 map = 74%,  reduce = 6%, Cumulative CPU 309.23 sec
2016-03-10 17:46:05,616 Stage-1 map = 74%,  reduce = 10%, Cumulative CPU 309.96 sec
2016-03-10 17:46:06,680 Stage-1 map = 75%,  reduce = 17%, Cumulative CPU 316.58 sec
2016-03-10 17:46:07,731 Stage-1 map = 79%,  reduce = 18%, Cumulative CPU 334.58 sec
2016-03-10 17:46:08,794 Stage-1 map = 79%,  reduce = 19%, Cumulative CPU 350.14 sec
2016-03-10 17:46:09,858 Stage-1 map = 84%,  reduce = 20%, Cumulative CPU 353.62 sec
2016-03-10 17:46:10,918 Stage-1 map = 85%,  reduce = 21%, Cumulative CPU 360.06 sec
2016-03-10 17:46:11,974 Stage-1 map = 90%,  reduce = 24%, Cumulative CPU 367.22 sec
2016-03-10 17:46:13,026 Stage-1 map = 90%,  reduce = 25%, Cumulative CPU 367.57 sec
2016-03-10 17:46:14,075 Stage-1 map = 90%,  reduce = 26%, Cumulative CPU 370.91 sec
2016-03-10 17:46:15,124 Stage-1 map = 90%,  reduce = 27%, Cumulative CPU 376.63 sec
2016-03-10 17:46:17,235 Stage-1 map = 91%,  reduce = 27%, Cumulative CPU 379.87 sec
2016-03-10 17:46:18,285 Stage-1 map = 96%,  reduce = 27%, Cumulative CPU 385.64 sec
2016-03-10 17:46:19,337 Stage-1 map = 96%,  reduce = 28%, Cumulative CPU 386.05 sec
2016-03-10 17:46:20,386 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 386.38 sec
2016-03-10 17:46:21,443 Stage-1 map = 97%,  reduce = 30%, Cumulative CPU 389.74 sec
2016-03-10 17:46:22,494 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 391.57 sec
2016-03-10 17:46:23,551 Stage-1 map = 100%,  reduce = 59%, Cumulative CPU 416.3 sec
2016-03-10 17:46:24,602 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 439.99 sec
2016-03-10 17:46:25,652 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 440.83 sec
2016-03-10 17:46:26,710 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 446.9 sec
MapReduce Total cumulative CPU time: 7 minutes 26 seconds 900 msec
Ended Job = job_1453192496319_1024900
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 11  Reduce: 11   Cumulative CPU: 446.9 sec   HDFS Read: 1745658502 HDFS Write: 196 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 26 seconds 900 msec
OK
Time taken: 70.733 seconds, Fetched: 4 row(s)
./data/mz_pv_20160204.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174639_5b6b3df7-4ef1-4b6b-8500-f711aa361769
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024912, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024912/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024912
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 7
2016-03-10 17:47:04,674 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:47:17,361 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 29.34 sec
2016-03-10 17:47:18,422 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 42.02 sec
2016-03-10 17:47:19,476 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 60.21 sec
2016-03-10 17:47:20,523 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 63.3 sec
2016-03-10 17:47:21,574 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 97.31 sec
2016-03-10 17:47:22,622 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 105.57 sec
2016-03-10 17:47:23,684 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 108.55 sec
2016-03-10 17:47:24,737 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 135.77 sec
2016-03-10 17:47:25,792 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 138.92 sec
2016-03-10 17:47:26,844 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 143.75 sec
2016-03-10 17:47:27,900 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 150.45 sec
2016-03-10 17:47:30,009 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 155.48 sec
2016-03-10 17:47:31,062 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 164.15 sec
2016-03-10 17:47:32,124 Stage-1 map = 44%,  reduce = 3%, Cumulative CPU 165.38 sec
2016-03-10 17:47:33,183 Stage-1 map = 47%,  reduce = 4%, Cumulative CPU 171.68 sec
2016-03-10 17:47:34,242 Stage-1 map = 51%,  reduce = 4%, Cumulative CPU 180.05 sec
2016-03-10 17:47:35,302 Stage-1 map = 51%,  reduce = 5%, Cumulative CPU 181.25 sec
2016-03-10 17:47:36,355 Stage-1 map = 62%,  reduce = 5%, Cumulative CPU 190.4 sec
2016-03-10 17:47:37,409 Stage-1 map = 64%,  reduce = 6%, Cumulative CPU 200.49 sec
2016-03-10 17:47:38,461 Stage-1 map = 64%,  reduce = 11%, Cumulative CPU 201.21 sec
2016-03-10 17:47:39,522 Stage-1 map = 73%,  reduce = 11%, Cumulative CPU 207.46 sec
2016-03-10 17:47:40,576 Stage-1 map = 76%,  reduce = 12%, Cumulative CPU 213.85 sec
2016-03-10 17:47:41,638 Stage-1 map = 76%,  reduce = 17%, Cumulative CPU 214.54 sec
2016-03-10 17:47:42,692 Stage-1 map = 85%,  reduce = 17%, Cumulative CPU 220.1 sec
2016-03-10 17:47:44,799 Stage-1 map = 85%,  reduce = 22%, Cumulative CPU 223.96 sec
2016-03-10 17:47:45,853 Stage-1 map = 92%,  reduce = 22%, Cumulative CPU 227.6 sec
2016-03-10 17:47:46,902 Stage-1 map = 94%,  reduce = 24%, Cumulative CPU 230.68 sec
2016-03-10 17:47:47,950 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 233.51 sec
2016-03-10 17:47:49,002 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 240.4 sec
2016-03-10 17:47:50,051 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 245.82 sec
2016-03-10 17:47:51,105 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 247.31 sec
2016-03-10 17:47:52,156 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 247.31 sec
MapReduce Total cumulative CPU time: 4 minutes 7 seconds 310 msec
Ended Job = job_1453192496319_1024912
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 7   Cumulative CPU: 249.16 sec   HDFS Read: 985965143 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 9 seconds 160 msec
OK
Time taken: 74.717 seconds, Fetched: 3 row(s)
2016-03-10 17:47:55 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:47:55 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160204
2016-03-10 17:47:55 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:47:55 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:47:55 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:47:55 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:47:55 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:47:55 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:47:56 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:47:56 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:47:56 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:47:56 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1038millis
2016-03-10 17:47:56 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:47:56 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:47:56 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:47:56 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:47:56 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160205.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174807_9ccc25e9-4a9e-40f2-94ba-5b8a3f866e95
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024930, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024930/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024930
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 10
2016-03-10 17:48:24,586 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:48:35,178 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 69.77 sec
2016-03-10 17:48:38,352 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 107.72 sec
2016-03-10 17:48:41,515 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 138.53 sec
2016-03-10 17:48:42,559 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 140.55 sec
2016-03-10 17:48:43,614 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 145.76 sec
2016-03-10 17:48:44,700 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 167.06 sec
2016-03-10 17:48:47,887 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 186.22 sec
2016-03-10 17:48:51,069 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 202.59 sec
2016-03-10 17:48:53,193 Stage-1 map = 68%,  reduce = 8%, Cumulative CPU 207.18 sec
2016-03-10 17:48:54,249 Stage-1 map = 76%,  reduce = 12%, Cumulative CPU 223.13 sec
2016-03-10 17:48:55,300 Stage-1 map = 84%,  reduce = 14%, Cumulative CPU 228.24 sec
2016-03-10 17:48:56,360 Stage-1 map = 85%,  reduce = 17%, Cumulative CPU 234.81 sec
2016-03-10 17:48:57,421 Stage-1 map = 89%,  reduce = 20%, Cumulative CPU 238.68 sec
2016-03-10 17:48:58,472 Stage-1 map = 89%,  reduce = 21%, Cumulative CPU 238.79 sec
2016-03-10 17:48:59,521 Stage-1 map = 90%,  reduce = 23%, Cumulative CPU 242.84 sec
2016-03-10 17:49:00,578 Stage-1 map = 90%,  reduce = 27%, Cumulative CPU 245.66 sec
2016-03-10 17:49:02,683 Stage-1 map = 91%,  reduce = 27%, Cumulative CPU 249.87 sec
2016-03-10 17:49:05,851 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 256.03 sec
2016-03-10 17:49:06,906 Stage-1 map = 95%,  reduce = 29%, Cumulative CPU 256.15 sec
2016-03-10 17:49:07,962 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 258.06 sec
2016-03-10 17:49:21,596 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 268.66 sec
2016-03-10 17:49:23,696 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 273.63 sec
2016-03-10 17:49:24,749 Stage-1 map = 100%,  reduce = 41%, Cumulative CPU 275.83 sec
2016-03-10 17:49:25,802 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 286.12 sec
2016-03-10 17:49:26,847 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 290.1 sec
2016-03-10 17:49:27,896 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 293.31 sec
MapReduce Total cumulative CPU time: 4 minutes 53 seconds 310 msec
Ended Job = job_1453192496319_1024930
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 11  Reduce: 10   Cumulative CPU: 293.31 sec   HDFS Read: 1675738643 HDFS Write: 187 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 53 seconds 310 msec
OK
Time taken: 81.483 seconds, Fetched: 4 row(s)
./data/mz_pv_20160205.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310174940_a6fab78d-88c9-4d88-9f03-86ec4444f448
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024957, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024957/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024957
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 17:49:59,350 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:50:09,962 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 29.42 sec
2016-03-10 17:50:13,133 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 52.03 sec
2016-03-10 17:50:15,244 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 78.86 sec
2016-03-10 17:50:16,300 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 90.65 sec
2016-03-10 17:50:17,344 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 96.24 sec
2016-03-10 17:50:18,390 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 101.32 sec
2016-03-10 17:50:21,542 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 112.52 sec
2016-03-10 17:50:23,637 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 115.03 sec
2016-03-10 17:50:24,690 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 121.26 sec
2016-03-10 17:50:26,786 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 123.1 sec
2016-03-10 17:50:27,852 Stage-1 map = 59%,  reduce = 4%, Cumulative CPU 131.33 sec
2016-03-10 17:50:29,120 Stage-1 map = 59%,  reduce = 6%, Cumulative CPU 131.66 sec
2016-03-10 17:50:30,184 Stage-1 map = 63%,  reduce = 9%, Cumulative CPU 138.18 sec
2016-03-10 17:50:31,240 Stage-1 map = 72%,  reduce = 10%, Cumulative CPU 144.51 sec
2016-03-10 17:50:32,292 Stage-1 map = 79%,  reduce = 14%, Cumulative CPU 144.98 sec
2016-03-10 17:50:33,344 Stage-1 map = 83%,  reduce = 17%, Cumulative CPU 153.5 sec
2016-03-10 17:50:34,400 Stage-1 map = 83%,  reduce = 19%, Cumulative CPU 154.58 sec
2016-03-10 17:50:35,466 Stage-1 map = 83%,  reduce = 21%, Cumulative CPU 154.81 sec
2016-03-10 17:50:36,535 Stage-1 map = 86%,  reduce = 22%, Cumulative CPU 160.47 sec
2016-03-10 17:50:37,586 Stage-1 map = 93%,  reduce = 22%, Cumulative CPU 162.55 sec
2016-03-10 17:50:38,640 Stage-1 map = 100%,  reduce = 24%, Cumulative CPU 165.74 sec
2016-03-10 17:50:39,695 Stage-1 map = 100%,  reduce = 34%, Cumulative CPU 166.14 sec
2016-03-10 17:50:40,749 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 176.73 sec
2016-03-10 17:50:41,806 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 180.18 sec
MapReduce Total cumulative CPU time: 3 minutes 0 seconds 180 msec
Ended Job = job_1453192496319_1024957
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 180.18 sec   HDFS Read: 949982735 HDFS Write: 126 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 0 seconds 180 msec
OK
Time taken: 62.556 seconds, Fetched: 3 row(s)
2016-03-10 17:50:43 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:50:43 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160205
2016-03-10 17:50:43 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:50:43 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:50:44 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:50:44 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:50:44 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:50:44 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:50:45 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:50:45 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:50:45 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:50:45 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1111millis
2016-03-10 17:50:45 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:50:45 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:50:45 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:50:45 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:50:45 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160206.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310175056_b8b4845b-cd72-4c2f-bf90-b56c939b1063
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024970, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024970/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024970
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 10
2016-03-10 17:51:14,354 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:51:25,124 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 22.71 sec
2016-03-10 17:51:29,630 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 78.5 sec
2016-03-10 17:51:32,812 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 137.89 sec
2016-03-10 17:51:33,962 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 147.79 sec
2016-03-10 17:51:35,021 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 156.58 sec
2016-03-10 17:51:36,091 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 168.54 sec
2016-03-10 17:51:37,338 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 175.0 sec
2016-03-10 17:51:38,392 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 183.76 sec
2016-03-10 17:51:39,845 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 196.5 sec
2016-03-10 17:51:41,963 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 207.91 sec
2016-03-10 17:51:43,098 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 223.23 sec
2016-03-10 17:51:44,161 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 226.28 sec
2016-03-10 17:51:45,235 Stage-1 map = 55%,  reduce = 5%, Cumulative CPU 240.66 sec
2016-03-10 17:51:46,468 Stage-1 map = 58%,  reduce = 5%, Cumulative CPU 249.74 sec
2016-03-10 17:51:47,534 Stage-1 map = 62%,  reduce = 10%, Cumulative CPU 259.26 sec
2016-03-10 17:51:48,604 Stage-1 map = 69%,  reduce = 11%, Cumulative CPU 268.88 sec
2016-03-10 17:51:50,720 Stage-1 map = 69%,  reduce = 16%, Cumulative CPU 272.64 sec
2016-03-10 17:51:51,785 Stage-1 map = 77%,  reduce = 17%, Cumulative CPU 329.02 sec
2016-03-10 17:51:52,853 Stage-1 map = 81%,  reduce = 17%, Cumulative CPU 333.93 sec
2016-03-10 17:51:53,910 Stage-1 map = 82%,  reduce = 23%, Cumulative CPU 339.05 sec
2016-03-10 17:51:54,966 Stage-1 map = 83%,  reduce = 23%, Cumulative CPU 342.2 sec
2016-03-10 17:51:58,122 Stage-1 map = 88%,  reduce = 23%, Cumulative CPU 355.13 sec
2016-03-10 17:52:00,224 Stage-1 map = 88%,  reduce = 26%, Cumulative CPU 358.55 sec
2016-03-10 17:52:01,270 Stage-1 map = 88%,  reduce = 27%, Cumulative CPU 361.66 sec
2016-03-10 17:52:02,321 Stage-1 map = 94%,  reduce = 27%, Cumulative CPU 366.62 sec
2016-03-10 17:52:03,374 Stage-1 map = 94%,  reduce = 28%, Cumulative CPU 367.05 sec
2016-03-10 17:52:04,432 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 367.11 sec
2016-03-10 17:52:06,535 Stage-1 map = 94%,  reduce = 30%, Cumulative CPU 371.13 sec
2016-03-10 17:52:08,620 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 374.7 sec
2016-03-10 17:52:13,868 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 382.42 sec
2016-03-10 17:52:22,271 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 396.78 sec
2016-03-10 17:52:23,319 Stage-1 map = 100%,  reduce = 37%, Cumulative CPU 398.36 sec
2016-03-10 17:52:24,358 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 411.33 sec
2016-03-10 17:52:25,424 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 416.0 sec
2016-03-10 17:52:26,475 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 416.84 sec
MapReduce Total cumulative CPU time: 6 minutes 56 seconds 840 msec
Ended Job = job_1453192496319_1024970
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 10   Cumulative CPU: 416.84 sec   HDFS Read: 1633844785 HDFS Write: 186 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 56 seconds 840 msec
OK
Time taken: 94.366 seconds, Fetched: 4 row(s)
./data/mz_pv_20160206.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310175242_6a8ad583-c693-43f2-a705-b1f1e6d42866
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1024985, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1024985/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1024985
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 6
2016-03-10 17:52:59,828 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:53:10,381 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 21.47 sec
2016-03-10 17:53:11,429 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 62.78 sec
2016-03-10 17:53:13,536 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 73.49 sec
2016-03-10 17:53:14,582 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 77.11 sec
2016-03-10 17:53:16,679 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 93.36 sec
2016-03-10 17:53:17,736 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 96.4 sec
2016-03-10 17:53:18,787 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 103.09 sec
2016-03-10 17:53:19,837 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 115.19 sec
2016-03-10 17:53:20,894 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 118.13 sec
2016-03-10 17:53:21,947 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 121.31 sec
2016-03-10 17:53:22,999 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 130.6 sec
2016-03-10 17:53:24,053 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 132.56 sec
2016-03-10 17:53:25,132 Stage-1 map = 56%,  reduce = 6%, Cumulative CPU 139.72 sec
2016-03-10 17:53:26,187 Stage-1 map = 60%,  reduce = 10%, Cumulative CPU 147.67 sec
2016-03-10 17:53:27,233 Stage-1 map = 66%,  reduce = 10%, Cumulative CPU 149.1 sec
2016-03-10 17:53:28,293 Stage-1 map = 70%,  reduce = 14%, Cumulative CPU 155.59 sec
2016-03-10 17:53:29,348 Stage-1 map = 73%,  reduce = 14%, Cumulative CPU 160.24 sec
2016-03-10 17:53:30,397 Stage-1 map = 86%,  reduce = 14%, Cumulative CPU 164.76 sec
2016-03-10 17:53:31,454 Stage-1 map = 87%,  reduce = 24%, Cumulative CPU 168.99 sec
2016-03-10 17:53:32,503 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 172.4 sec
2016-03-10 17:53:33,553 Stage-1 map = 94%,  reduce = 24%, Cumulative CPU 175.3 sec
2016-03-10 17:53:34,602 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 175.85 sec
2016-03-10 17:53:35,669 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 178.3 sec
2016-03-10 17:53:36,714 Stage-1 map = 100%,  reduce = 76%, Cumulative CPU 185.41 sec
2016-03-10 17:53:37,768 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 191.53 sec
MapReduce Total cumulative CPU time: 3 minutes 11 seconds 530 msec
Ended Job = job_1453192496319_1024985
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 6   Cumulative CPU: 191.53 sec   HDFS Read: 959973842 HDFS Write: 127 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 11 seconds 530 msec
OK
Time taken: 56.553 seconds, Fetched: 3 row(s)
2016-03-10 17:53:39 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:53:39 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160206
2016-03-10 17:53:39 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:53:39 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:53:40 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:53:40 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:53:40 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:53:40 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:53:41 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:53:41 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:53:41 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:53:41 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1078millis
2016-03-10 17:53:41 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:53:41 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:53:41 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:53:41 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:53:41 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160207.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310175352_409e079a-75c4-4fe4-9283-fd9d4195d894
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025004, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025004/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025004
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 10
2016-03-10 17:54:10,763 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:54:21,327 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 57.29 sec
2016-03-10 17:54:24,495 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 95.68 sec
2016-03-10 17:54:26,602 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 105.73 sec
2016-03-10 17:54:27,648 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 130.66 sec
2016-03-10 17:54:29,747 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 137.54 sec
2016-03-10 17:54:30,798 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 161.77 sec
2016-03-10 17:54:31,849 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 163.03 sec
2016-03-10 17:54:32,995 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 171.46 sec
2016-03-10 17:54:34,074 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 187.98 sec
2016-03-10 17:54:36,192 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 191.22 sec
2016-03-10 17:54:37,239 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 209.05 sec
2016-03-10 17:54:39,338 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 221.63 sec
2016-03-10 17:54:40,395 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 226.15 sec
2016-03-10 17:54:41,463 Stage-1 map = 87%,  reduce = 14%, Cumulative CPU 230.04 sec
2016-03-10 17:54:43,576 Stage-1 map = 87%,  reduce = 16%, Cumulative CPU 236.88 sec
2016-03-10 17:54:44,629 Stage-1 map = 87%,  reduce = 24%, Cumulative CPU 240.28 sec
2016-03-10 17:54:45,685 Stage-1 map = 87%,  reduce = 26%, Cumulative CPU 244.81 sec
2016-03-10 17:54:46,746 Stage-1 map = 92%,  reduce = 27%, Cumulative CPU 246.14 sec
2016-03-10 17:54:47,797 Stage-1 map = 92%,  reduce = 29%, Cumulative CPU 247.24 sec
2016-03-10 17:54:49,892 Stage-1 map = 92%,  reduce = 30%, Cumulative CPU 248.73 sec
2016-03-10 17:54:57,388 Stage-1 map = 93%,  reduce = 30%, Cumulative CPU 254.22 sec
2016-03-10 17:55:58,117 Stage-1 map = 93%,  reduce = 30%, Cumulative CPU 295.19 sec
2016-03-10 17:56:02,293 Stage-1 map = 94%,  reduce = 30%, Cumulative CPU 299.43 sec
2016-03-10 17:56:08,742 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 307.46 sec
2016-03-10 17:56:12,919 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 312.96 sec
2016-03-10 17:56:13,964 Stage-1 map = 100%,  reduce = 44%, Cumulative CPU 326.4 sec
2016-03-10 17:56:15,008 Stage-1 map = 100%,  reduce = 79%, Cumulative CPU 352.18 sec
2016-03-10 17:56:16,051 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 383.44 sec
MapReduce Total cumulative CPU time: 6 minutes 23 seconds 440 msec
Ended Job = job_1453192496319_1025004
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 10   Cumulative CPU: 383.44 sec   HDFS Read: 1701477455 HDFS Write: 187 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 23 seconds 440 msec
OK
Time taken: 144.953 seconds, Fetched: 4 row(s)
./data/mz_pv_20160207.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310175628_df0d5923-30e4-4edb-8225-2bd9e438b994
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025030, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025030/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025030
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 17:56:48,379 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:56:57,987 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 18.97 sec
2016-03-10 17:57:00,103 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 26.29 sec
2016-03-10 17:57:01,153 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 50.47 sec
2016-03-10 17:57:03,271 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 60.88 sec
2016-03-10 17:57:04,327 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 82.69 sec
2016-03-10 17:57:06,428 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 88.79 sec
2016-03-10 17:57:07,481 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 105.17 sec
2016-03-10 17:57:09,571 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 110.69 sec
2016-03-10 17:57:10,612 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 122.29 sec
2016-03-10 17:57:12,709 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 132.21 sec
2016-03-10 17:57:13,760 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 141.98 sec
2016-03-10 17:57:15,845 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 151.7 sec
2016-03-10 17:57:16,901 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 172.87 sec
2016-03-10 17:57:18,997 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 174.53 sec
2016-03-10 17:57:20,048 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 183.07 sec
2016-03-10 17:57:22,155 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 187.18 sec
2016-03-10 17:57:24,272 Stage-1 map = 82%,  reduce = 15%, Cumulative CPU 191.09 sec
2016-03-10 17:57:25,327 Stage-1 map = 83%,  reduce = 15%, Cumulative CPU 192.61 sec
2016-03-10 17:57:26,384 Stage-1 map = 83%,  reduce = 22%, Cumulative CPU 197.39 sec
2016-03-10 17:57:29,542 Stage-1 map = 85%,  reduce = 22%, Cumulative CPU 201.83 sec
2016-03-10 17:57:31,646 Stage-1 map = 93%,  reduce = 22%, Cumulative CPU 206.39 sec
2016-03-10 17:57:33,756 Stage-1 map = 93%,  reduce = 27%, Cumulative CPU 207.77 sec
2016-03-10 17:57:35,864 Stage-1 map = 100%,  reduce = 28%, Cumulative CPU 210.58 sec
2016-03-10 17:57:36,915 Stage-1 map = 100%,  reduce = 47%, Cumulative CPU 211.57 sec
2016-03-10 17:57:37,966 Stage-1 map = 100%,  reduce = 64%, Cumulative CPU 216.13 sec
2016-03-10 17:57:39,011 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 228.37 sec
MapReduce Total cumulative CPU time: 3 minutes 48 seconds 370 msec
Ended Job = job_1453192496319_1025030
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 228.37 sec   HDFS Read: 989658867 HDFS Write: 131 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 48 seconds 370 msec
OK
Time taken: 71.407 seconds, Fetched: 3 row(s)
2016-03-10 17:57:41 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 17:57:41 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160207
2016-03-10 17:57:41 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:57:41 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 17:57:41 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 17:57:41 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 17:57:41 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 17:57:41 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 17:57:42 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 17:57:42 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 17:57:42 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 17:57:42 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1072millis
2016-03-10 17:57:42 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 17:57:42 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 17:57:42 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 17:57:42 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 17:57:42 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160208.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310175753_85db42a8-7543-47f4-94fb-92ac879d2d2f
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025044, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025044/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025044
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 10
2016-03-10 17:58:11,154 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:58:21,761 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 35.96 sec
2016-03-10 17:58:22,820 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 58.18 sec
2016-03-10 17:58:24,944 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 90.11 sec
2016-03-10 17:58:25,999 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 99.74 sec
2016-03-10 17:58:27,053 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 111.99 sec
2016-03-10 17:58:28,107 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 127.13 sec
2016-03-10 17:58:29,163 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 139.72 sec
2016-03-10 17:58:30,215 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 145.9 sec
2016-03-10 17:58:32,332 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 166.75 sec
2016-03-10 17:58:33,400 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 169.75 sec
2016-03-10 17:58:34,467 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 183.35 sec
2016-03-10 17:58:35,529 Stage-1 map = 54%,  reduce = 0%, Cumulative CPU 190.31 sec
2016-03-10 17:58:36,590 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 200.0 sec
2016-03-10 17:58:37,635 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 213.52 sec
2016-03-10 17:58:38,679 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 216.49 sec
2016-03-10 17:58:39,725 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 225.76 sec
2016-03-10 17:58:40,786 Stage-1 map = 73%,  reduce = 13%, Cumulative CPU 234.31 sec
2016-03-10 17:58:41,853 Stage-1 map = 77%,  reduce = 15%, Cumulative CPU 239.81 sec
2016-03-10 17:58:42,904 Stage-1 map = 78%,  reduce = 15%, Cumulative CPU 245.41 sec
2016-03-10 17:58:43,954 Stage-1 map = 78%,  reduce = 18%, Cumulative CPU 249.34 sec
2016-03-10 17:58:45,004 Stage-1 map = 83%,  reduce = 20%, Cumulative CPU 255.25 sec
2016-03-10 17:58:46,070 Stage-1 map = 88%,  reduce = 20%, Cumulative CPU 260.07 sec
2016-03-10 17:58:47,147 Stage-1 map = 88%,  reduce = 26%, Cumulative CPU 260.97 sec
2016-03-10 17:58:48,205 Stage-1 map = 93%,  reduce = 27%, Cumulative CPU 267.01 sec
2016-03-10 17:58:50,311 Stage-1 map = 93%,  reduce = 30%, Cumulative CPU 267.76 sec
2016-03-10 17:58:54,527 Stage-1 map = 94%,  reduce = 30%, Cumulative CPU 273.94 sec
2016-03-10 17:59:01,887 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 260.01 sec
2016-03-10 17:59:05,027 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 263.71 sec
2016-03-10 17:59:07,141 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 267.78 sec
2016-03-10 17:59:08,194 Stage-1 map = 100%,  reduce = 56%, Cumulative CPU 273.2 sec
2016-03-10 17:59:09,256 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 288.19 sec
2016-03-10 17:59:11,343 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 293.3 sec
MapReduce Total cumulative CPU time: 4 minutes 53 seconds 300 msec
Ended Job = job_1453192496319_1025044
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 10   Cumulative CPU: 293.3 sec   HDFS Read: 1643130180 HDFS Write: 186 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 53 seconds 300 msec
OK
Time taken: 79.154 seconds, Fetched: 4 row(s)
./data/mz_pv_20160208.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310175923_5a465337-ba31-4b80-9bd0-c1901dd767e2
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025063, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025063/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025063
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 17:59:45,287 Stage-1 map = 0%,  reduce = 0%
2016-03-10 17:59:57,875 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 43.01 sec
2016-03-10 18:00:01,067 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 60.7 sec
2016-03-10 18:00:03,501 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 85.6 sec
2016-03-10 18:00:05,598 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 88.67 sec
2016-03-10 18:00:06,651 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 106.7 sec
2016-03-10 18:00:09,794 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 125.75 sec
2016-03-10 18:00:12,948 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 142.13 sec
2016-03-10 18:00:16,092 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 159.06 sec
2016-03-10 18:00:17,157 Stage-1 map = 73%,  reduce = 9%, Cumulative CPU 161.28 sec
2016-03-10 18:00:18,215 Stage-1 map = 79%,  reduce = 11%, Cumulative CPU 164.35 sec
2016-03-10 18:00:19,271 Stage-1 map = 92%,  reduce = 11%, Cumulative CPU 175.17 sec
2016-03-10 18:00:20,326 Stage-1 map = 92%,  reduce = 27%, Cumulative CPU 176.05 sec
2016-03-10 18:00:22,430 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 179.72 sec
2016-03-10 18:00:25,584 Stage-1 map = 95%,  reduce = 29%, Cumulative CPU 183.59 sec
2016-03-10 18:00:26,650 Stage-1 map = 100%,  reduce = 48%, Cumulative CPU 186.34 sec
2016-03-10 18:00:27,710 Stage-1 map = 100%,  reduce = 66%, Cumulative CPU 192.56 sec
2016-03-10 18:00:28,760 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 206.53 sec
MapReduce Total cumulative CPU time: 3 minutes 26 seconds 530 msec
Ended Job = job_1453192496319_1025063
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 206.53 sec   HDFS Read: 1109716355 HDFS Write: 142 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 26 seconds 530 msec
OK
Time taken: 66.134 seconds, Fetched: 3 row(s)
2016-03-10 18:00:30 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:00:30 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160208
2016-03-10 18:00:30 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:00:30 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:00:30 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:00:31 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:00:31 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:00:31 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:00:32 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:00:32 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:00:32 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:00:32 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1182millis
2016-03-10 18:00:32 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:00:32 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:00:32 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:00:32 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:00:32 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160209.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180043_6fd31f08-0d44-4a3e-86d0-4d5d39ef9dc0
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 9
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025076, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025076/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025076
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 9
2016-03-10 18:01:02,115 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:01:12,928 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 59.48 sec
2016-03-10 18:01:16,081 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 84.38 sec
2016-03-10 18:01:19,232 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 109.17 sec
2016-03-10 18:01:21,319 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 115.04 sec
2016-03-10 18:01:22,378 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 134.42 sec
2016-03-10 18:01:24,472 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 146.38 sec
2016-03-10 18:01:25,524 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 157.75 sec
2016-03-10 18:01:27,631 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 168.81 sec
2016-03-10 18:01:28,686 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 176.66 sec
2016-03-10 18:01:29,733 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 181.41 sec
2016-03-10 18:01:30,782 Stage-1 map = 89%,  reduce = 0%, Cumulative CPU 188.16 sec
2016-03-10 18:01:31,832 Stage-1 map = 94%,  reduce = 0%, Cumulative CPU 192.26 sec
2016-03-10 18:01:32,883 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 194.24 sec
2016-03-10 18:01:33,956 Stage-1 map = 100%,  reduce = 37%, Cumulative CPU 199.09 sec
2016-03-10 18:01:35,012 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 209.93 sec
2016-03-10 18:01:36,060 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 216.48 sec
MapReduce Total cumulative CPU time: 3 minutes 36 seconds 480 msec
Ended Job = job_1453192496319_1025076
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 9   Cumulative CPU: 216.48 sec   HDFS Read: 1454944847 HDFS Write: 169 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 36 seconds 480 msec
OK
Time taken: 53.69 seconds, Fetched: 4 row(s)
./data/mz_pv_20160209.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180148_e1eac18d-48d7-4821-b914-05cb3ea2326d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025084, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025084/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025084
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:02:04,090 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:02:14,653 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 50.8 sec
2016-03-10 18:02:17,810 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 73.06 sec
2016-03-10 18:02:20,962 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 94.54 sec
2016-03-10 18:02:22,009 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 95.95 sec
2016-03-10 18:02:24,169 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 113.91 sec
2016-03-10 18:02:25,220 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 118.22 sec
2016-03-10 18:02:27,323 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 128.25 sec
2016-03-10 18:02:28,375 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 130.4 sec
2016-03-10 18:02:29,434 Stage-1 map = 87%,  reduce = 0%, Cumulative CPU 132.96 sec
2016-03-10 18:02:31,534 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 140.48 sec
2016-03-10 18:02:32,602 Stage-1 map = 100%,  reduce = 60%, Cumulative CPU 149.71 sec
2016-03-10 18:02:33,661 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 156.76 sec
2016-03-10 18:02:34,713 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 159.64 sec
MapReduce Total cumulative CPU time: 2 minutes 39 seconds 640 msec
Ended Job = job_1453192496319_1025084
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 159.64 sec   HDFS Read: 991521542 HDFS Write: 139 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 39 seconds 640 msec
OK
Time taken: 47.435 seconds, Fetched: 3 row(s)
2016-03-10 18:02:36 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:02:36 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160209
2016-03-10 18:02:36 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:02:36 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:02:36 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:02:37 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:02:37 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:02:37 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:02:38 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:02:38 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:02:38 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:02:38 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1054millis
2016-03-10 18:02:38 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:02:38 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:02:38 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:02:38 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:02:38 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160210.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180249_8bd7a486-5b94-4e93-ac0b-f5c18c85c835
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 9
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025093, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025093/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025093
Hadoop job information for Stage-1: number of mappers: 9; number of reducers: 9
2016-03-10 18:03:04,620 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:03:15,173 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 65.63 sec
2016-03-10 18:03:18,341 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 94.13 sec
2016-03-10 18:03:21,502 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 122.73 sec
2016-03-10 18:03:24,661 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 146.86 sec
2016-03-10 18:03:25,710 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 148.74 sec
2016-03-10 18:03:27,804 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 171.6 sec
2016-03-10 18:03:29,910 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 174.41 sec
2016-03-10 18:03:30,959 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 189.9 sec
2016-03-10 18:03:32,020 Stage-1 map = 81%,  reduce = 8%, Cumulative CPU 193.76 sec
2016-03-10 18:03:33,073 Stage-1 map = 81%,  reduce = 15%, Cumulative CPU 195.21 sec
2016-03-10 18:03:34,123 Stage-1 map = 100%,  reduce = 15%, Cumulative CPU 209.82 sec
2016-03-10 18:03:35,168 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 223.69 sec
2016-03-10 18:03:36,213 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 228.81 sec
MapReduce Total cumulative CPU time: 3 minutes 48 seconds 810 msec
Ended Job = job_1453192496319_1025093
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 9  Reduce: 9   Cumulative CPU: 228.81 sec   HDFS Read: 1547800318 HDFS Write: 168 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 48 seconds 810 msec
OK
Time taken: 48.338 seconds, Fetched: 4 row(s)
./data/mz_pv_20160210.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180348_f870995c-48d5-48d4-a4eb-55d3980c41e5
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025110, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025110/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025110
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:04:04,291 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:04:13,768 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 51.52 sec
2016-03-10 18:04:16,914 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 73.89 sec
2016-03-10 18:04:20,077 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 95.78 sec
2016-03-10 18:04:22,168 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 98.67 sec
2016-03-10 18:04:23,222 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 113.53 sec
2016-03-10 18:04:25,327 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 116.24 sec
2016-03-10 18:04:26,382 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 132.48 sec
2016-03-10 18:04:28,484 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 135.58 sec
2016-03-10 18:04:29,543 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 142.63 sec
2016-03-10 18:04:30,594 Stage-1 map = 93%,  reduce = 4%, Cumulative CPU 142.63 sec
2016-03-10 18:04:31,651 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 147.6 sec
2016-03-10 18:04:32,711 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 154.72 sec
2016-03-10 18:04:33,763 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 162.05 sec
2016-03-10 18:04:34,819 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 169.38 sec
MapReduce Total cumulative CPU time: 2 minutes 49 seconds 380 msec
Ended Job = job_1453192496319_1025110
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 169.38 sec   HDFS Read: 1027524094 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 49 seconds 380 msec
OK
Time taken: 48.6 seconds, Fetched: 3 row(s)
2016-03-10 18:04:37 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:04:37 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160210
2016-03-10 18:04:37 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:04:37 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:04:38 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:04:38 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:04:38 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:04:38 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:04:39 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:04:39 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:04:39 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:04:39 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1076millis
2016-03-10 18:04:39 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:04:39 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:04:39 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:04:39 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:04:39 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160211.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180450_e73c72bc-771b-46f0-9958-fc1778e348ba
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025119, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025119/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025119
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 10
2016-03-10 18:05:08,146 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:05:18,688 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 74.29 sec
2016-03-10 18:05:21,858 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 106.02 sec
2016-03-10 18:05:25,009 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 137.4 sec
2016-03-10 18:05:27,113 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 139.79 sec
2016-03-10 18:05:28,162 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 168.74 sec
2016-03-10 18:05:29,238 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 172.51 sec
2016-03-10 18:05:31,350 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 190.56 sec
2016-03-10 18:05:34,529 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 209.49 sec
2016-03-10 18:05:35,583 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 211.62 sec
2016-03-10 18:05:37,689 Stage-1 map = 77%,  reduce = 15%, Cumulative CPU 229.55 sec
2016-03-10 18:05:38,759 Stage-1 map = 82%,  reduce = 15%, Cumulative CPU 231.67 sec
2016-03-10 18:05:39,818 Stage-1 map = 91%,  reduce = 17%, Cumulative CPU 238.25 sec
2016-03-10 18:05:40,877 Stage-1 map = 96%,  reduce = 27%, Cumulative CPU 245.33 sec
2016-03-10 18:05:41,934 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 245.46 sec
2016-03-10 18:05:44,045 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 248.66 sec
2016-03-10 18:05:45,099 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 250.49 sec
2016-03-10 18:05:46,158 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 265.92 sec
2016-03-10 18:05:47,210 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 269.53 sec
2016-03-10 18:05:49,312 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 273.99 sec
2016-03-10 18:05:53,511 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 286.47 sec
MapReduce Total cumulative CPU time: 4 minutes 46 seconds 470 msec
Ended Job = job_1453192496319_1025119
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 10   Cumulative CPU: 286.47 sec   HDFS Read: 1632241720 HDFS Write: 188 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 46 seconds 470 msec
OK
Time taken: 64.381 seconds, Fetched: 4 row(s)
./data/mz_pv_20160211.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180606_babf6656-5817-4299-9a6c-b08a33fbc3bf
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025127, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025127/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025127
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:06:22,589 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:06:33,174 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 51.72 sec
2016-03-10 18:06:36,351 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 73.38 sec
2016-03-10 18:06:39,519 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 95.07 sec
2016-03-10 18:06:42,753 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 114.3 sec
2016-03-10 18:06:45,923 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 129.9 sec
2016-03-10 18:06:46,980 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 131.79 sec
2016-03-10 18:06:48,042 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 134.77 sec
2016-03-10 18:06:49,096 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 154.42 sec
2016-03-10 18:06:50,156 Stage-1 map = 87%,  reduce = 8%, Cumulative CPU 159.51 sec
2016-03-10 18:06:51,205 Stage-1 map = 87%,  reduce = 14%, Cumulative CPU 161.13 sec
2016-03-10 18:06:52,254 Stage-1 map = 94%,  reduce = 14%, Cumulative CPU 167.07 sec
2016-03-10 18:06:53,301 Stage-1 map = 94%,  reduce = 24%, Cumulative CPU 167.55 sec
2016-03-10 18:06:54,348 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 171.86 sec
2016-03-10 18:06:55,408 Stage-1 map = 100%,  reduce = 49%, Cumulative CPU 175.62 sec
2016-03-10 18:06:56,469 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 186.57 sec
2016-03-10 18:06:59,651 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 191.27 sec
2016-03-10 18:07:01,746 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 195.39 sec
MapReduce Total cumulative CPU time: 3 minutes 15 seconds 390 msec
Ended Job = job_1453192496319_1025127
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 195.39 sec   HDFS Read: 1076912043 HDFS Write: 140 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 15 seconds 390 msec
OK
Time taken: 56.898 seconds, Fetched: 3 row(s)
2016-03-10 18:07:03 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:07:03 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160211
2016-03-10 18:07:03 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:07:03 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:07:03 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:07:04 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:07:04 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:07:04 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:07:05 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:07:05 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:07:05 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:07:05 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1141millis
2016-03-10 18:07:05 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:07:05 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:07:05 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:07:05 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:07:05 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160212.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180715_03123c9c-902b-4615-84f8-7441a7488a60
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025136, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025136/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025136
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 10
2016-03-10 18:07:33,008 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:07:43,601 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 59.37 sec
2016-03-10 18:07:46,773 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 129.68 sec
2016-03-10 18:07:47,827 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 133.14 sec
2016-03-10 18:07:48,885 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 139.05 sec
2016-03-10 18:07:49,935 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 163.57 sec
2016-03-10 18:07:50,990 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 168.52 sec
2016-03-10 18:07:52,035 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 177.36 sec
2016-03-10 18:07:53,210 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 196.12 sec
2016-03-10 18:07:54,262 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 200.41 sec
2016-03-10 18:07:55,313 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 206.76 sec
2016-03-10 18:07:56,371 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 223.56 sec
2016-03-10 18:07:58,482 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 232.93 sec
2016-03-10 18:07:59,537 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 245.76 sec
2016-03-10 18:08:01,652 Stage-1 map = 74%,  reduce = 9%, Cumulative CPU 254.04 sec
2016-03-10 18:08:02,713 Stage-1 map = 78%,  reduce = 13%, Cumulative CPU 267.53 sec
2016-03-10 18:08:03,770 Stage-1 map = 83%,  reduce = 13%, Cumulative CPU 272.65 sec
2016-03-10 18:08:04,825 Stage-1 map = 87%,  reduce = 17%, Cumulative CPU 277.08 sec
2016-03-10 18:08:05,874 Stage-1 map = 92%,  reduce = 19%, Cumulative CPU 288.72 sec
2016-03-10 18:08:06,925 Stage-1 map = 96%,  reduce = 22%, Cumulative CPU 292.7 sec
2016-03-10 18:08:07,970 Stage-1 map = 96%,  reduce = 28%, Cumulative CPU 293.48 sec
2016-03-10 18:08:09,029 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 296.69 sec
2016-03-10 18:08:10,083 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 298.53 sec
2016-03-10 18:08:11,134 Stage-1 map = 100%,  reduce = 76%, Cumulative CPU 313.08 sec
2016-03-10 18:08:12,177 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 318.39 sec
2016-03-10 18:08:13,220 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 332.92 sec
2016-03-10 18:08:15,318 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 356.92 sec
2016-03-10 18:08:18,480 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 359.64 sec
MapReduce Total cumulative CPU time: 5 minutes 59 seconds 640 msec
Ended Job = job_1453192496319_1025136
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 11  Reduce: 10   Cumulative CPU: 359.64 sec   HDFS Read: 1731796719 HDFS Write: 190 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 59 seconds 640 msec
OK
Time taken: 63.71 seconds, Fetched: 4 row(s)
./data/mz_pv_20160212.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310180830_49c57faa-1389-498a-bc04-717c08005c1c
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025150, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025150/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025150
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 8
2016-03-10 18:08:49,382 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:08:59,941 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 27.42 sec
2016-03-10 18:09:02,038 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 58.53 sec
2016-03-10 18:09:03,104 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 75.92 sec
2016-03-10 18:09:06,269 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 95.73 sec
2016-03-10 18:09:07,322 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 114.03 sec
2016-03-10 18:09:09,412 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 125.79 sec
2016-03-10 18:09:10,464 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 134.62 sec
2016-03-10 18:09:12,563 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 145.8 sec
2016-03-10 18:09:13,611 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 151.79 sec
2016-03-10 18:09:15,726 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 164.51 sec
2016-03-10 18:09:16,787 Stage-1 map = 60%,  reduce = 2%, Cumulative CPU 167.99 sec
2016-03-10 18:09:17,844 Stage-1 map = 60%,  reduce = 5%, Cumulative CPU 169.01 sec
2016-03-10 18:09:18,895 Stage-1 map = 63%,  reduce = 6%, Cumulative CPU 178.07 sec
2016-03-10 18:09:19,951 Stage-1 map = 65%,  reduce = 6%, Cumulative CPU 185.16 sec
2016-03-10 18:09:21,004 Stage-1 map = 76%,  reduce = 8%, Cumulative CPU 191.24 sec
2016-03-10 18:09:22,061 Stage-1 map = 79%,  reduce = 9%, Cumulative CPU 197.7 sec
2016-03-10 18:09:23,113 Stage-1 map = 84%,  reduce = 14%, Cumulative CPU 200.39 sec
2016-03-10 18:09:24,166 Stage-1 map = 84%,  reduce = 17%, Cumulative CPU 201.4 sec
2016-03-10 18:09:25,227 Stage-1 map = 84%,  reduce = 19%, Cumulative CPU 210.07 sec
2016-03-10 18:09:26,282 Stage-1 map = 89%,  reduce = 22%, Cumulative CPU 212.48 sec
2016-03-10 18:09:28,382 Stage-1 map = 90%,  reduce = 23%, Cumulative CPU 217.25 sec
2016-03-10 18:09:29,437 Stage-1 map = 90%,  reduce = 24%, Cumulative CPU 217.59 sec
2016-03-10 18:09:30,490 Stage-1 map = 94%,  reduce = 24%, Cumulative CPU 218.85 sec
2016-03-10 18:09:31,550 Stage-1 map = 94%,  reduce = 26%, Cumulative CPU 220.24 sec
2016-03-10 18:09:32,607 Stage-1 map = 94%,  reduce = 28%, Cumulative CPU 220.85 sec
2016-03-10 18:09:34,713 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 222.1 sec
2016-03-10 18:09:36,830 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 223.73 sec
2016-03-10 18:09:37,880 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 233.44 sec
2016-03-10 18:09:38,930 Stage-1 map = 100%,  reduce = 87%, Cumulative CPU 240.3 sec
2016-03-10 18:09:39,981 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 241.46 sec
2016-03-10 18:09:43,133 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 257.51 sec
2016-03-10 18:09:46,265 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 262.19 sec
MapReduce Total cumulative CPU time: 4 minutes 22 seconds 190 msec
Ended Job = job_1453192496319_1025150
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 8   Cumulative CPU: 262.19 sec   HDFS Read: 1164070689 HDFS Write: 135 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 22 seconds 190 msec
OK
Time taken: 76.424 seconds, Fetched: 3 row(s)
2016-03-10 18:09:48 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:09:48 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160212
2016-03-10 18:09:48 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:09:48 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
Exception in thread "Prototyper" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
	at java.util.HashMap$KeyIterator.next(HashMap.java:956)
	at java.util.AbstractCollection.toArray(AbstractCollection.java:195)
	at org.logicalcobwebs.proxool.ConnectionPoolManager.getConnectionPools(ConnectionPoolManager.java:89)
	at org.logicalcobwebs.proxool.PrototyperThread.run(PrototyperThread.java:38)
2016-03-10 18:09:48 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:09:48 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:09:48 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:09:48 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:09:49 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:09:49 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:09:49 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:09:49 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1241millis
2016-03-10 18:09:50 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:09:50 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:09:50 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:09:50 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:09:50 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160213.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181000_8d1d0de1-18d0-474b-87b6-3b78d819f883
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025167, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025167/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025167
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 11
2016-03-10 18:10:21,555 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:10:32,118 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 21.59 sec
2016-03-10 18:10:35,291 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 89.51 sec
2016-03-10 18:10:36,343 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 100.4 sec
2016-03-10 18:10:37,400 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 120.87 sec
2016-03-10 18:10:38,454 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 130.73 sec
2016-03-10 18:10:39,513 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 142.9 sec
2016-03-10 18:10:40,568 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 150.33 sec
2016-03-10 18:10:41,623 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 165.06 sec
2016-03-10 18:10:42,672 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 172.71 sec
2016-03-10 18:10:43,737 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 185.02 sec
2016-03-10 18:10:44,790 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 202.45 sec
2016-03-10 18:10:45,846 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 205.61 sec
2016-03-10 18:10:46,908 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 215.15 sec
2016-03-10 18:10:47,956 Stage-1 map = 54%,  reduce = 0%, Cumulative CPU 227.27 sec
2016-03-10 18:10:50,059 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 240.73 sec
2016-03-10 18:10:51,109 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 269.46 sec
2016-03-10 18:10:52,154 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 272.47 sec
2016-03-10 18:10:53,204 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 280.29 sec
2016-03-10 18:10:54,256 Stage-1 map = 77%,  reduce = 0%, Cumulative CPU 287.84 sec
2016-03-10 18:10:55,312 Stage-1 map = 77%,  reduce = 9%, Cumulative CPU 293.58 sec
2016-03-10 18:10:56,366 Stage-1 map = 81%,  reduce = 10%, Cumulative CPU 303.68 sec
2016-03-10 18:10:57,427 Stage-1 map = 86%,  reduce = 12%, Cumulative CPU 305.74 sec
2016-03-10 18:10:58,486 Stage-1 map = 91%,  reduce = 22%, Cumulative CPU 316.91 sec
2016-03-10 18:10:59,545 Stage-1 map = 96%,  reduce = 23%, Cumulative CPU 321.54 sec
2016-03-10 18:11:00,590 Stage-1 map = 96%,  reduce = 24%, Cumulative CPU 321.65 sec
2016-03-10 18:11:01,650 Stage-1 map = 97%,  reduce = 30%, Cumulative CPU 326.89 sec
2016-03-10 18:11:02,698 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 327.74 sec
2016-03-10 18:11:03,751 Stage-1 map = 100%,  reduce = 59%, Cumulative CPU 336.14 sec
2016-03-10 18:11:04,799 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 350.99 sec
2016-03-10 18:11:05,853 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 355.54 sec
MapReduce Total cumulative CPU time: 5 minutes 55 seconds 540 msec
Ended Job = job_1453192496319_1025167
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 11  Reduce: 11   Cumulative CPU: 355.54 sec   HDFS Read: 1852430607 HDFS Write: 197 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 55 seconds 540 msec
OK
Time taken: 66.089 seconds, Fetched: 4 row(s)
./data/mz_pv_20160213.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181118_d066b65a-1aaf-49f2-b0aa-43f823533db7
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025183, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025183/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025183
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 8
2016-03-10 18:11:41,631 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:11:52,199 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 42.13 sec
2016-03-10 18:11:54,305 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 69.88 sec
2016-03-10 18:11:55,395 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 88.63 sec
2016-03-10 18:11:57,508 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 91.51 sec
2016-03-10 18:11:58,569 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 123.56 sec
2016-03-10 18:12:00,681 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 129.69 sec
2016-03-10 18:12:01,734 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 148.41 sec
2016-03-10 18:12:03,875 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 154.58 sec
2016-03-10 18:12:04,926 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 169.53 sec
2016-03-10 18:12:07,049 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 176.16 sec
2016-03-10 18:12:08,099 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 188.44 sec
2016-03-10 18:12:10,199 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 212.2 sec
2016-03-10 18:12:11,262 Stage-1 map = 70%,  reduce = 9%, Cumulative CPU 229.02 sec
2016-03-10 18:12:12,324 Stage-1 map = 76%,  reduce = 12%, Cumulative CPU 232.75 sec
2016-03-10 18:12:13,378 Stage-1 map = 81%,  reduce = 12%, Cumulative CPU 235.7 sec
2016-03-10 18:12:14,425 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 257.98 sec
2016-03-10 18:12:15,479 Stage-1 map = 95%,  reduce = 24%, Cumulative CPU 262.39 sec
2016-03-10 18:12:16,529 Stage-1 map = 100%,  reduce = 24%, Cumulative CPU 265.79 sec
2016-03-10 18:12:17,584 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 280.27 sec
2016-03-10 18:12:18,645 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 283.88 sec
MapReduce Total cumulative CPU time: 4 minutes 43 seconds 880 msec
Ended Job = job_1453192496319_1025183
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 8   Cumulative CPU: 283.88 sec   HDFS Read: 1214741320 HDFS Write: 134 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 43 seconds 880 msec
OK
Time taken: 61.502 seconds, Fetched: 3 row(s)
2016-03-10 18:12:20 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:12:20 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160213
2016-03-10 18:12:20 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:12:20 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:12:20 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:12:21 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:12:21 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:12:21 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:12:22 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:12:22 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:12:22 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:12:22 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1126millis
2016-03-10 18:12:22 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:12:22 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:12:22 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:12:22 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:12:22 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160214.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181233_39c1bb83-b370-41b4-ba51-99f8f1770cfd
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 12
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025199, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025199/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025199
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 12
2016-03-10 18:12:50,488 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:13:02,104 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 45.58 sec
2016-03-10 18:13:03,157 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 89.82 sec
2016-03-10 18:13:05,272 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 136.52 sec
2016-03-10 18:13:06,328 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 149.23 sec
2016-03-10 18:13:07,379 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 168.9 sec
2016-03-10 18:13:08,424 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 194.16 sec
2016-03-10 18:13:09,473 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 209.38 sec
2016-03-10 18:13:10,519 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 225.04 sec
2016-03-10 18:13:11,573 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 234.29 sec
2016-03-10 18:13:12,618 Stage-1 map = 54%,  reduce = 0%, Cumulative CPU 254.96 sec
2016-03-10 18:13:13,674 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 261.03 sec
2016-03-10 18:13:14,721 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 270.29 sec
2016-03-10 18:13:15,787 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 282.62 sec
2016-03-10 18:13:16,834 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 287.64 sec
2016-03-10 18:13:17,892 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 297.31 sec
2016-03-10 18:13:18,938 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 310.42 sec
2016-03-10 18:13:20,013 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 316.56 sec
2016-03-10 18:13:22,129 Stage-1 map = 77%,  reduce = 9%, Cumulative CPU 331.04 sec
2016-03-10 18:13:23,188 Stage-1 map = 81%,  reduce = 10%, Cumulative CPU 353.99 sec
2016-03-10 18:13:24,246 Stage-1 map = 85%,  reduce = 12%, Cumulative CPU 358.91 sec
2016-03-10 18:13:25,304 Stage-1 map = 85%,  reduce = 16%, Cumulative CPU 366.57 sec
2016-03-10 18:13:26,368 Stage-1 map = 85%,  reduce = 18%, Cumulative CPU 370.87 sec
2016-03-10 18:13:27,426 Stage-1 map = 85%,  reduce = 21%, Cumulative CPU 377.11 sec
2016-03-10 18:13:28,484 Stage-1 map = 85%,  reduce = 23%, Cumulative CPU 380.48 sec
2016-03-10 18:13:30,598 Stage-1 map = 89%,  reduce = 23%, Cumulative CPU 393.34 sec
2016-03-10 18:13:31,653 Stage-1 map = 93%,  reduce = 25%, Cumulative CPU 398.26 sec
2016-03-10 18:13:34,822 Stage-1 map = 96%,  reduce = 28%, Cumulative CPU 404.37 sec
2016-03-10 18:13:35,883 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 407.84 sec
2016-03-10 18:13:37,996 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 411.01 sec
2016-03-10 18:13:39,057 Stage-1 map = 96%,  reduce = 31%, Cumulative CPU 413.55 sec
2016-03-10 18:13:47,532 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 442.85 sec
2016-03-10 18:13:48,598 Stage-1 map = 100%,  reduce = 68%, Cumulative CPU 455.5 sec
2016-03-10 18:13:49,656 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 464.93 sec
2016-03-10 18:13:50,704 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 467.75 sec
2016-03-10 18:13:51,760 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 469.86 sec
2016-03-10 18:13:52,816 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 478.49 sec
2016-03-10 18:13:54,930 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 481.96 sec
MapReduce Total cumulative CPU time: 8 minutes 1 seconds 960 msec
Ended Job = job_1453192496319_1025199
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 13  Reduce: 12   Cumulative CPU: 481.96 sec   HDFS Read: 1923491006 HDFS Write: 206 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 1 seconds 960 msec
OK
Time taken: 82.994 seconds, Fetched: 4 row(s)
./data/mz_pv_20160214.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181407_3b92b373-c738-446c-bdf2-ab383718f115
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025210, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025210/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025210
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 8
2016-03-10 18:14:35,910 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:14:47,695 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 12.54 sec
2016-03-10 18:14:50,259 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 25.25 sec
2016-03-10 18:14:51,330 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 39.08 sec
2016-03-10 18:14:52,386 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 42.0 sec
2016-03-10 18:14:53,432 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 56.73 sec
2016-03-10 18:14:54,477 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 58.46 sec
2016-03-10 18:14:56,645 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 74.44 sec
2016-03-10 18:14:57,696 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 80.49 sec
2016-03-10 18:14:59,793 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 89.89 sec
2016-03-10 18:15:00,856 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 94.16 sec
2016-03-10 18:15:03,059 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 100.5 sec
2016-03-10 18:15:04,121 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 108.15 sec
2016-03-10 18:15:07,616 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 118.55 sec
2016-03-10 18:15:08,666 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 131.53 sec
2016-03-10 18:15:10,023 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 136.65 sec
2016-03-10 18:15:11,077 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 147.82 sec
2016-03-10 18:15:12,123 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 153.09 sec
2016-03-10 18:15:14,225 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 163.52 sec
2016-03-10 18:15:16,866 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 174.49 sec
2016-03-10 18:15:17,922 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 184.31 sec
2016-03-10 18:15:20,031 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 191.78 sec
2016-03-10 18:15:21,084 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 197.73 sec
2016-03-10 18:15:22,142 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 201.07 sec
2016-03-10 18:15:24,243 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 208.08 sec
2016-03-10 18:15:26,715 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 209.8 sec
2016-03-10 18:15:27,775 Stage-1 map = 84%,  reduce = 6%, Cumulative CPU 213.08 sec
2016-03-10 18:15:28,832 Stage-1 map = 84%,  reduce = 9%, Cumulative CPU 213.5 sec
2016-03-10 18:15:29,882 Stage-1 map = 90%,  reduce = 12%, Cumulative CPU 218.83 sec
2016-03-10 18:15:30,943 Stage-1 map = 90%,  reduce = 20%, Cumulative CPU 220.01 sec
2016-03-10 18:15:31,994 Stage-1 map = 90%,  reduce = 21%, Cumulative CPU 220.12 sec
2016-03-10 18:15:33,049 Stage-1 map = 90%,  reduce = 29%, Cumulative CPU 223.62 sec
2016-03-10 18:15:36,197 Stage-1 map = 92%,  reduce = 29%, Cumulative CPU 227.23 sec
2016-03-10 18:15:42,547 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 237.92 sec
2016-03-10 18:15:46,830 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 244.41 sec
2016-03-10 18:15:47,884 Stage-1 map = 100%,  reduce = 47%, Cumulative CPU 248.5 sec
2016-03-10 18:15:48,972 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 261.33 sec
2016-03-10 18:15:50,026 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 263.57 sec
MapReduce Total cumulative CPU time: 4 minutes 23 seconds 570 msec
Ended Job = job_1453192496319_1025210
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 8   Cumulative CPU: 263.57 sec   HDFS Read: 1185323061 HDFS Write: 133 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 23 seconds 570 msec
OK
Time taken: 103.736 seconds, Fetched: 3 row(s)
2016-03-10 18:15:52 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:15:52 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160214
2016-03-10 18:15:52 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:15:52 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:15:52 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:15:52 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:15:52 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:15:52 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:15:53 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:15:53 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:15:53 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:15:53 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1077millis
2016-03-10 18:15:53 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:15:53 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:15:53 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:15:53 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:15:53 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160215.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181604_11e91c12-cd4e-44ff-be9e-c1b063423832
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025234, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025234/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025234
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 11
2016-03-10 18:16:20,653 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:16:31,243 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 22.95 sec
2016-03-10 18:16:32,298 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 30.45 sec
2016-03-10 18:16:34,406 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 93.88 sec
2016-03-10 18:16:36,514 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 121.86 sec
2016-03-10 18:16:37,562 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 138.05 sec
2016-03-10 18:16:39,672 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 166.86 sec
2016-03-10 18:16:40,724 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 179.36 sec
2016-03-10 18:16:41,775 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 222.47 sec
2016-03-10 18:16:42,824 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 231.76 sec
2016-03-10 18:16:43,879 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 255.22 sec
2016-03-10 18:16:44,932 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 275.92 sec
2016-03-10 18:16:45,991 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 288.3 sec
2016-03-10 18:16:47,041 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 297.48 sec
2016-03-10 18:16:48,091 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 306.27 sec
2016-03-10 18:16:49,140 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 312.19 sec
2016-03-10 18:16:50,201 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 328.06 sec
2016-03-10 18:16:51,266 Stage-1 map = 67%,  reduce = 4%, Cumulative CPU 332.67 sec
2016-03-10 18:16:52,325 Stage-1 map = 72%,  reduce = 9%, Cumulative CPU 365.99 sec
2016-03-10 18:16:53,379 Stage-1 map = 75%,  reduce = 13%, Cumulative CPU 373.18 sec
2016-03-10 18:16:54,432 Stage-1 map = 76%,  reduce = 19%, Cumulative CPU 377.37 sec
2016-03-10 18:16:55,482 Stage-1 map = 78%,  reduce = 19%, Cumulative CPU 383.59 sec
2016-03-10 18:16:57,600 Stage-1 map = 86%,  reduce = 20%, Cumulative CPU 395.53 sec
2016-03-10 18:16:58,653 Stage-1 map = 91%,  reduce = 21%, Cumulative CPU 424.21 sec
2016-03-10 18:16:59,697 Stage-1 map = 95%,  reduce = 23%, Cumulative CPU 430.45 sec
2016-03-10 18:17:00,741 Stage-1 map = 95%,  reduce = 29%, Cumulative CPU 431.65 sec
2016-03-10 18:17:01,790 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 431.76 sec
2016-03-10 18:17:02,846 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 434.78 sec
2016-03-10 18:17:03,890 Stage-1 map = 96%,  reduce = 31%, Cumulative CPU 436.35 sec
2016-03-10 18:17:05,984 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 440.17 sec
2016-03-10 18:17:07,028 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 444.58 sec
2016-03-10 18:17:08,080 Stage-1 map = 100%,  reduce = 55%, Cumulative CPU 452.94 sec
2016-03-10 18:17:09,123 Stage-1 map = 100%,  reduce = 71%, Cumulative CPU 458.94 sec
2016-03-10 18:17:10,166 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 492.39 sec
2016-03-10 18:17:11,212 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 502.82 sec
MapReduce Total cumulative CPU time: 8 minutes 22 seconds 820 msec
Ended Job = job_1453192496319_1025234
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 12  Reduce: 11   Cumulative CPU: 502.82 sec   HDFS Read: 1871208384 HDFS Write: 197 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 22 seconds 820 msec
OK
Time taken: 67.751 seconds, Fetched: 4 row(s)
./data/mz_pv_20160215.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181723_cab46fb9-bc7c-44df-bf5c-bb101da15299
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025241, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025241/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025241
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:17:40,462 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:17:51,018 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 14.21 sec
2016-03-10 18:17:52,072 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 27.51 sec
2016-03-10 18:17:53,119 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 57.21 sec
2016-03-10 18:17:54,175 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 63.64 sec
2016-03-10 18:17:55,224 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 84.05 sec
2016-03-10 18:17:56,270 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 112.99 sec
2016-03-10 18:17:57,317 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 118.46 sec
2016-03-10 18:17:58,366 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 124.56 sec
2016-03-10 18:17:59,419 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 134.0 sec
2016-03-10 18:18:00,468 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 139.94 sec
2016-03-10 18:18:01,516 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 145.99 sec
2016-03-10 18:18:02,565 Stage-1 map = 54%,  reduce = 0%, Cumulative CPU 156.52 sec
2016-03-10 18:18:03,606 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 162.35 sec
2016-03-10 18:18:04,671 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 168.34 sec
2016-03-10 18:18:05,729 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 173.78 sec
2016-03-10 18:18:06,782 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 177.25 sec
2016-03-10 18:18:07,830 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 187.38 sec
2016-03-10 18:18:08,881 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 196.61 sec
2016-03-10 18:18:09,929 Stage-1 map = 84%,  reduce = 0%, Cumulative CPU 200.52 sec
2016-03-10 18:18:10,983 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 206.13 sec
2016-03-10 18:18:13,099 Stage-1 map = 86%,  reduce = 10%, Cumulative CPU 207.28 sec
2016-03-10 18:18:14,164 Stage-1 map = 94%,  reduce = 17%, Cumulative CPU 214.81 sec
2016-03-10 18:18:15,224 Stage-1 map = 94%,  reduce = 21%, Cumulative CPU 215.26 sec
2016-03-10 18:18:16,286 Stage-1 map = 100%,  reduce = 30%, Cumulative CPU 218.15 sec
2016-03-10 18:18:17,354 Stage-1 map = 100%,  reduce = 55%, Cumulative CPU 223.49 sec
2016-03-10 18:18:18,409 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 232.85 sec
2016-03-10 18:18:19,455 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 265.9 sec
2016-03-10 18:18:20,566 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 267.97 sec
MapReduce Total cumulative CPU time: 4 minutes 27 seconds 970 msec
Ended Job = job_1453192496319_1025241
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 267.97 sec   HDFS Read: 1039487381 HDFS Write: 134 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 27 seconds 970 msec
OK
Time taken: 57.957 seconds, Fetched: 3 row(s)
2016-03-10 18:18:22 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:18:22 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160215
2016-03-10 18:18:22 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:18:22 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:18:22 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:18:22 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:18:22 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:18:22 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:18:24 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:18:24 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:18:24 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:18:24 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1165millis
2016-03-10 18:18:24 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:18:24 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:18:24 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:18:24 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:18:24 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160216.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310181834_57175cf2-686f-4c68-8673-cf9d20fe64cc
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025253, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025253/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025253
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 11
2016-03-10 18:18:56,165 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:19:06,767 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 6.83 sec
2016-03-10 18:19:08,865 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 31.8 sec
2016-03-10 18:19:10,984 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 71.85 sec
2016-03-10 18:19:12,039 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 75.2 sec
2016-03-10 18:19:14,130 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 90.69 sec
2016-03-10 18:19:16,239 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 100.85 sec
2016-03-10 18:19:17,296 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 111.22 sec
2016-03-10 18:19:18,354 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 114.45 sec
2016-03-10 18:19:19,408 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 121.86 sec
2016-03-10 18:19:21,514 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 132.42 sec
2016-03-10 18:19:22,565 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 140.06 sec
2016-03-10 18:19:23,615 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 145.59 sec
2016-03-10 18:19:25,710 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 156.59 sec
2016-03-10 18:19:26,764 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 163.48 sec
2016-03-10 18:19:27,821 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 166.8 sec
2016-03-10 18:19:28,890 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 185.88 sec
2016-03-10 18:19:29,949 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 204.03 sec
2016-03-10 18:19:32,049 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 213.49 sec
2016-03-10 18:19:33,119 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 222.86 sec
2016-03-10 18:19:36,284 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 241.07 sec
2016-03-10 18:19:37,338 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 276.86 sec
2016-03-10 18:19:40,513 Stage-1 map = 59%,  reduce = 8%, Cumulative CPU 296.62 sec
2016-03-10 18:19:41,570 Stage-1 map = 59%,  reduce = 12%, Cumulative CPU 300.56 sec
2016-03-10 18:19:42,630 Stage-1 map = 59%,  reduce = 14%, Cumulative CPU 301.0 sec
2016-03-10 18:19:43,686 Stage-1 map = 63%,  reduce = 14%, Cumulative CPU 315.38 sec
2016-03-10 18:19:45,801 Stage-1 map = 66%,  reduce = 15%, Cumulative CPU 320.61 sec
2016-03-10 18:19:46,859 Stage-1 map = 67%,  reduce = 17%, Cumulative CPU 327.39 sec
2016-03-10 18:19:47,913 Stage-1 map = 68%,  reduce = 17%, Cumulative CPU 329.25 sec
2016-03-10 18:19:48,967 Stage-1 map = 73%,  reduce = 17%, Cumulative CPU 332.3 sec
2016-03-10 18:19:50,017 Stage-1 map = 76%,  reduce = 18%, Cumulative CPU 341.06 sec
2016-03-10 18:19:51,074 Stage-1 map = 81%,  reduce = 19%, Cumulative CPU 343.93 sec
2016-03-10 18:19:52,126 Stage-1 map = 81%,  reduce = 20%, Cumulative CPU 344.43 sec
2016-03-10 18:19:53,177 Stage-1 map = 82%,  reduce = 22%, Cumulative CPU 352.05 sec
2016-03-10 18:19:54,242 Stage-1 map = 82%,  reduce = 24%, Cumulative CPU 353.7 sec
2016-03-10 18:19:56,341 Stage-1 map = 83%,  reduce = 24%, Cumulative CPU 363.32 sec
2016-03-10 18:19:57,389 Stage-1 map = 84%,  reduce = 24%, Cumulative CPU 359.4 sec
2016-03-10 18:19:58,437 Stage-1 map = 90%,  reduce = 25%, Cumulative CPU 364.38 sec
2016-03-10 18:19:59,485 Stage-1 map = 91%,  reduce = 25%, Cumulative CPU 367.69 sec
2016-03-10 18:20:00,527 Stage-1 map = 91%,  reduce = 26%, Cumulative CPU 367.95 sec
2016-03-10 18:20:01,577 Stage-1 map = 91%,  reduce = 27%, Cumulative CPU 373.89 sec
2016-03-10 18:20:03,676 Stage-1 map = 96%,  reduce = 27%, Cumulative CPU 376.41 sec
2016-03-10 18:20:04,726 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 379.05 sec
2016-03-10 18:20:05,782 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 381.04 sec
2016-03-10 18:20:06,831 Stage-1 map = 100%,  reduce = 46%, Cumulative CPU 385.53 sec
2016-03-10 18:20:07,879 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 399.15 sec
2016-03-10 18:20:08,929 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 414.71 sec
MapReduce Total cumulative CPU time: 6 minutes 54 seconds 710 msec
Ended Job = job_1453192496319_1025253
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 11  Reduce: 11   Cumulative CPU: 414.71 sec   HDFS Read: 1893168801 HDFS Write: 197 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 54 seconds 710 msec
OK
Time taken: 95.177 seconds, Fetched: 4 row(s)
./data/mz_pv_20160216.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182021_aab456e5-52e8-4740-ba79-d12b732cc7b4
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025263, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025263/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025263
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 7
2016-03-10 18:20:41,126 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:20:49,643 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 5.23 sec
2016-03-10 18:20:51,743 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 48.62 sec
2016-03-10 18:20:54,907 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 68.3 sec
2016-03-10 18:20:58,062 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 88.91 sec
2016-03-10 18:20:59,108 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 90.99 sec
2016-03-10 18:21:00,153 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 93.08 sec
2016-03-10 18:21:01,223 Stage-1 map = 59%,  reduce = 9%, Cumulative CPU 109.32 sec
2016-03-10 18:21:03,331 Stage-1 map = 59%,  reduce = 11%, Cumulative CPU 110.07 sec
2016-03-10 18:21:04,387 Stage-1 map = 65%,  reduce = 13%, Cumulative CPU 125.49 sec
2016-03-10 18:21:06,510 Stage-1 map = 66%,  reduce = 13%, Cumulative CPU 129.08 sec
2016-03-10 18:21:07,575 Stage-1 map = 70%,  reduce = 13%, Cumulative CPU 141.5 sec
2016-03-10 18:21:08,624 Stage-1 map = 75%,  reduce = 13%, Cumulative CPU 144.43 sec
2016-03-10 18:21:09,679 Stage-1 map = 87%,  reduce = 18%, Cumulative CPU 157.84 sec
2016-03-10 18:21:10,726 Stage-1 map = 88%,  reduce = 21%, Cumulative CPU 161.59 sec
2016-03-10 18:21:11,780 Stage-1 map = 88%,  reduce = 23%, Cumulative CPU 162.11 sec
2016-03-10 18:21:12,836 Stage-1 map = 90%,  reduce = 24%, Cumulative CPU 169.4 sec
2016-03-10 18:21:13,883 Stage-1 map = 100%,  reduce = 25%, Cumulative CPU 170.65 sec
2016-03-10 18:21:14,937 Stage-1 map = 100%,  reduce = 71%, Cumulative CPU 178.13 sec
2016-03-10 18:21:15,989 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 181.15 sec
2016-03-10 18:21:17,042 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 194.94 sec
MapReduce Total cumulative CPU time: 3 minutes 14 seconds 940 msec
Ended Job = job_1453192496319_1025263
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 7   Cumulative CPU: 194.94 sec   HDFS Read: 1023928174 HDFS Write: 134 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 14 seconds 940 msec
OK
Time taken: 56.791 seconds, Fetched: 3 row(s)
2016-03-10 18:21:19 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:21:19 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160216
2016-03-10 18:21:19 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:21:19 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:21:19 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:21:19 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:21:19 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:21:19 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:21:20 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:21:20 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:21:20 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:21:20 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1084millis
2016-03-10 18:21:20 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:21:20 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:21:20 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:21:20 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:21:20 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160217.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182131_5d944f67-33c9-4bb2-820d-0f51ef864fd2
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025280, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025280/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025280
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 11
2016-03-10 18:21:52,901 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:22:03,485 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 7.25 sec
2016-03-10 18:22:05,613 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 38.76 sec
2016-03-10 18:22:06,678 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 54.95 sec
2016-03-10 18:22:08,781 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 89.78 sec
2016-03-10 18:22:09,831 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 104.59 sec
2016-03-10 18:22:10,887 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 113.56 sec
2016-03-10 18:22:11,942 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 133.48 sec
2016-03-10 18:22:13,004 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 142.68 sec
2016-03-10 18:22:14,057 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 148.72 sec
2016-03-10 18:22:15,143 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 174.39 sec
2016-03-10 18:22:17,369 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 207.79 sec
2016-03-10 18:22:18,436 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 224.22 sec
2016-03-10 18:22:19,495 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 235.93 sec
2016-03-10 18:22:20,554 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 245.54 sec
2016-03-10 18:22:21,607 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 255.08 sec
2016-03-10 18:22:22,662 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 261.48 sec
2016-03-10 18:22:23,720 Stage-1 map = 44%,  reduce = 2%, Cumulative CPU 274.75 sec
2016-03-10 18:22:24,770 Stage-1 map = 46%,  reduce = 2%, Cumulative CPU 281.58 sec
2016-03-10 18:22:25,822 Stage-1 map = 47%,  reduce = 6%, Cumulative CPU 290.36 sec
2016-03-10 18:22:26,875 Stage-1 map = 54%,  reduce = 7%, Cumulative CPU 305.14 sec
2016-03-10 18:22:28,991 Stage-1 map = 54%,  reduce = 8%, Cumulative CPU 335.99 sec
2016-03-10 18:22:30,055 Stage-1 map = 56%,  reduce = 10%, Cumulative CPU 354.01 sec
2016-03-10 18:22:31,113 Stage-1 map = 62%,  reduce = 10%, Cumulative CPU 362.01 sec
2016-03-10 18:22:32,168 Stage-1 map = 64%,  reduce = 11%, Cumulative CPU 365.17 sec
2016-03-10 18:22:33,225 Stage-1 map = 66%,  reduce = 14%, Cumulative CPU 372.77 sec
2016-03-10 18:22:34,281 Stage-1 map = 75%,  reduce = 14%, Cumulative CPU 426.21 sec
2016-03-10 18:22:35,339 Stage-1 map = 76%,  reduce = 15%, Cumulative CPU 429.42 sec
2016-03-10 18:22:36,391 Stage-1 map = 80%,  reduce = 16%, Cumulative CPU 435.95 sec
2016-03-10 18:22:37,446 Stage-1 map = 90%,  reduce = 17%, Cumulative CPU 450.01 sec
2016-03-10 18:22:38,489 Stage-1 map = 95%,  reduce = 21%, Cumulative CPU 452.68 sec
2016-03-10 18:22:39,538 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 455.38 sec
2016-03-10 18:22:40,592 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 457.95 sec
2016-03-10 18:22:41,637 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 458.2 sec
2016-03-10 18:22:42,690 Stage-1 map = 96%,  reduce = 31%, Cumulative CPU 461.79 sec
2016-03-10 18:22:44,794 Stage-1 map = 100%,  reduce = 37%, Cumulative CPU 465.35 sec
2016-03-10 18:22:45,841 Stage-1 map = 100%,  reduce = 64%, Cumulative CPU 478.73 sec
2016-03-10 18:22:46,889 Stage-1 map = 100%,  reduce = 79%, Cumulative CPU 486.57 sec
2016-03-10 18:22:47,944 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 493.0 sec
2016-03-10 18:22:48,992 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 508.79 sec
2016-03-10 18:22:52,141 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 513.1 sec
MapReduce Total cumulative CPU time: 8 minutes 33 seconds 100 msec
Ended Job = job_1453192496319_1025280
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 12  Reduce: 11   Cumulative CPU: 513.1 sec   HDFS Read: 1886088173 HDFS Write: 196 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 33 seconds 100 msec
OK
Time taken: 81.556 seconds, Fetched: 4 row(s)
./data/mz_pv_20160217.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182304_d776a317-728a-4c4b-ba10-2934e39d9977
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025289, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025289/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025289
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:23:27,020 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:23:37,625 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 6.98 sec
2016-03-10 18:23:38,677 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 21.56 sec
2016-03-10 18:23:39,734 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 29.08 sec
2016-03-10 18:23:40,803 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 42.0 sec
2016-03-10 18:23:41,858 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 45.12 sec
2016-03-10 18:23:42,913 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 63.46 sec
2016-03-10 18:23:43,962 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 68.97 sec
2016-03-10 18:23:45,006 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 96.19 sec
2016-03-10 18:23:46,054 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 105.54 sec
2016-03-10 18:23:47,098 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 111.57 sec
2016-03-10 18:23:48,150 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 117.0 sec
2016-03-10 18:23:49,197 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 125.89 sec
2016-03-10 18:23:50,242 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 131.59 sec
2016-03-10 18:23:51,300 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 137.62 sec
2016-03-10 18:23:52,361 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 146.63 sec
2016-03-10 18:23:53,412 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 153.23 sec
2016-03-10 18:23:54,467 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 159.82 sec
2016-03-10 18:23:55,521 Stage-1 map = 84%,  reduce = 0%, Cumulative CPU 168.41 sec
2016-03-10 18:23:58,672 Stage-1 map = 94%,  reduce = 0%, Cumulative CPU 175.38 sec
2016-03-10 18:23:59,725 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 177.01 sec
2016-03-10 18:24:00,777 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 191.39 sec
2016-03-10 18:24:01,833 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 195.38 sec
MapReduce Total cumulative CPU time: 3 minutes 15 seconds 380 msec
Ended Job = job_1453192496319_1025289
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 195.38 sec   HDFS Read: 991702693 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 15 seconds 380 msec
OK
Time taken: 58.373 seconds, Fetched: 3 row(s)
2016-03-10 18:24:03 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:24:03 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160217
2016-03-10 18:24:03 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:24:03 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:24:04 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:24:04 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:24:04 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:24:04 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:24:05 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:24:05 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:24:05 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:24:05 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1132millis
2016-03-10 18:24:05 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:24:05 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:24:05 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:24:05 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:24:05 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160218.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182416_4f974f15-449c-4160-86cd-b82c0aa60a98
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025306, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025306/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025306
Hadoop job information for Stage-1: number of mappers: 15; number of reducers: 13
2016-03-10 18:24:33,523 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:24:44,063 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 99.1 sec
2016-03-10 18:24:46,178 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 131.58 sec
2016-03-10 18:24:47,239 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 172.7 sec
2016-03-10 18:24:50,398 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 243.82 sec
2016-03-10 18:24:51,449 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 246.99 sec
2016-03-10 18:24:52,502 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 265.34 sec
2016-03-10 18:24:53,549 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 287.33 sec
2016-03-10 18:24:55,649 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 295.8 sec
2016-03-10 18:24:56,700 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 310.73 sec
2016-03-10 18:24:59,911 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 336.67 sec
2016-03-10 18:25:00,962 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 338.27 sec
2016-03-10 18:25:02,009 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 347.73 sec
2016-03-10 18:25:03,064 Stage-1 map = 85%,  reduce = 5%, Cumulative CPU 358.81 sec
2016-03-10 18:25:04,133 Stage-1 map = 91%,  reduce = 24%, Cumulative CPU 368.6 sec
2016-03-10 18:25:06,248 Stage-1 map = 92%,  reduce = 26%, Cumulative CPU 374.68 sec
2016-03-10 18:25:07,316 Stage-1 map = 92%,  reduce = 29%, Cumulative CPU 375.9 sec
2016-03-10 18:25:08,361 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 378.87 sec
2016-03-10 18:25:10,465 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 385.44 sec
2016-03-10 18:25:12,568 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 389.3 sec
2016-03-10 18:25:14,671 Stage-1 map = 100%,  reduce = 36%, Cumulative CPU 393.41 sec
2016-03-10 18:25:15,721 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 415.98 sec
2016-03-10 18:25:16,769 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 417.19 sec
MapReduce Total cumulative CPU time: 6 minutes 57 seconds 190 msec
Ended Job = job_1453192496319_1025306
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 15  Reduce: 13   Cumulative CPU: 417.19 sec   HDFS Read: 2164732560 HDFS Write: 211 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 57 seconds 190 msec
OK
Time taken: 61.889 seconds, Fetched: 4 row(s)
./data/mz_pv_20160218.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182529_0413a8f9-51f3-48b4-a08c-738f922da6ab
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025326, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025326/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025326
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:25:47,163 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:25:57,721 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 35.79 sec
2016-03-10 18:26:00,881 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 50.6 sec
2016-03-10 18:26:02,981 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 60.65 sec
2016-03-10 18:26:04,030 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 72.58 sec
2016-03-10 18:26:06,130 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 75.7 sec
2016-03-10 18:26:07,181 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 87.48 sec
2016-03-10 18:26:08,227 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 89.4 sec
2016-03-10 18:26:09,277 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 109.6 sec
2016-03-10 18:26:10,329 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 118.7 sec
2016-03-10 18:26:11,371 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 122.36 sec
2016-03-10 18:26:12,426 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 141.04 sec
2016-03-10 18:26:13,487 Stage-1 map = 79%,  reduce = 17%, Cumulative CPU 142.85 sec
2016-03-10 18:26:14,537 Stage-1 map = 81%,  reduce = 24%, Cumulative CPU 146.68 sec
2016-03-10 18:26:15,589 Stage-1 map = 84%,  reduce = 24%, Cumulative CPU 149.81 sec
2016-03-10 18:26:16,642 Stage-1 map = 90%,  reduce = 27%, Cumulative CPU 152.53 sec
2016-03-10 18:26:17,699 Stage-1 map = 90%,  reduce = 28%, Cumulative CPU 152.65 sec
2016-03-10 18:26:18,753 Stage-1 map = 91%,  reduce = 28%, Cumulative CPU 155.62 sec
2016-03-10 18:26:19,808 Stage-1 map = 91%,  reduce = 29%, Cumulative CPU 156.2 sec
2016-03-10 18:26:20,863 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 159.59 sec
2016-03-10 18:26:24,009 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 163.62 sec
2016-03-10 18:26:26,106 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 175.36 sec
2016-03-10 18:26:27,161 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 178.81 sec
2016-03-10 18:26:28,207 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 181.32 sec
MapReduce Total cumulative CPU time: 3 minutes 1 seconds 320 msec
Ended Job = job_1453192496319_1025326
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 181.32 sec   HDFS Read: 1011387376 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 1 seconds 320 msec
OK
Time taken: 59.765 seconds, Fetched: 3 row(s)
2016-03-10 18:26:30 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:26:30 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160218
2016-03-10 18:26:30 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:26:30 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:26:30 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:26:30 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:26:30 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:26:30 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:26:31 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:26:31 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:26:31 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:26:31 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1111millis
2016-03-10 18:26:31 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:26:31 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:26:31 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:26:31 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:26:31 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160219.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182643_df813ef4-7725-4d10-be18-f96cb46b3a42
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025341, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025341/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025341
Hadoop job information for Stage-1: number of mappers: 16; number of reducers: 14
2016-03-10 18:26:59,587 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:27:10,208 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 110.26 sec
2016-03-10 18:27:13,380 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 170.29 sec
2016-03-10 18:27:14,436 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 173.19 sec
2016-03-10 18:27:16,545 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 220.19 sec
2016-03-10 18:27:17,595 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 226.92 sec
2016-03-10 18:27:18,648 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 232.12 sec
2016-03-10 18:27:19,758 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 266.63 sec
2016-03-10 18:27:20,833 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 272.82 sec
2016-03-10 18:27:21,888 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 278.81 sec
2016-03-10 18:27:22,937 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 300.39 sec
2016-03-10 18:27:23,985 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 305.67 sec
2016-03-10 18:27:25,058 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 308.35 sec
2016-03-10 18:27:26,110 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 326.34 sec
2016-03-10 18:27:27,161 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 332.37 sec
2016-03-10 18:27:28,225 Stage-1 map = 80%,  reduce = 8%, Cumulative CPU 342.97 sec
2016-03-10 18:27:29,311 Stage-1 map = 81%,  reduce = 17%, Cumulative CPU 354.39 sec
2016-03-10 18:27:30,376 Stage-1 map = 84%,  reduce = 19%, Cumulative CPU 357.51 sec
2016-03-10 18:27:31,647 Stage-1 map = 86%,  reduce = 20%, Cumulative CPU 386.27 sec
2016-03-10 18:27:32,731 Stage-1 map = 86%,  reduce = 21%, Cumulative CPU 390.38 sec
2016-03-10 18:27:33,790 Stage-1 map = 91%,  reduce = 21%, Cumulative CPU 396.53 sec
2016-03-10 18:27:34,849 Stage-1 map = 91%,  reduce = 23%, Cumulative CPU 402.66 sec
2016-03-10 18:27:35,910 Stage-1 map = 94%,  reduce = 26%, Cumulative CPU 407.72 sec
2016-03-10 18:27:36,969 Stage-1 map = 94%,  reduce = 27%, Cumulative CPU 408.46 sec
2016-03-10 18:27:38,017 Stage-1 map = 94%,  reduce = 28%, Cumulative CPU 411.62 sec
2016-03-10 18:27:39,072 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 414.51 sec
2016-03-10 18:27:43,279 Stage-1 map = 97%,  reduce = 29%, Cumulative CPU 424.83 sec
2016-03-10 18:27:44,331 Stage-1 map = 98%,  reduce = 30%, Cumulative CPU 429.57 sec
2016-03-10 18:27:45,381 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 432.27 sec
2016-03-10 18:27:46,446 Stage-1 map = 100%,  reduce = 41%, Cumulative CPU 433.85 sec
2016-03-10 18:27:47,516 Stage-1 map = 100%,  reduce = 56%, Cumulative CPU 439.58 sec
2016-03-10 18:27:48,668 Stage-1 map = 100%,  reduce = 76%, Cumulative CPU 453.36 sec
2016-03-10 18:27:49,724 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 470.1 sec
2016-03-10 18:27:50,789 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 480.13 sec
MapReduce Total cumulative CPU time: 8 minutes 0 seconds 130 msec
Ended Job = job_1453192496319_1025341
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 16  Reduce: 14   Cumulative CPU: 480.13 sec   HDFS Read: 2404159243 HDFS Write: 218 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 0 seconds 130 msec
OK
Time taken: 70.415 seconds, Fetched: 4 row(s)
./data/mz_pv_20160219.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182805_9cb477d1-ad2b-4a1a-a6e7-400a96910040
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025348, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025348/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025348
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:28:28,228 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:28:39,695 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 41.28 sec
2016-03-10 18:28:41,850 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 47.68 sec
2016-03-10 18:28:42,946 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 64.25 sec
2016-03-10 18:28:45,364 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 76.9 sec
2016-03-10 18:28:46,440 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 82.24 sec
2016-03-10 18:28:47,515 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 87.37 sec
2016-03-10 18:28:48,592 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 96.49 sec
2016-03-10 18:28:49,694 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 99.55 sec
2016-03-10 18:28:50,770 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 103.5 sec
2016-03-10 18:28:51,842 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 110.71 sec
2016-03-10 18:28:53,132 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 112.62 sec
2016-03-10 18:28:54,214 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 121.98 sec
2016-03-10 18:28:55,298 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 128.42 sec
2016-03-10 18:28:57,454 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 131.48 sec
2016-03-10 18:28:58,548 Stage-1 map = 84%,  reduce = 20%, Cumulative CPU 136.68 sec
2016-03-10 18:28:59,627 Stage-1 map = 84%,  reduce = 24%, Cumulative CPU 137.01 sec
2016-03-10 18:29:00,685 Stage-1 map = 92%,  reduce = 24%, Cumulative CPU 143.5 sec
2016-03-10 18:29:02,805 Stage-1 map = 92%,  reduce = 25%, Cumulative CPU 144.2 sec
2016-03-10 18:29:03,863 Stage-1 map = 93%,  reduce = 25%, Cumulative CPU 147.21 sec
2016-03-10 18:29:04,925 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 147.75 sec
2016-03-10 18:29:07,041 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 150.84 sec
2016-03-10 18:29:08,103 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 153.27 sec
2016-03-10 18:29:09,159 Stage-1 map = 100%,  reduce = 80%, Cumulative CPU 161.99 sec
2016-03-10 18:29:10,215 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 167.74 sec
MapReduce Total cumulative CPU time: 2 minutes 47 seconds 740 msec
Ended Job = job_1453192496319_1025348
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 167.74 sec   HDFS Read: 1000140780 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 47 seconds 740 msec
OK
Time taken: 66.222 seconds, Fetched: 3 row(s)
2016-03-10 18:29:12 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:29:12 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160219
2016-03-10 18:29:12 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:29:12 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:29:12 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:29:12 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:29:12 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:29:12 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:29:13 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:29:13 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:29:13 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:29:13 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1175millis
2016-03-10 18:29:13 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:29:13 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:29:13 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:29:13 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:29:13 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160220.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310182924_1409eb18-57f5-46bc-9b50-cb9b280595e3
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025359, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025359/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025359
Hadoop job information for Stage-1: number of mappers: 16; number of reducers: 15
2016-03-10 18:29:44,242 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:29:54,827 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 42.18 sec
2016-03-10 18:29:58,010 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 79.34 sec
2016-03-10 18:30:01,186 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 103.74 sec
2016-03-10 18:30:04,589 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 123.56 sec
2016-03-10 18:30:06,705 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 134.06 sec
2016-03-10 18:30:07,758 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 158.93 sec
2016-03-10 18:30:08,813 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 161.56 sec
2016-03-10 18:30:09,874 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 167.75 sec
2016-03-10 18:30:11,972 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 202.7 sec
2016-03-10 18:30:14,059 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 217.42 sec
2016-03-10 18:30:15,109 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 245.15 sec
2016-03-10 18:30:16,156 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 254.44 sec
2016-03-10 18:30:17,229 Stage-1 map = 38%,  reduce = 3%, Cumulative CPU 276.97 sec
2016-03-10 18:30:18,290 Stage-1 map = 39%,  reduce = 5%, Cumulative CPU 286.12 sec
2016-03-10 18:30:19,344 Stage-1 map = 44%,  reduce = 6%, Cumulative CPU 301.67 sec
2016-03-10 18:30:20,401 Stage-1 map = 49%,  reduce = 7%, Cumulative CPU 311.39 sec
2016-03-10 18:30:21,449 Stage-1 map = 51%,  reduce = 8%, Cumulative CPU 317.36 sec
2016-03-10 18:30:22,499 Stage-1 map = 56%,  reduce = 9%, Cumulative CPU 330.52 sec
2016-03-10 18:30:23,554 Stage-1 map = 58%,  reduce = 10%, Cumulative CPU 342.53 sec
2016-03-10 18:30:24,600 Stage-1 map = 58%,  reduce = 11%, Cumulative CPU 347.2 sec
2016-03-10 18:30:25,642 Stage-1 map = 65%,  reduce = 14%, Cumulative CPU 364.94 sec
2016-03-10 18:30:26,694 Stage-1 map = 69%,  reduce = 15%, Cumulative CPU 393.79 sec
2016-03-10 18:30:27,743 Stage-1 map = 69%,  reduce = 16%, Cumulative CPU 394.4 sec
2016-03-10 18:30:28,793 Stage-1 map = 74%,  reduce = 17%, Cumulative CPU 407.74 sec
2016-03-10 18:30:29,844 Stage-1 map = 80%,  reduce = 18%, Cumulative CPU 422.03 sec
2016-03-10 18:30:30,887 Stage-1 map = 84%,  reduce = 19%, Cumulative CPU 426.68 sec
2016-03-10 18:30:31,944 Stage-1 map = 87%,  reduce = 20%, Cumulative CPU 433.62 sec
2016-03-10 18:30:32,993 Stage-1 map = 90%,  reduce = 23%, Cumulative CPU 445.53 sec
2016-03-10 18:30:34,038 Stage-1 map = 96%,  reduce = 25%, Cumulative CPU 450.86 sec
2016-03-10 18:30:35,089 Stage-1 map = 96%,  reduce = 27%, Cumulative CPU 451.52 sec
2016-03-10 18:30:36,144 Stage-1 map = 97%,  reduce = 29%, Cumulative CPU 455.05 sec
2016-03-10 18:30:37,189 Stage-1 map = 97%,  reduce = 30%, Cumulative CPU 455.79 sec
2016-03-10 18:30:39,297 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 465.94 sec
2016-03-10 18:30:43,507 Stage-1 map = 100%,  reduce = 34%, Cumulative CPU 476.43 sec
2016-03-10 18:30:44,560 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 486.49 sec
2016-03-10 18:30:45,616 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 512.97 sec
2016-03-10 18:30:46,663 Stage-1 map = 100%,  reduce = 87%, Cumulative CPU 525.15 sec
2016-03-10 18:30:47,707 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 529.9 sec
2016-03-10 18:30:48,749 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 535.78 sec
2016-03-10 18:30:50,842 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 542.13 sec
2016-03-10 18:30:51,886 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 544.14 sec
MapReduce Total cumulative CPU time: 9 minutes 4 seconds 140 msec
Ended Job = job_1453192496319_1025359
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 16  Reduce: 15   Cumulative CPU: 544.14 sec   HDFS Read: 2482728884 HDFS Write: 213 SUCCESS
Total MapReduce CPU Time Spent: 9 minutes 4 seconds 140 msec
OK
Time taken: 89.5 seconds, Fetched: 4 row(s)
./data/mz_pv_20160220.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183105_4983c1d7-d556-4ba3-87f1-09559a7a33f7
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025379, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025379/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025379
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:31:33,119 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:31:43,702 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 29.16 sec
2016-03-10 18:31:46,862 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 55.65 sec
2016-03-10 18:31:50,023 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 89.88 sec
2016-03-10 18:31:51,072 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 97.63 sec
2016-03-10 18:31:52,123 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 100.83 sec
2016-03-10 18:31:53,182 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 115.68 sec
2016-03-10 18:31:54,238 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 119.49 sec
2016-03-10 18:31:55,292 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 122.64 sec
2016-03-10 18:31:56,343 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 134.74 sec
2016-03-10 18:31:57,404 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 138.91 sec
2016-03-10 18:31:59,523 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 151.83 sec
2016-03-10 18:32:00,581 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 157.25 sec
2016-03-10 18:32:01,638 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 160.3 sec
2016-03-10 18:32:02,685 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 166.2 sec
2016-03-10 18:32:03,732 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 169.16 sec
2016-03-10 18:32:04,789 Stage-1 map = 73%,  reduce = 4%, Cumulative CPU 173.33 sec
2016-03-10 18:32:05,845 Stage-1 map = 79%,  reduce = 12%, Cumulative CPU 181.63 sec
2016-03-10 18:32:06,901 Stage-1 map = 85%,  reduce = 12%, Cumulative CPU 183.17 sec
2016-03-10 18:32:07,949 Stage-1 map = 86%,  reduce = 16%, Cumulative CPU 187.64 sec
2016-03-10 18:32:09,007 Stage-1 map = 87%,  reduce = 24%, Cumulative CPU 192.12 sec
2016-03-10 18:32:11,119 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 195.12 sec
2016-03-10 18:32:13,218 Stage-1 map = 95%,  reduce = 24%, Cumulative CPU 203.35 sec
2016-03-10 18:32:14,265 Stage-1 map = 95%,  reduce = 27%, Cumulative CPU 203.85 sec
2016-03-10 18:32:15,321 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 204.75 sec
2016-03-10 18:32:17,442 Stage-1 map = 95%,  reduce = 29%, Cumulative CPU 205.21 sec
2016-03-10 18:32:18,493 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 210.07 sec
2016-03-10 18:32:20,613 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 223.39 sec
2016-03-10 18:32:21,667 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 225.75 sec
2016-03-10 18:32:22,719 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 229.99 sec
MapReduce Total cumulative CPU time: 3 minutes 49 seconds 990 msec
Ended Job = job_1453192496319_1025379
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 229.99 sec   HDFS Read: 1013319173 HDFS Write: 134 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 49 seconds 990 msec
OK
Time taken: 78.217 seconds, Fetched: 3 row(s)
2016-03-10 18:32:24 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:32:24 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160220
2016-03-10 18:32:24 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:32:24 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:32:25 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:32:25 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:32:25 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:32:25 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:32:26 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:32:26 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:32:26 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:32:26 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1056millis
2016-03-10 18:32:26 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:32:26 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:32:26 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:32:26 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:32:26 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160221.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183237_2f3dc508-6488-4d22-bf0d-0c9832ac7e68
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025397, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025397/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025397
Hadoop job information for Stage-1: number of mappers: 18; number of reducers: 15
2016-03-10 18:32:58,040 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:33:09,644 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 120.76 sec
2016-03-10 18:33:11,750 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 132.11 sec
2016-03-10 18:33:12,804 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 179.48 sec
2016-03-10 18:33:13,850 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 182.84 sec
2016-03-10 18:33:14,894 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 214.44 sec
2016-03-10 18:33:15,939 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 239.57 sec
2016-03-10 18:33:16,986 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 246.63 sec
2016-03-10 18:33:18,032 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 281.98 sec
2016-03-10 18:33:19,076 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 296.75 sec
2016-03-10 18:33:20,121 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 304.68 sec
2016-03-10 18:33:21,165 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 322.8 sec
2016-03-10 18:33:22,207 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 327.64 sec
2016-03-10 18:33:24,301 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 346.23 sec
2016-03-10 18:33:25,344 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 349.28 sec
2016-03-10 18:33:26,388 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 358.23 sec
2016-03-10 18:33:27,432 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 364.74 sec
2016-03-10 18:33:28,487 Stage-1 map = 85%,  reduce = 24%, Cumulative CPU 374.74 sec
2016-03-10 18:33:30,587 Stage-1 map = 87%,  reduce = 24%, Cumulative CPU 395.24 sec
2016-03-10 18:33:31,634 Stage-1 map = 90%,  reduce = 25%, Cumulative CPU 399.86 sec
2016-03-10 18:33:32,683 Stage-1 map = 91%,  reduce = 25%, Cumulative CPU 406.59 sec
2016-03-10 18:33:33,732 Stage-1 map = 92%,  reduce = 25%, Cumulative CPU 409.95 sec
2016-03-10 18:33:34,791 Stage-1 map = 92%,  reduce = 28%, Cumulative CPU 411.73 sec
2016-03-10 18:33:35,841 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 421.17 sec
2016-03-10 18:33:37,933 Stage-1 map = 97%,  reduce = 30%, Cumulative CPU 426.31 sec
2016-03-10 18:33:41,073 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 431.98 sec
2016-03-10 18:33:45,279 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 442.14 sec
2016-03-10 18:33:46,324 Stage-1 map = 100%,  reduce = 68%, Cumulative CPU 458.98 sec
2016-03-10 18:33:47,374 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 478.51 sec
2016-03-10 18:33:48,419 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 484.39 sec
2016-03-10 18:33:49,472 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 498.62 sec
2016-03-10 18:33:52,600 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 501.49 sec
MapReduce Total cumulative CPU time: 8 minutes 21 seconds 490 msec
Ended Job = job_1453192496319_1025397
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 18  Reduce: 15   Cumulative CPU: 501.49 sec   HDFS Read: 2514223690 HDFS Write: 212 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 21 seconds 490 msec
OK
Time taken: 76.833 seconds, Fetched: 4 row(s)
./data/mz_pv_20160221.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183405_1c769b3f-0e9f-4115-9b45-aa7665b55f21
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025428, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025428/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025428
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:34:23,407 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:34:33,985 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 51.88 sec
2016-03-10 18:34:37,138 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 73.24 sec
2016-03-10 18:34:40,289 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 94.84 sec
2016-03-10 18:34:43,438 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 116.54 sec
2016-03-10 18:34:46,591 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 132.24 sec
2016-03-10 18:34:47,640 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 136.3 sec
2016-03-10 18:34:49,750 Stage-1 map = 88%,  reduce = 0%, Cumulative CPU 149.62 sec
2016-03-10 18:34:50,804 Stage-1 map = 94%,  reduce = 0%, Cumulative CPU 150.95 sec
2016-03-10 18:34:52,921 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 154.19 sec
2016-03-10 18:34:53,971 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 171.81 sec
2016-03-10 18:34:55,028 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 172.8 sec
MapReduce Total cumulative CPU time: 2 minutes 52 seconds 800 msec
Ended Job = job_1453192496319_1025428
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 172.8 sec   HDFS Read: 996599286 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 52 seconds 800 msec
OK
Time taken: 50.526 seconds, Fetched: 3 row(s)
2016-03-10 18:34:57 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:34:57 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160221
2016-03-10 18:34:57 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:34:57 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:34:57 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:34:57 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:34:57 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:34:57 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:34:58 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:34:58 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:34:58 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:34:58 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1082millis
2016-03-10 18:34:58 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:34:58 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:34:58 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:34:58 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:34:58 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160222.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183509_b806dbdd-4ea4-4559-8a3c-ce363f072570
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025453, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025453/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025453
Hadoop job information for Stage-1: number of mappers: 17; number of reducers: 15
2016-03-10 18:35:27,291 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:35:37,903 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 59.27 sec
2016-03-10 18:35:41,070 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 143.69 sec
2016-03-10 18:35:42,140 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 160.44 sec
2016-03-10 18:35:44,242 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 202.76 sec
2016-03-10 18:35:45,294 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 216.78 sec
2016-03-10 18:35:46,348 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 224.73 sec
2016-03-10 18:35:47,602 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 276.16 sec
2016-03-10 18:35:49,717 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 290.8 sec
2016-03-10 18:35:50,771 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 312.27 sec
2016-03-10 18:35:52,888 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 324.42 sec
2016-03-10 18:35:53,940 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 343.66 sec
2016-03-10 18:35:54,992 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 347.14 sec
2016-03-10 18:35:56,055 Stage-1 map = 75%,  reduce = 9%, Cumulative CPU 358.71 sec
2016-03-10 18:35:57,120 Stage-1 map = 79%,  reduce = 17%, Cumulative CPU 379.62 sec
2016-03-10 18:35:58,169 Stage-1 map = 86%,  reduce = 17%, Cumulative CPU 385.34 sec
2016-03-10 18:35:59,225 Stage-1 map = 88%,  reduce = 20%, Cumulative CPU 392.53 sec
2016-03-10 18:36:00,275 Stage-1 map = 88%,  reduce = 22%, Cumulative CPU 399.12 sec
2016-03-10 18:36:01,322 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 404.87 sec
2016-03-10 18:36:02,378 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 411.4 sec
2016-03-10 18:36:03,431 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 414.78 sec
2016-03-10 18:36:04,498 Stage-1 map = 97%,  reduce = 29%, Cumulative CPU 418.58 sec
2016-03-10 18:36:05,557 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 419.86 sec
2016-03-10 18:36:10,811 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 434.37 sec
2016-03-10 18:36:11,860 Stage-1 map = 100%,  reduce = 64%, Cumulative CPU 445.52 sec
2016-03-10 18:36:12,903 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 464.96 sec
2016-03-10 18:36:13,945 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 471.44 sec
MapReduce Total cumulative CPU time: 7 minutes 51 seconds 440 msec
Ended Job = job_1453192496319_1025453
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 17  Reduce: 15   Cumulative CPU: 471.44 sec   HDFS Read: 2470817144 HDFS Write: 215 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 51 seconds 440 msec
OK
Time taken: 66.614 seconds, Fetched: 4 row(s)
./data/mz_pv_20160222.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183627_c603f07a-ccd1-4960-a324-db3b9e0fc13e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025474, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025474/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025474
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 18:36:48,231 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:36:58,868 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 14.36 sec
2016-03-10 18:36:59,926 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 43.68 sec
2016-03-10 18:37:02,056 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 58.25 sec
2016-03-10 18:37:03,115 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 60.6 sec
2016-03-10 18:37:05,227 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 77.43 sec
2016-03-10 18:37:08,377 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 92.02 sec
2016-03-10 18:37:10,489 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 96.97 sec
2016-03-10 18:37:11,546 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 111.71 sec
2016-03-10 18:37:13,777 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 114.38 sec
2016-03-10 18:37:14,827 Stage-1 map = 77%,  reduce = 0%, Cumulative CPU 128.17 sec
2016-03-10 18:37:15,885 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 129.76 sec
2016-03-10 18:37:18,005 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 135.73 sec
2016-03-10 18:37:19,057 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 137.32 sec
2016-03-10 18:37:20,109 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 140.09 sec
2016-03-10 18:37:21,162 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 144.06 sec
2016-03-10 18:37:22,219 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 151.84 sec
2016-03-10 18:37:23,279 Stage-1 map = 100%,  reduce = 78%, Cumulative CPU 156.78 sec
2016-03-10 18:37:24,335 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 161.54 sec
2016-03-10 18:37:25,395 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 163.94 sec
MapReduce Total cumulative CPU time: 2 minutes 43 seconds 940 msec
Ended Job = job_1453192496319_1025474
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 163.94 sec   HDFS Read: 914703945 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 43 seconds 940 msec
OK
Time taken: 59.213 seconds, Fetched: 3 row(s)
2016-03-10 18:37:27 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:37:27 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160222
2016-03-10 18:37:27 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:37:27 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:37:27 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:37:27 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:37:27 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:37:27 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:37:28 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:37:28 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:37:28 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:37:28 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1147millis
2016-03-10 18:37:29 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:37:29 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:37:29 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:37:29 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:37:29 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160223.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183739_c857a631-a9a1-409d-82ba-408838cc1b76
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025488, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025488/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025488
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 14
2016-03-10 18:37:56,823 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:38:07,418 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 97.45 sec
2016-03-10 18:38:10,576 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 138.55 sec
2016-03-10 18:38:13,728 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 178.76 sec
2016-03-10 18:38:15,847 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 181.49 sec
2016-03-10 18:38:16,905 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 219.89 sec
2016-03-10 18:38:19,089 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 222.42 sec
2016-03-10 18:38:20,141 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 253.32 sec
2016-03-10 18:38:22,256 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 259.16 sec
2016-03-10 18:38:23,312 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 283.67 sec
2016-03-10 18:38:26,493 Stage-1 map = 71%,  reduce = 5%, Cumulative CPU 313.26 sec
2016-03-10 18:38:27,551 Stage-1 map = 71%,  reduce = 12%, Cumulative CPU 317.0 sec
2016-03-10 18:38:28,608 Stage-1 map = 72%,  reduce = 12%, Cumulative CPU 326.26 sec
2016-03-10 18:38:29,662 Stage-1 map = 77%,  reduce = 14%, Cumulative CPU 341.11 sec
2016-03-10 18:38:30,721 Stage-1 map = 84%,  reduce = 14%, Cumulative CPU 344.77 sec
2016-03-10 18:38:31,785 Stage-1 map = 94%,  reduce = 16%, Cumulative CPU 358.0 sec
2016-03-10 18:38:32,846 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 359.58 sec
2016-03-10 18:38:33,894 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 359.62 sec
2016-03-10 18:38:34,943 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 362.87 sec
2016-03-10 18:38:35,998 Stage-1 map = 100%,  reduce = 57%, Cumulative CPU 366.22 sec
2016-03-10 18:38:37,052 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 389.54 sec
MapReduce Total cumulative CPU time: 6 minutes 29 seconds 540 msec
Ended Job = job_1453192496319_1025488
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 13  Reduce: 14   Cumulative CPU: 392.23 sec   HDFS Read: 2344708649 HDFS Write: 216 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 32 seconds 230 msec
OK
Time taken: 59.41 seconds, Fetched: 4 row(s)
./data/mz_pv_20160223.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183850_69cddd1b-a561-4132-979e-f57f7b77a0ee
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025506, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025506/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025506
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 6
2016-03-10 18:39:07,073 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:39:16,595 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 41.2 sec
2016-03-10 18:39:17,654 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 48.44 sec
2016-03-10 18:39:19,759 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 70.31 sec
2016-03-10 18:39:21,877 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 71.99 sec
2016-03-10 18:39:22,938 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 89.14 sec
2016-03-10 18:39:25,043 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 92.03 sec
2016-03-10 18:39:26,097 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 105.35 sec
2016-03-10 18:39:27,149 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 106.27 sec
2016-03-10 18:39:29,249 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 117.35 sec
2016-03-10 18:39:32,430 Stage-1 map = 94%,  reduce = 0%, Cumulative CPU 127.71 sec
2016-03-10 18:39:33,501 Stage-1 map = 100%,  reduce = 16%, Cumulative CPU 130.69 sec
2016-03-10 18:39:34,566 Stage-1 map = 100%,  reduce = 58%, Cumulative CPU 134.68 sec
2016-03-10 18:39:35,624 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 145.45 sec
2016-03-10 18:39:41,129 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 152.66 sec
MapReduce Total cumulative CPU time: 2 minutes 32 seconds 660 msec
Ended Job = job_1453192496319_1025506
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 6   Cumulative CPU: 152.66 sec   HDFS Read: 899826201 HDFS Write: 126 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 32 seconds 660 msec
OK
Time taken: 52.083 seconds, Fetched: 3 row(s)
2016-03-10 18:39:43 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:39:43 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160223
2016-03-10 18:39:43 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:39:43 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:39:43 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:39:43 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:39:43 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:39:43 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:39:44 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:39:44 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:39:44 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:39:44 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1044millis
2016-03-10 18:39:44 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:39:44 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:39:44 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:39:44 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:39:44 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160224.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310183955_87910725-04db-431a-af9f-852932eb05a8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025524, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025524/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025524
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 13
2016-03-10 18:40:13,248 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:40:23,873 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 91.67 sec
2016-03-10 18:40:27,030 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 133.41 sec
2016-03-10 18:40:28,080 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 140.9 sec
2016-03-10 18:40:30,184 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 180.31 sec
2016-03-10 18:40:31,231 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 185.26 sec
2016-03-10 18:40:32,280 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 187.63 sec
2016-03-10 18:40:33,561 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 224.89 sec
2016-03-10 18:40:35,667 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 233.65 sec
2016-03-10 18:40:36,729 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 259.27 sec
2016-03-10 18:40:38,837 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 265.2 sec
2016-03-10 18:40:39,903 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 280.82 sec
2016-03-10 18:40:42,018 Stage-1 map = 78%,  reduce = 10%, Cumulative CPU 300.08 sec
2016-03-10 18:40:43,076 Stage-1 map = 78%,  reduce = 18%, Cumulative CPU 305.4 sec
2016-03-10 18:40:44,125 Stage-1 map = 88%,  reduce = 18%, Cumulative CPU 312.99 sec
2016-03-10 18:40:45,181 Stage-1 map = 89%,  reduce = 23%, Cumulative CPU 321.29 sec
2016-03-10 18:40:46,244 Stage-1 map = 93%,  reduce = 25%, Cumulative CPU 326.66 sec
2016-03-10 18:40:48,338 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 331.73 sec
2016-03-10 18:40:51,502 Stage-1 map = 96%,  reduce = 31%, Cumulative CPU 336.16 sec
2016-03-10 18:40:52,553 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 339.58 sec
2016-03-10 18:40:53,603 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 360.15 sec
2016-03-10 18:40:54,648 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 366.3 sec
2016-03-10 18:40:55,695 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 367.6 sec
MapReduce Total cumulative CPU time: 6 minutes 7 seconds 600 msec
Ended Job = job_1453192496319_1025524
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 14  Reduce: 13   Cumulative CPU: 367.6 sec   HDFS Read: 2211620167 HDFS Write: 210 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 7 seconds 600 msec
OK
Time taken: 61.349 seconds, Fetched: 4 row(s)
./data/mz_pv_20160224.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184108_90c08235-63b4-4e9d-a4c0-62d5f6b72462
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025548, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025548/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025548
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 18:41:24,580 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:41:35,177 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 36.19 sec
2016-03-10 18:41:36,227 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 53.68 sec
2016-03-10 18:41:37,278 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 59.84 sec
2016-03-10 18:41:38,329 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 69.21 sec
2016-03-10 18:41:39,389 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 72.48 sec
2016-03-10 18:41:40,436 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 78.5 sec
2016-03-10 18:41:41,480 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 87.85 sec
2016-03-10 18:41:42,534 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 89.99 sec
2016-03-10 18:41:43,595 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 102.79 sec
2016-03-10 18:41:45,720 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 109.16 sec
2016-03-10 18:41:46,994 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 114.64 sec
2016-03-10 18:41:49,089 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 117.73 sec
2016-03-10 18:41:50,145 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 123.21 sec
2016-03-10 18:41:52,243 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 127.15 sec
2016-03-10 18:41:53,292 Stage-1 map = 91%,  reduce = 0%, Cumulative CPU 132.77 sec
2016-03-10 18:41:54,349 Stage-1 map = 91%,  reduce = 9%, Cumulative CPU 133.48 sec
2016-03-10 18:41:55,420 Stage-1 map = 91%,  reduce = 23%, Cumulative CPU 134.56 sec
2016-03-10 18:41:56,472 Stage-1 map = 93%,  reduce = 23%, Cumulative CPU 137.39 sec
2016-03-10 18:41:58,572 Stage-1 map = 100%,  reduce = 28%, Cumulative CPU 140.84 sec
2016-03-10 18:41:59,623 Stage-1 map = 100%,  reduce = 64%, Cumulative CPU 145.88 sec
2016-03-10 18:42:00,678 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 154.44 sec
MapReduce Total cumulative CPU time: 2 minutes 34 seconds 440 msec
Ended Job = job_1453192496319_1025548
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 154.44 sec   HDFS Read: 855570344 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 34 seconds 440 msec
OK
Time taken: 53.303 seconds, Fetched: 3 row(s)
2016-03-10 18:42:02 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:42:02 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160224
2016-03-10 18:42:02 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:42:02 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:42:02 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:42:02 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:42:02 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:42:02 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:42:03 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:42:03 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:42:04 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:42:04 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1026millis
2016-03-10 18:42:04 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:42:04 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:42:04 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:42:04 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:42:04 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160225.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184214_4ac1cd16-c7ae-4a78-ab60-4cf19055ff12
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025564, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025564/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025564
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 14
2016-03-10 18:42:32,187 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:42:42,759 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 51.33 sec
2016-03-10 18:42:44,866 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 140.16 sec
2016-03-10 18:42:45,921 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 174.93 sec
2016-03-10 18:42:46,968 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 181.49 sec
2016-03-10 18:42:48,019 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 187.83 sec
2016-03-10 18:42:49,073 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 221.68 sec
2016-03-10 18:42:50,123 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 239.02 sec
2016-03-10 18:42:51,177 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 247.75 sec
2016-03-10 18:42:52,232 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 276.87 sec
2016-03-10 18:42:53,279 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 281.94 sec
2016-03-10 18:42:54,329 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 288.45 sec
2016-03-10 18:42:55,380 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 313.74 sec
2016-03-10 18:42:56,442 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 325.46 sec
2016-03-10 18:42:57,502 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 331.87 sec
2016-03-10 18:42:58,571 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 347.06 sec
2016-03-10 18:42:59,636 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 356.66 sec
2016-03-10 18:43:00,685 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 362.69 sec
2016-03-10 18:43:01,794 Stage-1 map = 71%,  reduce = 1%, Cumulative CPU 375.06 sec
2016-03-10 18:43:02,858 Stage-1 map = 71%,  reduce = 15%, Cumulative CPU 383.3 sec
2016-03-10 18:43:03,915 Stage-1 map = 73%,  reduce = 16%, Cumulative CPU 426.61 sec
2016-03-10 18:43:04,965 Stage-1 map = 73%,  reduce = 17%, Cumulative CPU 433.42 sec
2016-03-10 18:43:06,016 Stage-1 map = 77%,  reduce = 17%, Cumulative CPU 439.43 sec
2016-03-10 18:43:07,062 Stage-1 map = 82%,  reduce = 17%, Cumulative CPU 453.17 sec
2016-03-10 18:43:08,121 Stage-1 map = 83%,  reduce = 19%, Cumulative CPU 459.65 sec
2016-03-10 18:43:09,172 Stage-1 map = 83%,  reduce = 21%, Cumulative CPU 460.35 sec
2016-03-10 18:43:10,229 Stage-1 map = 87%,  reduce = 21%, Cumulative CPU 470.91 sec
2016-03-10 18:43:11,281 Stage-1 map = 90%,  reduce = 25%, Cumulative CPU 477.95 sec
2016-03-10 18:43:12,336 Stage-1 map = 100%,  reduce = 26%, Cumulative CPU 485.13 sec
2016-03-10 18:43:13,392 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 485.88 sec
2016-03-10 18:43:14,446 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 509.43 sec
2016-03-10 18:43:15,492 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 522.67 sec
2016-03-10 18:43:17,591 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 525.01 sec
MapReduce Total cumulative CPU time: 8 minutes 45 seconds 10 msec
Ended Job = job_1453192496319_1025564
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 14  Reduce: 14   Cumulative CPU: 525.01 sec   HDFS Read: 2348338622 HDFS Write: 218 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 45 seconds 10 msec
OK
Time taken: 63.875 seconds, Fetched: 4 row(s)
./data/mz_pv_20160225.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184330_e5a3bd41-9ea5-42e1-97dd-f869229d9396
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025577, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025577/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025577
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 6
2016-03-10 18:43:46,625 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:43:56,137 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 6.27 sec
2016-03-10 18:43:57,197 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 38.41 sec
2016-03-10 18:43:59,305 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 47.81 sec
2016-03-10 18:44:00,365 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 52.59 sec
2016-03-10 18:44:02,469 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 61.84 sec
2016-03-10 18:44:03,515 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 68.02 sec
2016-03-10 18:44:05,612 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 76.46 sec
2016-03-10 18:44:06,657 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 82.2 sec
2016-03-10 18:44:08,752 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 90.7 sec
2016-03-10 18:44:09,791 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 96.14 sec
2016-03-10 18:44:11,896 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 104.81 sec
2016-03-10 18:44:12,946 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 112.3 sec
2016-03-10 18:44:13,997 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 115.01 sec
2016-03-10 18:44:15,048 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 120.64 sec
2016-03-10 18:44:16,098 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 124.63 sec
2016-03-10 18:44:18,197 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 127.59 sec
2016-03-10 18:44:19,247 Stage-1 map = 92%,  reduce = 0%, Cumulative CPU 132.29 sec
2016-03-10 18:44:21,360 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 134.11 sec
2016-03-10 18:44:22,427 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 138.88 sec
2016-03-10 18:44:23,490 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 144.49 sec
2016-03-10 18:44:24,540 Stage-1 map = 100%,  reduce = 78%, Cumulative CPU 147.61 sec
2016-03-10 18:44:26,638 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 150.74 sec
2016-03-10 18:44:28,740 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 157.32 sec
2016-03-10 18:44:29,789 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 163.94 sec
MapReduce Total cumulative CPU time: 2 minutes 43 seconds 940 msec
Ended Job = job_1453192496319_1025577
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 5  Reduce: 6   Cumulative CPU: 163.94 sec   HDFS Read: 844054917 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 43 seconds 940 msec
OK
Time taken: 60.652 seconds, Fetched: 3 row(s)
2016-03-10 18:44:31 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:44:31 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160225
2016-03-10 18:44:31 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:44:31 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:44:32 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:44:32 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:44:32 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:44:32 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:44:33 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:44:33 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:44:33 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:44:33 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1179millis
2016-03-10 18:44:33 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:44:33 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:44:33 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:44:33 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:44:33 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160226.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184444_964521ee-cf32-40f6-8d04-b1c7f17abb82
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025584, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025584/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025584
Hadoop job information for Stage-1: number of mappers: 16; number of reducers: 14
2016-03-10 18:45:04,813 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:45:15,360 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 35.12 sec
2016-03-10 18:45:18,514 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 155.36 sec
2016-03-10 18:45:19,568 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 169.16 sec
2016-03-10 18:45:20,623 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 182.51 sec
2016-03-10 18:45:21,677 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 210.32 sec
2016-03-10 18:45:22,727 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 233.74 sec
2016-03-10 18:45:23,778 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 258.79 sec
2016-03-10 18:45:24,830 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 281.96 sec
2016-03-10 18:45:25,878 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 290.51 sec
2016-03-10 18:45:26,930 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 304.35 sec
2016-03-10 18:45:27,981 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 317.79 sec
2016-03-10 18:45:29,034 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 327.19 sec
2016-03-10 18:45:30,100 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 333.39 sec
2016-03-10 18:45:31,152 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 348.71 sec
2016-03-10 18:45:32,205 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 358.16 sec
2016-03-10 18:45:33,264 Stage-1 map = 67%,  reduce = 2%, Cumulative CPU 363.64 sec
2016-03-10 18:45:34,330 Stage-1 map = 74%,  reduce = 9%, Cumulative CPU 381.59 sec
2016-03-10 18:45:36,442 Stage-1 map = 78%,  reduce = 10%, Cumulative CPU 397.43 sec
2016-03-10 18:45:37,491 Stage-1 map = 82%,  reduce = 14%, Cumulative CPU 411.52 sec
2016-03-10 18:45:38,546 Stage-1 map = 82%,  reduce = 18%, Cumulative CPU 421.23 sec
2016-03-10 18:45:39,601 Stage-1 map = 83%,  reduce = 22%, Cumulative CPU 427.38 sec
2016-03-10 18:45:40,654 Stage-1 map = 86%,  reduce = 23%, Cumulative CPU 434.8 sec
2016-03-10 18:45:41,711 Stage-1 map = 89%,  reduce = 23%, Cumulative CPU 442.38 sec
2016-03-10 18:45:42,770 Stage-1 map = 92%,  reduce = 26%, Cumulative CPU 449.54 sec
2016-03-10 18:45:43,824 Stage-1 map = 92%,  reduce = 27%, Cumulative CPU 451.34 sec
2016-03-10 18:45:44,878 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 458.68 sec
2016-03-10 18:45:49,073 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 470.91 sec
2016-03-10 18:45:50,127 Stage-1 map = 100%,  reduce = 43%, Cumulative CPU 474.98 sec
2016-03-10 18:45:51,178 Stage-1 map = 100%,  reduce = 76%, Cumulative CPU 489.08 sec
2016-03-10 18:45:52,235 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 493.86 sec
2016-03-10 18:45:53,284 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 519.93 sec
2016-03-10 18:45:54,335 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 520.69 sec
2016-03-10 18:45:55,386 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 524.51 sec
MapReduce Total cumulative CPU time: 8 minutes 44 seconds 510 msec
Ended Job = job_1453192496319_1025584
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 16  Reduce: 14   Cumulative CPU: 524.51 sec   HDFS Read: 2380762186 HDFS Write: 218 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 44 seconds 510 msec
OK
Time taken: 72.353 seconds, Fetched: 4 row(s)
./data/mz_pv_20160226.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184607_7194b0cf-5609-496a-9fdd-53f7ae85d338
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025592, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025592/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025592
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 18:46:31,487 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:46:40,984 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 13.98 sec
2016-03-10 18:46:44,136 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 54.49 sec
2016-03-10 18:46:45,196 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 96.65 sec
2016-03-10 18:46:47,303 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 111.76 sec
2016-03-10 18:46:48,356 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 115.04 sec
2016-03-10 18:46:50,472 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 130.08 sec
2016-03-10 18:46:51,520 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 133.05 sec
2016-03-10 18:46:52,595 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 135.56 sec
2016-03-10 18:46:53,656 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 144.35 sec
2016-03-10 18:46:54,708 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 147.36 sec
2016-03-10 18:46:56,893 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 157.87 sec
2016-03-10 18:46:57,951 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 159.84 sec
2016-03-10 18:47:00,061 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 168.86 sec
2016-03-10 18:47:01,125 Stage-1 map = 82%,  reduce = 14%, Cumulative CPU 172.1 sec
2016-03-10 18:47:03,237 Stage-1 map = 86%,  reduce = 18%, Cumulative CPU 179.76 sec
2016-03-10 18:47:04,296 Stage-1 map = 93%,  reduce = 22%, Cumulative CPU 182.32 sec
2016-03-10 18:47:06,404 Stage-1 map = 100%,  reduce = 22%, Cumulative CPU 185.75 sec
2016-03-10 18:47:07,463 Stage-1 map = 100%,  reduce = 73%, Cumulative CPU 192.79 sec
2016-03-10 18:47:08,517 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 200.45 sec
MapReduce Total cumulative CPU time: 3 minutes 20 seconds 450 msec
Ended Job = job_1453192496319_1025592
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 200.45 sec   HDFS Read: 911326306 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 20 seconds 450 msec
OK
Time taken: 61.84 seconds, Fetched: 3 row(s)
2016-03-10 18:47:10 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:47:10 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160226
2016-03-10 18:47:10 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:47:10 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:47:10 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:47:10 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:47:10 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:47:10 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:47:11 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:47:11 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:47:11 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:47:11 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1133millis
2016-03-10 18:47:12 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:47:12 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:47:12 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:47:12 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:47:12 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160227.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184723_2a0018b8-3fbd-42c1-9426-37a53760b8cc
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025602, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025602/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025602
Hadoop job information for Stage-1: number of mappers: 20; number of reducers: 15
2016-03-10 18:47:38,788 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:47:49,356 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 116.95 sec
2016-03-10 18:47:52,541 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 196.05 sec
2016-03-10 18:47:53,585 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 213.22 sec
2016-03-10 18:47:55,686 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 263.16 sec
2016-03-10 18:47:56,734 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 269.39 sec
2016-03-10 18:47:57,789 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 284.57 sec
2016-03-10 18:47:58,843 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 317.11 sec
2016-03-10 18:47:59,896 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 323.38 sec
2016-03-10 18:48:00,944 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 328.39 sec
2016-03-10 18:48:01,994 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 353.14 sec
2016-03-10 18:48:04,107 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 369.57 sec
2016-03-10 18:48:05,163 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 382.13 sec
2016-03-10 18:48:06,228 Stage-1 map = 73%,  reduce = 13%, Cumulative CPU 390.18 sec
2016-03-10 18:48:07,286 Stage-1 map = 78%,  reduce = 13%, Cumulative CPU 408.0 sec
2016-03-10 18:48:08,346 Stage-1 map = 82%,  reduce = 14%, Cumulative CPU 416.93 sec
2016-03-10 18:48:09,407 Stage-1 map = 82%,  reduce = 18%, Cumulative CPU 419.05 sec
2016-03-10 18:48:10,455 Stage-1 map = 82%,  reduce = 19%, Cumulative CPU 429.25 sec
2016-03-10 18:48:11,499 Stage-1 map = 85%,  reduce = 23%, Cumulative CPU 437.36 sec
2016-03-10 18:48:12,550 Stage-1 map = 88%,  reduce = 25%, Cumulative CPU 453.41 sec
2016-03-10 18:48:13,598 Stage-1 map = 92%,  reduce = 25%, Cumulative CPU 458.12 sec
2016-03-10 18:48:14,654 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 460.6 sec
2016-03-10 18:48:15,706 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 464.2 sec
2016-03-10 18:48:18,852 Stage-1 map = 98%,  reduce = 30%, Cumulative CPU 472.62 sec
2016-03-10 18:48:19,907 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 475.61 sec
2016-03-10 18:48:20,956 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 496.35 sec
2016-03-10 18:48:22,011 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 505.5 sec
MapReduce Total cumulative CPU time: 8 minutes 25 seconds 500 msec
Ended Job = job_1453192496319_1025602
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 20  Reduce: 15   Cumulative CPU: 505.5 sec   HDFS Read: 2628691621 HDFS Write: 216 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 25 seconds 500 msec
OK
Time taken: 60.172 seconds, Fetched: 4 row(s)
./data/mz_pv_20160227.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310184834_ce9fc7b6-62f3-40ee-b95a-1ecec5bc837c
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025616, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025616/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025616
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2016-03-10 18:48:54,863 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:49:05,574 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 32.9 sec
2016-03-10 18:49:06,632 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 39.78 sec
2016-03-10 18:49:08,743 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 76.75 sec
2016-03-10 18:49:09,797 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 81.44 sec
2016-03-10 18:49:11,902 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 97.11 sec
2016-03-10 18:49:14,003 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 99.27 sec
2016-03-10 18:49:15,053 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 113.42 sec
2016-03-10 18:49:17,153 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 116.23 sec
2016-03-10 18:49:18,205 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 125.82 sec
2016-03-10 18:49:20,325 Stage-1 map = 65%,  reduce = 6%, Cumulative CPU 126.45 sec
2016-03-10 18:49:21,387 Stage-1 map = 70%,  reduce = 12%, Cumulative CPU 137.49 sec
2016-03-10 18:49:22,441 Stage-1 map = 70%,  reduce = 14%, Cumulative CPU 138.31 sec
2016-03-10 18:49:23,497 Stage-1 map = 76%,  reduce = 16%, Cumulative CPU 141.04 sec
2016-03-10 18:49:24,553 Stage-1 map = 83%,  reduce = 16%, Cumulative CPU 148.54 sec
2016-03-10 18:49:25,616 Stage-1 map = 83%,  reduce = 17%, Cumulative CPU 148.75 sec
2016-03-10 18:49:26,667 Stage-1 map = 83%,  reduce = 24%, Cumulative CPU 149.27 sec
2016-03-10 18:49:27,722 Stage-1 map = 86%,  reduce = 24%, Cumulative CPU 153.1 sec
2016-03-10 18:49:30,880 Stage-1 map = 92%,  reduce = 24%, Cumulative CPU 156.86 sec
2016-03-10 18:49:32,993 Stage-1 map = 92%,  reduce = 29%, Cumulative CPU 157.47 sec
2016-03-10 18:49:42,419 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 161.0 sec
2016-03-10 18:49:45,577 Stage-1 map = 94%,  reduce = 29%, Cumulative CPU 164.61 sec
2016-03-10 18:49:46,633 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 166.52 sec
2016-03-10 18:49:47,683 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 173.27 sec
2016-03-10 18:49:48,730 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 182.47 sec
MapReduce Total cumulative CPU time: 3 minutes 2 seconds 470 msec
Ended Job = job_1453192496319_1025616
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 182.47 sec   HDFS Read: 1009469568 HDFS Write: 137 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 2 seconds 470 msec
OK
Time taken: 75.335 seconds, Fetched: 3 row(s)
2016-03-10 18:49:50 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:49:50 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160227
2016-03-10 18:49:50 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:49:50 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:49:50 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:49:51 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:49:51 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:49:51 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:49:52 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:49:52 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:49:52 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:49:52 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1081millis
2016-03-10 18:49:52 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:49:52 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:49:52 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:49:52 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:49:52 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160228.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185003_2e3f8aad-8ce8-4827-a7b3-efb97ebc15d5
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025630, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025630/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025630
Hadoop job information for Stage-1: number of mappers: 20; number of reducers: 15
2016-03-10 18:50:20,114 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:50:30,644 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 134.64 sec
2016-03-10 18:50:33,790 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 203.61 sec
2016-03-10 18:50:35,901 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 209.52 sec
2016-03-10 18:50:36,951 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 273.23 sec
2016-03-10 18:50:38,002 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 282.0 sec
2016-03-10 18:50:39,054 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 292.62 sec
2016-03-10 18:50:40,101 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 311.93 sec
2016-03-10 18:50:43,268 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 330.49 sec
2016-03-10 18:50:44,323 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 333.6 sec
2016-03-10 18:50:45,381 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 338.99 sec
2016-03-10 18:50:46,444 Stage-1 map = 84%,  reduce = 17%, Cumulative CPU 351.6 sec
2016-03-10 18:50:47,505 Stage-1 map = 87%,  reduce = 22%, Cumulative CPU 358.61 sec
2016-03-10 18:50:48,551 Stage-1 map = 90%,  reduce = 22%, Cumulative CPU 389.81 sec
2016-03-10 18:50:49,603 Stage-1 map = 92%,  reduce = 27%, Cumulative CPU 392.83 sec
2016-03-10 18:50:50,655 Stage-1 map = 93%,  reduce = 27%, Cumulative CPU 396.46 sec
2016-03-10 18:50:51,714 Stage-1 map = 95%,  reduce = 27%, Cumulative CPU 399.53 sec
2016-03-10 18:50:52,763 Stage-1 map = 95%,  reduce = 30%, Cumulative CPU 401.28 sec
2016-03-10 18:50:55,935 Stage-1 map = 100%,  reduce = 36%, Cumulative CPU 416.28 sec
2016-03-10 18:50:56,991 Stage-1 map = 100%,  reduce = 41%, Cumulative CPU 419.21 sec
2016-03-10 18:50:58,045 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 441.23 sec
2016-03-10 18:50:59,096 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 444.21 sec
MapReduce Total cumulative CPU time: 7 minutes 24 seconds 210 msec
Ended Job = job_1453192496319_1025630
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 20  Reduce: 15   Cumulative CPU: 444.21 sec   HDFS Read: 2573338564 HDFS Write: 215 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 24 seconds 210 msec
OK
Time taken: 56.945 seconds, Fetched: 4 row(s)
./data/mz_pv_20160228.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185111_8cfa167e-8973-43b0-bc20-ea43c7eaee44
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025643, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025643/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025643
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 7
2016-03-10 18:51:28,342 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:51:37,909 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 36.8 sec
2016-03-10 18:51:41,080 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 52.36 sec
2016-03-10 18:51:44,478 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 83.07 sec
2016-03-10 18:51:47,644 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 105.8 sec
2016-03-10 18:51:50,786 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 123.75 sec
2016-03-10 18:51:51,835 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 126.03 sec
2016-03-10 18:51:53,952 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 142.41 sec
2016-03-10 18:51:54,998 Stage-1 map = 88%,  reduce = 0%, Cumulative CPU 148.77 sec
2016-03-10 18:51:56,042 Stage-1 map = 89%,  reduce = 0%, Cumulative CPU 151.79 sec
2016-03-10 18:51:59,197 Stage-1 map = 91%,  reduce = 0%, Cumulative CPU 154.73 sec
2016-03-10 18:52:02,341 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 157.7 sec
2016-03-10 18:52:03,404 Stage-1 map = 93%,  reduce = 28%, Cumulative CPU 160.09 sec
2016-03-10 18:52:05,523 Stage-1 map = 100%,  reduce = 38%, Cumulative CPU 164.81 sec
2016-03-10 18:52:06,583 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 175.32 sec
2016-03-10 18:52:07,629 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 178.27 sec
MapReduce Total cumulative CPU time: 2 minutes 58 seconds 270 msec
Ended Job = job_1453192496319_1025643
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 7   Cumulative CPU: 178.27 sec   HDFS Read: 1027431935 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 58 seconds 270 msec
OK
Time taken: 57.419 seconds, Fetched: 3 row(s)
2016-03-10 18:52:09 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:52:09 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160228
2016-03-10 18:52:09 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:52:09 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:52:10 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:52:10 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:52:10 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:52:10 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:52:11 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:52:11 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:52:11 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:52:11 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1144millis
2016-03-10 18:52:11 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:52:11 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:52:11 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:52:11 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:52:11 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160229.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185222_dba540c5-a713-4a90-8f95-6a58a8a2b096
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025654, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025654/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025654
Hadoop job information for Stage-1: number of mappers: 15; number of reducers: 13
2016-03-10 18:52:39,651 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:52:50,264 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 87.78 sec
2016-03-10 18:52:51,311 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 88.7 sec
2016-03-10 18:52:53,416 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 124.85 sec
2016-03-10 18:52:56,562 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 168.8 sec
2016-03-10 18:52:57,614 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 172.62 sec
2016-03-10 18:52:58,665 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 189.32 sec
2016-03-10 18:52:59,721 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 211.62 sec
2016-03-10 18:53:00,768 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 219.51 sec
2016-03-10 18:53:01,825 Stage-1 map = 64%,  reduce = 16%, Cumulative CPU 242.62 sec
2016-03-10 18:53:02,875 Stage-1 map = 70%,  reduce = 16%, Cumulative CPU 254.57 sec
2016-03-10 18:53:05,013 Stage-1 map = 71%,  reduce = 20%, Cumulative CPU 262.79 sec
2016-03-10 18:53:06,074 Stage-1 map = 72%,  reduce = 20%, Cumulative CPU 271.64 sec
2016-03-10 18:53:07,130 Stage-1 map = 79%,  reduce = 20%, Cumulative CPU 275.87 sec
2016-03-10 18:53:08,190 Stage-1 map = 83%,  reduce = 24%, Cumulative CPU 289.87 sec
2016-03-10 18:53:10,282 Stage-1 map = 84%,  reduce = 26%, Cumulative CPU 294.56 sec
2016-03-10 18:53:11,333 Stage-1 map = 84%,  reduce = 27%, Cumulative CPU 298.1 sec
2016-03-10 18:53:13,438 Stage-1 map = 85%,  reduce = 27%, Cumulative CPU 303.6 sec
2016-03-10 18:53:16,621 Stage-1 map = 86%,  reduce = 27%, Cumulative CPU 313.73 sec
2016-03-10 18:53:17,682 Stage-1 map = 87%,  reduce = 27%, Cumulative CPU 316.89 sec
2016-03-10 18:53:19,779 Stage-1 map = 89%,  reduce = 27%, Cumulative CPU 326.81 sec
2016-03-10 18:53:21,870 Stage-1 map = 95%,  reduce = 27%, Cumulative CPU 331.39 sec
2016-03-10 18:53:22,914 Stage-1 map = 96%,  reduce = 31%, Cumulative CPU 335.72 sec
2016-03-10 18:53:28,195 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 343.27 sec
2016-03-10 18:53:32,439 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 351.68 sec
2016-03-10 18:53:33,487 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 364.15 sec
2016-03-10 18:53:34,537 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 371.29 sec
MapReduce Total cumulative CPU time: 6 minutes 11 seconds 290 msec
Ended Job = job_1453192496319_1025654
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 15  Reduce: 13   Cumulative CPU: 371.29 sec   HDFS Read: 2215368377 HDFS Write: 209 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 11 seconds 290 msec
OK
Time taken: 73.428 seconds, Fetched: 4 row(s)
./data/mz_pv_20160229.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185346_b90f3e8e-e4d6-4a35-b9b2-f6662c78b64f
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025664, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025664/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025664
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 5
2016-03-10 18:54:05,615 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:54:16,149 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 34.6 sec
2016-03-10 18:54:17,200 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 41.75 sec
2016-03-10 18:54:19,308 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 53.18 sec
2016-03-10 18:54:20,366 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 56.11 sec
2016-03-10 18:54:22,462 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 67.17 sec
2016-03-10 18:54:23,510 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 68.21 sec
2016-03-10 18:54:25,615 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 76.95 sec
2016-03-10 18:54:27,726 Stage-1 map = 58%,  reduce = 9%, Cumulative CPU 78.05 sec
2016-03-10 18:54:28,783 Stage-1 map = 64%,  reduce = 11%, Cumulative CPU 90.47 sec
2016-03-10 18:54:29,834 Stage-1 map = 71%,  reduce = 12%, Cumulative CPU 92.56 sec
2016-03-10 18:54:30,892 Stage-1 map = 79%,  reduce = 12%, Cumulative CPU 95.28 sec
2016-03-10 18:54:31,949 Stage-1 map = 90%,  reduce = 13%, Cumulative CPU 101.95 sec
2016-03-10 18:54:33,004 Stage-1 map = 90%,  reduce = 26%, Cumulative CPU 102.45 sec
2016-03-10 18:54:35,103 Stage-1 map = 92%,  reduce = 28%, Cumulative CPU 106.11 sec
2016-03-10 18:54:37,212 Stage-1 map = 100%,  reduce = 28%, Cumulative CPU 109.78 sec
2016-03-10 18:54:39,315 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 118.24 sec
2016-03-10 18:54:40,369 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 121.81 sec
MapReduce Total cumulative CPU time: 2 minutes 1 seconds 810 msec
Ended Job = job_1453192496319_1025664
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 5   Cumulative CPU: 122.02 sec   HDFS Read: 784775099 HDFS Write: 120 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 2 seconds 20 msec
OK
Time taken: 55.628 seconds, Fetched: 3 row(s)
2016-03-10 18:54:43 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:54:43 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160229
2016-03-10 18:54:43 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:54:43 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:54:43 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:54:43 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:54:43 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:54:43 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:54:44 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:54:44 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:54:44 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:54:44 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1042millis
2016-03-10 18:54:44 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:54:44 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:54:44 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:54:44 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:54:44 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160301.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185455_342a21d9-f068-49b2-94a5-397c136b9977
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025668, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025668/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025668
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 13
2016-03-10 18:55:13,097 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:55:23,676 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 49.99 sec
2016-03-10 18:55:24,731 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 72.99 sec
2016-03-10 18:55:26,848 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 105.45 sec
2016-03-10 18:55:30,000 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 161.04 sec
2016-03-10 18:55:31,050 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 164.44 sec
2016-03-10 18:55:32,094 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 171.95 sec
2016-03-10 18:55:33,148 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 191.44 sec
2016-03-10 18:55:35,247 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 238.57 sec
2016-03-10 18:55:36,319 Stage-1 map = 36%,  reduce = 2%, Cumulative CPU 255.14 sec
2016-03-10 18:55:37,384 Stage-1 map = 40%,  reduce = 2%, Cumulative CPU 263.91 sec
2016-03-10 18:55:38,454 Stage-1 map = 40%,  reduce = 3%, Cumulative CPU 269.97 sec
2016-03-10 18:55:39,508 Stage-1 map = 43%,  reduce = 5%, Cumulative CPU 283.25 sec
2016-03-10 18:55:40,562 Stage-1 map = 48%,  reduce = 5%, Cumulative CPU 291.91 sec
2016-03-10 18:55:41,618 Stage-1 map = 49%,  reduce = 6%, Cumulative CPU 295.98 sec
2016-03-10 18:55:42,670 Stage-1 map = 52%,  reduce = 7%, Cumulative CPU 308.58 sec
2016-03-10 18:55:43,727 Stage-1 map = 53%,  reduce = 8%, Cumulative CPU 345.56 sec
2016-03-10 18:55:44,780 Stage-1 map = 58%,  reduce = 8%, Cumulative CPU 358.51 sec
2016-03-10 18:55:45,832 Stage-1 map = 65%,  reduce = 9%, Cumulative CPU 366.74 sec
2016-03-10 18:55:46,888 Stage-1 map = 72%,  reduce = 10%, Cumulative CPU 380.1 sec
2016-03-10 18:55:47,939 Stage-1 map = 72%,  reduce = 12%, Cumulative CPU 383.52 sec
2016-03-10 18:55:48,997 Stage-1 map = 75%,  reduce = 14%, Cumulative CPU 387.89 sec
2016-03-10 18:55:50,049 Stage-1 map = 77%,  reduce = 17%, Cumulative CPU 397.81 sec
2016-03-10 18:55:51,097 Stage-1 map = 78%,  reduce = 20%, Cumulative CPU 402.91 sec
2016-03-10 18:55:52,147 Stage-1 map = 78%,  reduce = 21%, Cumulative CPU 405.96 sec
2016-03-10 18:55:54,243 Stage-1 map = 79%,  reduce = 23%, Cumulative CPU 418.07 sec
2016-03-10 18:55:55,290 Stage-1 map = 81%,  reduce = 23%, Cumulative CPU 423.02 sec
2016-03-10 18:55:57,398 Stage-1 map = 84%,  reduce = 23%, Cumulative CPU 433.11 sec
2016-03-10 18:55:59,496 Stage-1 map = 85%,  reduce = 24%, Cumulative CPU 441.61 sec
2016-03-10 18:56:00,555 Stage-1 map = 89%,  reduce = 25%, Cumulative CPU 445.53 sec
2016-03-10 18:56:01,603 Stage-1 map = 89%,  reduce = 26%, Cumulative CPU 447.99 sec
2016-03-10 18:56:02,643 Stage-1 map = 90%,  reduce = 26%, Cumulative CPU 454.19 sec
2016-03-10 18:56:03,695 Stage-1 map = 90%,  reduce = 28%, Cumulative CPU 454.82 sec
2016-03-10 18:56:05,794 Stage-1 map = 92%,  reduce = 28%, Cumulative CPU 462.66 sec
2016-03-10 18:56:11,027 Stage-1 map = 96%,  reduce = 29%, Cumulative CPU 499.63 sec
2016-03-10 18:56:13,124 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 504.47 sec
2016-03-10 18:56:14,165 Stage-1 map = 96%,  reduce = 31%, Cumulative CPU 512.64 sec
2016-03-10 18:56:18,336 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 524.97 sec
2016-03-10 18:56:24,625 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 547.42 sec
2016-03-10 18:56:25,673 Stage-1 map = 100%,  reduce = 57%, Cumulative CPU 554.21 sec
2016-03-10 18:56:26,716 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 563.45 sec
2016-03-10 18:56:27,763 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 571.52 sec
2016-03-10 18:56:28,810 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 573.76 sec
MapReduce Total cumulative CPU time: 9 minutes 33 seconds 760 msec
Ended Job = job_1453192496319_1025668
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 13  Reduce: 13   Cumulative CPU: 573.76 sec   HDFS Read: 2146740934 HDFS Write: 207 SUCCESS
Total MapReduce CPU Time Spent: 9 minutes 33 seconds 760 msec
OK
Time taken: 94.014 seconds, Fetched: 4 row(s)
./data/mz_pv_20160301.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185641_391911c2-25b3-4dcc-ae04-01f20e8be693
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025675, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025675/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025675
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 5
2016-03-10 18:57:02,541 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:57:14,180 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 11.44 sec
2016-03-10 18:57:15,234 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 19.45 sec
2016-03-10 18:57:16,295 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 27.47 sec
2016-03-10 18:57:17,349 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 30.47 sec
2016-03-10 18:57:18,405 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 33.6 sec
2016-03-10 18:57:19,466 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 49.62 sec
2016-03-10 18:57:20,517 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 59.66 sec
2016-03-10 18:57:21,572 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 62.81 sec
2016-03-10 18:57:22,627 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 68.93 sec
2016-03-10 18:57:23,681 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 72.05 sec
2016-03-10 18:57:24,730 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 75.02 sec
2016-03-10 18:57:25,777 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 80.77 sec
2016-03-10 18:57:26,827 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 83.62 sec
2016-03-10 18:57:27,894 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 86.67 sec
2016-03-10 18:57:28,947 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 95.62 sec
2016-03-10 18:57:31,034 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 113.56 sec
2016-03-10 18:57:32,102 Stage-1 map = 65%,  reduce = 1%, Cumulative CPU 119.64 sec
2016-03-10 18:57:33,154 Stage-1 map = 65%,  reduce = 9%, Cumulative CPU 120.77 sec
2016-03-10 18:57:34,212 Stage-1 map = 69%,  reduce = 12%, Cumulative CPU 126.96 sec
2016-03-10 18:57:35,260 Stage-1 map = 69%,  reduce = 13%, Cumulative CPU 127.05 sec
2016-03-10 18:57:36,311 Stage-1 map = 79%,  reduce = 13%, Cumulative CPU 150.5 sec
2016-03-10 18:57:37,371 Stage-1 map = 80%,  reduce = 16%, Cumulative CPU 153.69 sec
2016-03-10 18:57:38,432 Stage-1 map = 80%,  reduce = 17%, Cumulative CPU 153.81 sec
2016-03-10 18:57:39,487 Stage-1 map = 90%,  reduce = 19%, Cumulative CPU 159.84 sec
2016-03-10 18:57:40,534 Stage-1 map = 90%,  reduce = 24%, Cumulative CPU 160.66 sec
2016-03-10 18:57:41,588 Stage-1 map = 91%,  reduce = 24%, Cumulative CPU 163.62 sec
2016-03-10 18:57:42,635 Stage-1 map = 91%,  reduce = 27%, Cumulative CPU 164.83 sec
2016-03-10 18:57:44,736 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 168.07 sec
2016-03-10 18:57:45,786 Stage-1 map = 100%,  reduce = 49%, Cumulative CPU 171.9 sec
2016-03-10 18:57:46,835 Stage-1 map = 100%,  reduce = 66%, Cumulative CPU 178.62 sec
2016-03-10 18:57:47,884 Stage-1 map = 100%,  reduce = 80%, Cumulative CPU 199.98 sec
2016-03-10 18:57:48,935 Stage-1 map = 100%,  reduce = 87%, Cumulative CPU 204.38 sec
2016-03-10 18:57:49,996 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 212.66 sec
2016-03-10 18:57:51,042 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 214.15 sec
MapReduce Total cumulative CPU time: 3 minutes 34 seconds 150 msec
Ended Job = job_1453192496319_1025675
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 5  Reduce: 5   Cumulative CPU: 214.15 sec   HDFS Read: 785268175 HDFS Write: 120 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 34 seconds 150 msec
OK
Time taken: 70.877 seconds, Fetched: 3 row(s)
2016-03-10 18:57:53 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 18:57:53 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160301
2016-03-10 18:57:53 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:57:53 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 18:57:53 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 18:57:53 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 18:57:53 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 18:57:53 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 18:57:54 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 18:57:54 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 18:57:54 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 18:57:54 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1073millis
2016-03-10 18:57:54 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 18:57:54 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 18:57:54 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 18:57:54 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 18:57:54 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160302.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185805_92e64fa8-9298-459a-9db9-a2ea3d2a4aa6
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 12
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025681, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025681/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025681
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 12
2016-03-10 18:58:21,277 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:58:31,835 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 44.29 sec
2016-03-10 18:58:32,886 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 66.94 sec
2016-03-10 18:58:34,996 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 104.49 sec
2016-03-10 18:58:36,045 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 115.75 sec
2016-03-10 18:58:37,091 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 119.14 sec
2016-03-10 18:58:38,142 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 149.16 sec
2016-03-10 18:58:39,199 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 156.07 sec
2016-03-10 18:58:41,315 Stage-1 map = 32%,  reduce = 0%, Cumulative CPU 184.99 sec
2016-03-10 18:58:42,373 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 193.7 sec
2016-03-10 18:58:43,585 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 199.29 sec
2016-03-10 18:58:44,697 Stage-1 map = 45%,  reduce = 0%, Cumulative CPU 217.29 sec
2016-03-10 18:58:45,758 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 222.64 sec
2016-03-10 18:58:46,833 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 227.97 sec
2016-03-10 18:58:47,887 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 242.44 sec
2016-03-10 18:58:49,993 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 257.75 sec
2016-03-10 18:58:51,039 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 270.06 sec
2016-03-10 18:58:52,089 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 278.19 sec
2016-03-10 18:58:53,151 Stage-1 map = 65%,  reduce = 1%, Cumulative CPU 281.54 sec
2016-03-10 18:58:54,216 Stage-1 map = 69%,  reduce = 1%, Cumulative CPU 303.0 sec
2016-03-10 18:58:55,690 Stage-1 map = 69%,  reduce = 4%, Cumulative CPU 311.12 sec
2016-03-10 18:58:56,751 Stage-1 map = 69%,  reduce = 8%, Cumulative CPU 324.34 sec
2016-03-10 18:58:57,825 Stage-1 map = 72%,  reduce = 10%, Cumulative CPU 339.27 sec
2016-03-10 18:58:58,913 Stage-1 map = 76%,  reduce = 12%, Cumulative CPU 344.86 sec
2016-03-10 18:58:59,979 Stage-1 map = 76%,  reduce = 16%, Cumulative CPU 352.0 sec
2016-03-10 18:59:01,029 Stage-1 map = 76%,  reduce = 18%, Cumulative CPU 358.64 sec
2016-03-10 18:59:02,078 Stage-1 map = 78%,  reduce = 19%, Cumulative CPU 365.95 sec
2016-03-10 18:59:03,124 Stage-1 map = 79%,  reduce = 19%, Cumulative CPU 369.37 sec
2016-03-10 18:59:04,179 Stage-1 map = 84%,  reduce = 19%, Cumulative CPU 376.95 sec
2016-03-10 18:59:05,287 Stage-1 map = 85%,  reduce = 20%, Cumulative CPU 383.66 sec
2016-03-10 18:59:06,343 Stage-1 map = 91%,  reduce = 21%, Cumulative CPU 387.94 sec
2016-03-10 18:59:07,398 Stage-1 map = 96%,  reduce = 23%, Cumulative CPU 396.58 sec
2016-03-10 18:59:08,461 Stage-1 map = 96%,  reduce = 27%, Cumulative CPU 399.24 sec
2016-03-10 18:59:09,529 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 397.73 sec
2016-03-10 18:59:10,587 Stage-1 map = 100%,  reduce = 41%, Cumulative CPU 401.27 sec
2016-03-10 18:59:11,638 Stage-1 map = 100%,  reduce = 57%, Cumulative CPU 414.62 sec
2016-03-10 18:59:12,690 Stage-1 map = 100%,  reduce = 80%, Cumulative CPU 427.01 sec
2016-03-10 18:59:13,742 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 442.8 sec
2016-03-10 18:59:15,836 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 445.05 sec
2016-03-10 18:59:16,888 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 449.62 sec
MapReduce Total cumulative CPU time: 7 minutes 29 seconds 620 msec
Ended Job = job_1453192496319_1025681
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 12  Reduce: 12   Cumulative CPU: 449.62 sec   HDFS Read: 2075061015 HDFS Write: 199 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 29 seconds 620 msec
OK
Time taken: 72.505 seconds, Fetched: 4 row(s)
./data/mz_pv_20160302.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310185929_62168e89-744d-4811-a438-8b5f7b940935
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025687, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025687/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025687
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6
2016-03-10 18:59:47,960 Stage-1 map = 0%,  reduce = 0%
2016-03-10 18:59:58,645 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 14.72 sec
2016-03-10 18:59:59,699 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 37.96 sec
2016-03-10 19:00:01,822 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 53.19 sec
2016-03-10 19:00:02,880 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 59.5 sec
2016-03-10 19:00:04,981 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 85.28 sec
2016-03-10 19:00:06,039 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 88.26 sec
2016-03-10 19:00:08,129 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 103.22 sec
2016-03-10 19:00:09,177 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 106.91 sec
2016-03-10 19:00:10,232 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 108.7 sec
2016-03-10 19:00:11,336 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 120.7 sec
2016-03-10 19:00:12,386 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 123.46 sec
2016-03-10 19:00:13,441 Stage-1 map = 77%,  reduce = 0%, Cumulative CPU 126.56 sec
2016-03-10 19:00:17,324 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 131.05 sec
2016-03-10 19:00:20,381 Stage-1 map = 83%,  reduce = 4%, Cumulative CPU 136.64 sec
2016-03-10 19:00:21,447 Stage-1 map = 83%,  reduce = 7%, Cumulative CPU 140.18 sec
2016-03-10 19:00:22,501 Stage-1 map = 90%,  reduce = 12%, Cumulative CPU 142.95 sec
2016-03-10 19:00:23,556 Stage-1 map = 91%,  reduce = 19%, Cumulative CPU 147.31 sec
2016-03-10 19:00:24,613 Stage-1 map = 91%,  reduce = 23%, Cumulative CPU 147.87 sec
2016-03-10 19:00:25,686 Stage-1 map = 93%,  reduce = 28%, Cumulative CPU 153.32 sec
2016-03-10 19:00:27,812 Stage-1 map = 100%,  reduce = 28%, Cumulative CPU 157.78 sec
2016-03-10 19:00:29,933 Stage-1 map = 100%,  reduce = 46%, Cumulative CPU 161.34 sec
2016-03-10 19:00:30,985 Stage-1 map = 100%,  reduce = 70%, Cumulative CPU 168.35 sec
2016-03-10 19:00:32,034 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 176.18 sec
2016-03-10 19:00:33,090 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 179.51 sec
MapReduce Total cumulative CPU time: 2 minutes 59 seconds 510 msec
Ended Job = job_1453192496319_1025687
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 179.51 sec   HDFS Read: 844108282 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 59 seconds 510 msec
OK
Time taken: 65.1 seconds, Fetched: 3 row(s)
2016-03-10 19:00:35 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:00:35 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160302
2016-03-10 19:00:35 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:00:35 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:00:35 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:00:35 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:00:35 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:00:35 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:00:36 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:00:36 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:00:36 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:00:36 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1103millis
2016-03-10 19:00:37 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:00:37 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:00:37 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:00:37 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:00:37 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160303.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190048_bae333e8-9db3-425d-8317-fe4fa409c5ed
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025695, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025695/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025695
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 13
2016-03-10 19:01:05,186 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:01:15,775 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 102.71 sec
2016-03-10 19:01:18,938 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 146.47 sec
2016-03-10 19:01:22,091 Stage-1 map = 34%,  reduce = 0%, Cumulative CPU 190.04 sec
2016-03-10 19:01:23,136 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 192.77 sec
2016-03-10 19:01:24,190 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 201.46 sec
2016-03-10 19:01:25,251 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 230.03 sec
2016-03-10 19:01:26,301 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 231.68 sec
2016-03-10 19:01:28,420 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 252.96 sec
2016-03-10 19:01:29,466 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 254.45 sec
2016-03-10 19:01:31,576 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 273.48 sec
2016-03-10 19:01:32,626 Stage-1 map = 89%,  reduce = 0%, Cumulative CPU 277.94 sec
2016-03-10 19:01:33,689 Stage-1 map = 89%,  reduce = 24%, Cumulative CPU 285.45 sec
2016-03-10 19:01:34,754 Stage-1 map = 97%,  reduce = 26%, Cumulative CPU 295.2 sec
2016-03-10 19:01:36,860 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 299.51 sec
2016-03-10 19:01:38,970 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 301.5 sec
2016-03-10 19:01:40,017 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 324.75 sec
2016-03-10 19:01:41,076 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 327.01 sec
MapReduce Total cumulative CPU time: 5 minutes 27 seconds 10 msec
Ended Job = job_1453192496319_1025695
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 14  Reduce: 13   Cumulative CPU: 327.01 sec   HDFS Read: 2171753361 HDFS Write: 207 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 27 seconds 10 msec
OK
Time taken: 53.826 seconds, Fetched: 4 row(s)
./data/mz_pv_20160303.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190153_ba12bee6-91b9-4727-8f3b-351bd0f03dcd
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025709, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025709/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025709
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 6
2016-03-10 19:02:11,532 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:02:22,086 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 44.19 sec
2016-03-10 19:02:24,190 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 50.71 sec
2016-03-10 19:02:25,240 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 69.53 sec
2016-03-10 19:02:27,347 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 73.07 sec
2016-03-10 19:02:28,399 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 91.5 sec
2016-03-10 19:02:30,497 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 94.66 sec
2016-03-10 19:02:31,547 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 112.86 sec
2016-03-10 19:02:32,639 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 112.86 sec
2016-03-10 19:02:33,689 Stage-1 map = 74%,  reduce = 0%, Cumulative CPU 118.69 sec
2016-03-10 19:02:34,747 Stage-1 map = 77%,  reduce = 0%, Cumulative CPU 124.07 sec
2016-03-10 19:02:35,794 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 126.82 sec
2016-03-10 19:02:37,896 Stage-1 map = 87%,  reduce = 0%, Cumulative CPU 135.36 sec
2016-03-10 19:02:38,944 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 137.37 sec
2016-03-10 19:02:41,061 Stage-1 map = 94%,  reduce = 19%, Cumulative CPU 141.49 sec
2016-03-10 19:02:42,115 Stage-1 map = 100%,  reduce = 24%, Cumulative CPU 144.16 sec
2016-03-10 19:02:43,158 Stage-1 map = 100%,  reduce = 36%, Cumulative CPU 145.85 sec
2016-03-10 19:02:44,207 Stage-1 map = 100%,  reduce = 79%, Cumulative CPU 158.02 sec
2016-03-10 19:02:45,261 Stage-1 map = 100%,  reduce = 85%, Cumulative CPU 161.12 sec
2016-03-10 19:02:47,372 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 166.05 sec
2016-03-10 19:02:48,425 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 171.99 sec
2016-03-10 19:02:49,473 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 174.46 sec
MapReduce Total cumulative CPU time: 2 minutes 54 seconds 460 msec
Ended Job = job_1453192496319_1025709
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 6   Cumulative CPU: 174.46 sec   HDFS Read: 855070748 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 54 seconds 460 msec
OK
Time taken: 56.9 seconds, Fetched: 3 row(s)
2016-03-10 19:02:51 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:02:51 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160303
2016-03-10 19:02:51 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:02:51 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:02:51 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:02:51 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:02:51 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:02:51 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:02:52 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:02:52 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:02:52 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:02:52 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1077millis
2016-03-10 19:02:52 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:02:52 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:02:52 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:02:52 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:02:52 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160304.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190303_3c8c7172-4c03-416c-ae49-cde487925551
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025715, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025715/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025715
Hadoop job information for Stage-1: number of mappers: 17; number of reducers: 14
2016-03-10 19:03:29,475 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:03:40,070 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 7.23 sec
2016-03-10 19:03:41,130 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 118.75 sec
2016-03-10 19:03:43,243 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 145.38 sec
2016-03-10 19:03:44,299 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 192.26 sec
2016-03-10 19:03:46,401 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 229.23 sec
2016-03-10 19:03:47,450 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 244.78 sec
2016-03-10 19:03:48,499 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 250.23 sec
2016-03-10 19:03:49,553 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 282.43 sec
2016-03-10 19:03:50,599 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 305.51 sec
2016-03-10 19:03:51,649 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 314.49 sec
2016-03-10 19:03:53,756 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 340.53 sec
2016-03-10 19:03:55,866 Stage-1 map = 84%,  reduce = 0%, Cumulative CPU 346.81 sec
2016-03-10 19:03:56,918 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 353.04 sec
2016-03-10 19:03:59,021 Stage-1 map = 87%,  reduce = 0%, Cumulative CPU 359.44 sec
2016-03-10 19:04:00,083 Stage-1 map = 87%,  reduce = 25%, Cumulative CPU 371.67 sec
2016-03-10 19:04:03,250 Stage-1 map = 90%,  reduce = 25%, Cumulative CPU 387.48 sec
2016-03-10 19:04:04,302 Stage-1 map = 93%,  reduce = 25%, Cumulative CPU 388.73 sec
2016-03-10 19:04:05,364 Stage-1 map = 93%,  reduce = 26%, Cumulative CPU 392.08 sec
2016-03-10 19:04:06,415 Stage-1 map = 97%,  reduce = 30%, Cumulative CPU 397.37 sec
2016-03-10 19:04:08,515 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 401.42 sec
2016-03-10 19:04:11,647 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 408.02 sec
2016-03-10 19:04:12,701 Stage-1 map = 100%,  reduce = 72%, Cumulative CPU 421.36 sec
2016-03-10 19:04:13,751 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 430.83 sec
2016-03-10 19:04:14,795 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 434.1 sec
MapReduce Total cumulative CPU time: 7 minutes 14 seconds 100 msec
Ended Job = job_1453192496319_1025715
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 17  Reduce: 14   Cumulative CPU: 434.1 sec   HDFS Read: 2377629796 HDFS Write: 219 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 14 seconds 100 msec
OK
Time taken: 72.342 seconds, Fetched: 4 row(s)
./data/mz_pv_20160304.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190427_8214f4ea-df20-473d-b9e9-368d3bd29213
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025724, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025724/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025724
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 7
2016-03-10 19:04:46,321 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:04:57,951 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 21.14 sec
2016-03-10 19:05:00,068 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 43.26 sec
2016-03-10 19:05:01,128 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 51.08 sec
2016-03-10 19:05:02,181 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 58.31 sec
2016-03-10 19:05:03,244 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 65.81 sec
2016-03-10 19:05:04,289 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 79.81 sec
2016-03-10 19:05:05,334 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 83.03 sec
2016-03-10 19:05:06,383 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 92.64 sec
2016-03-10 19:05:07,429 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 99.18 sec
2016-03-10 19:05:08,483 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 107.29 sec
2016-03-10 19:05:09,542 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 113.18 sec
2016-03-10 19:05:10,596 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 117.45 sec
2016-03-10 19:05:11,658 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 127.07 sec
2016-03-10 19:05:12,709 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 133.44 sec
2016-03-10 19:05:13,750 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 136.14 sec
2016-03-10 19:05:14,798 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 140.48 sec
2016-03-10 19:05:15,854 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 143.71 sec
2016-03-10 19:05:16,910 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 147.63 sec
2016-03-10 19:05:17,969 Stage-1 map = 67%,  reduce = 5%, Cumulative CPU 150.57 sec
2016-03-10 19:05:19,042 Stage-1 map = 69%,  reduce = 10%, Cumulative CPU 154.9 sec
2016-03-10 19:05:20,101 Stage-1 map = 69%,  reduce = 14%, Cumulative CPU 159.09 sec
2016-03-10 19:05:21,156 Stage-1 map = 77%,  reduce = 17%, Cumulative CPU 163.96 sec
2016-03-10 19:05:23,266 Stage-1 map = 82%,  reduce = 19%, Cumulative CPU 168.55 sec
2016-03-10 19:05:24,319 Stage-1 map = 82%,  reduce = 22%, Cumulative CPU 171.83 sec
2016-03-10 19:05:25,367 Stage-1 map = 82%,  reduce = 23%, Cumulative CPU 172.33 sec
2016-03-10 19:05:26,418 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 177.15 sec
2016-03-10 19:05:27,469 Stage-1 map = 88%,  reduce = 26%, Cumulative CPU 177.37 sec
2016-03-10 19:05:28,520 Stage-1 map = 88%,  reduce = 27%, Cumulative CPU 178.0 sec
2016-03-10 19:05:29,577 Stage-1 map = 88%,  reduce = 29%, Cumulative CPU 178.77 sec
2016-03-10 19:05:35,908 Stage-1 map = 89%,  reduce = 29%, Cumulative CPU 181.35 sec
2016-03-10 19:05:53,756 Stage-1 map = 90%,  reduce = 29%, Cumulative CPU 193.76 sec
2016-03-10 19:06:00,030 Stage-1 map = 91%,  reduce = 29%, Cumulative CPU 201.35 sec
2016-03-10 19:06:06,301 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 209.46 sec
2016-03-10 19:06:14,633 Stage-1 map = 95%,  reduce = 29%, Cumulative CPU 228.2 sec
2016-03-10 19:06:15,684 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 229.63 sec
2016-03-10 19:06:16,722 Stage-1 map = 100%,  reduce = 70%, Cumulative CPU 235.44 sec
2016-03-10 19:06:17,773 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 243.06 sec
MapReduce Total cumulative CPU time: 4 minutes 3 seconds 60 msec
Ended Job = job_1453192496319_1025724
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 7   Cumulative CPU: 243.06 sec   HDFS Read: 1016594162 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 3 seconds 60 msec
OK
Time taken: 111.682 seconds, Fetched: 3 row(s)
2016-03-10 19:06:19 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:06:19 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160304
2016-03-10 19:06:19 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:06:19 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:06:20 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:06:20 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:06:20 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:06:20 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:06:21 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:06:21 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:06:21 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:06:21 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1077millis
2016-03-10 19:06:21 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:06:21 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:06:21 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:06:21 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:06:21 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160305.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190632_3a1f6c70-509c-4de3-8b85-5e32651853b6
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 17
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025728, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025728/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025728
Hadoop job information for Stage-1: number of mappers: 19; number of reducers: 17
2016-03-10 19:06:50,649 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:07:02,043 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 57.93 sec
2016-03-10 19:07:03,098 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 102.57 sec
2016-03-10 19:07:05,225 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 141.98 sec
2016-03-10 19:07:06,276 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 158.44 sec
2016-03-10 19:07:07,338 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 178.86 sec
2016-03-10 19:07:08,395 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 222.34 sec
2016-03-10 19:07:10,509 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 238.14 sec
2016-03-10 19:07:11,558 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 281.89 sec
2016-03-10 19:07:12,925 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 286.6 sec
2016-03-10 19:07:13,983 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 311.26 sec
2016-03-10 19:07:15,054 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 348.58 sec
2016-03-10 19:07:17,562 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 362.41 sec
2016-03-10 19:07:18,625 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 392.82 sec
2016-03-10 19:07:19,689 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 395.89 sec
2016-03-10 19:07:25,259 Stage-1 map = 54%,  reduce = 0%, Cumulative CPU 459.94 sec
2016-03-10 19:07:26,317 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 464.57 sec
2016-03-10 19:07:27,601 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 479.76 sec
2016-03-10 19:07:28,704 Stage-1 map = 62%,  reduce = 1%, Cumulative CPU 494.58 sec
2016-03-10 19:07:29,779 Stage-1 map = 65%,  reduce = 1%, Cumulative CPU 499.12 sec
2016-03-10 19:07:31,900 Stage-1 map = 65%,  reduce = 4%, Cumulative CPU 524.81 sec
2016-03-10 19:07:32,954 Stage-1 map = 68%,  reduce = 4%, Cumulative CPU 533.27 sec
2016-03-10 19:07:34,012 Stage-1 map = 72%,  reduce = 5%, Cumulative CPU 540.18 sec
2016-03-10 19:07:35,086 Stage-1 map = 72%,  reduce = 9%, Cumulative CPU 553.96 sec
2016-03-10 19:07:36,148 Stage-1 map = 72%,  reduce = 12%, Cumulative CPU 561.53 sec
2016-03-10 19:07:37,438 Stage-1 map = 74%,  reduce = 13%, Cumulative CPU 567.14 sec
2016-03-10 19:07:38,523 Stage-1 map = 74%,  reduce = 16%, Cumulative CPU 581.97 sec
2016-03-10 19:07:40,646 Stage-1 map = 78%,  reduce = 17%, Cumulative CPU 598.1 sec
2016-03-10 19:07:41,716 Stage-1 map = 81%,  reduce = 18%, Cumulative CPU 606.42 sec
2016-03-10 19:07:43,152 Stage-1 map = 82%,  reduce = 19%, Cumulative CPU 612.01 sec
2016-03-10 19:07:44,216 Stage-1 map = 84%,  reduce = 21%, Cumulative CPU 620.87 sec
2016-03-10 19:07:45,477 Stage-1 map = 84%,  reduce = 22%, Cumulative CPU 623.08 sec
2016-03-10 19:07:47,492 Stage-1 map = 84%,  reduce = 23%, Cumulative CPU 645.43 sec
2016-03-10 19:07:48,548 Stage-1 map = 85%,  reduce = 23%, Cumulative CPU 650.46 sec
2016-03-10 19:07:49,603 Stage-1 map = 85%,  reduce = 24%, Cumulative CPU 655.6 sec
2016-03-10 19:07:50,659 Stage-1 map = 85%,  reduce = 25%, Cumulative CPU 667.19 sec
2016-03-10 19:07:51,713 Stage-1 map = 91%,  reduce = 25%, Cumulative CPU 672.15 sec
2016-03-10 19:07:53,824 Stage-1 map = 93%,  reduce = 27%, Cumulative CPU 682.42 sec
2016-03-10 19:07:54,879 Stage-1 map = 95%,  reduce = 28%, Cumulative CPU 686.86 sec
2016-03-10 19:07:56,977 Stage-1 map = 95%,  reduce = 29%, Cumulative CPU 692.94 sec
2016-03-10 19:07:58,033 Stage-1 map = 96%,  reduce = 30%, Cumulative CPU 696.59 sec
2016-03-10 19:07:59,086 Stage-1 map = 98%,  reduce = 30%, Cumulative CPU 698.6 sec
2016-03-10 19:08:00,138 Stage-1 map = 98%,  reduce = 31%, Cumulative CPU 702.48 sec
2016-03-10 19:08:02,231 Stage-1 map = 100%,  reduce = 34%, Cumulative CPU 714.03 sec
2016-03-10 19:08:03,277 Stage-1 map = 100%,  reduce = 62%, Cumulative CPU 729.1 sec
2016-03-10 19:08:04,324 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 745.98 sec
2016-03-10 19:08:05,376 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 769.66 sec
2016-03-10 19:08:06,427 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 788.53 sec
2016-03-10 19:08:07,488 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 792.21 sec
2016-03-10 19:08:09,574 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 795.58 sec
MapReduce Total cumulative CPU time: 13 minutes 15 seconds 580 msec
Ended Job = job_1453192496319_1025728
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 19  Reduce: 17   Cumulative CPU: 796.05 sec   HDFS Read: 2985548963 HDFS Write: 246 SUCCESS
Total MapReduce CPU Time Spent: 13 minutes 16 seconds 50 msec
OK
Time taken: 100.077 seconds, Fetched: 4 row(s)
./data/mz_pv_20160305.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190824_18fb280b-c611-4183-9d3b-731aa3a06e31
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 9
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025736, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025736/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025736
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 9
2016-03-10 19:08:47,102 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:08:58,124 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 34.79 sec
2016-03-10 19:09:01,363 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 53.0 sec
2016-03-10 19:09:02,416 Stage-1 map = 12%,  reduce = 0%, Cumulative CPU 56.18 sec
2016-03-10 19:09:03,497 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 72.32 sec
2016-03-10 19:09:04,581 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 83.69 sec
2016-03-10 19:09:05,661 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 90.21 sec
2016-03-10 19:09:06,742 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 96.29 sec
2016-03-10 19:09:07,805 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 105.72 sec
2016-03-10 19:09:08,867 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 107.49 sec
2016-03-10 19:09:09,922 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 120.42 sec
2016-03-10 19:09:10,980 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 128.06 sec
2016-03-10 19:09:12,033 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 130.9 sec
2016-03-10 19:09:13,109 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 142.78 sec
2016-03-10 19:09:14,177 Stage-1 map = 52%,  reduce = 0%, Cumulative CPU 149.28 sec
2016-03-10 19:09:15,239 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 153.87 sec
2016-03-10 19:09:16,293 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 161.52 sec
2016-03-10 19:09:18,407 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 166.9 sec
2016-03-10 19:09:19,455 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 177.95 sec
2016-03-10 19:09:20,505 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 181.94 sec
2016-03-10 19:09:21,571 Stage-1 map = 78%,  reduce = 7%, Cumulative CPU 182.64 sec
2016-03-10 19:09:22,626 Stage-1 map = 83%,  reduce = 19%, Cumulative CPU 193.9 sec
2016-03-10 19:09:24,746 Stage-1 map = 94%,  reduce = 21%, Cumulative CPU 199.86 sec
2016-03-10 19:09:25,796 Stage-1 map = 94%,  reduce = 23%, Cumulative CPU 203.27 sec
2016-03-10 19:09:27,904 Stage-1 map = 100%,  reduce = 42%, Cumulative CPU 206.39 sec
2016-03-10 19:09:28,957 Stage-1 map = 100%,  reduce = 70%, Cumulative CPU 212.94 sec
2016-03-10 19:09:30,003 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 226.57 sec
MapReduce Total cumulative CPU time: 3 minutes 46 seconds 570 msec
Ended Job = job_1453192496319_1025736
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 9   Cumulative CPU: 226.57 sec   HDFS Read: 1284218069 HDFS Write: 150 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 46 seconds 570 msec
OK
Time taken: 67.921 seconds, Fetched: 3 row(s)
2016-03-10 19:09:33 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:09:33 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160305
2016-03-10 19:09:33 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:09:33 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:09:33 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:09:33 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:09:33 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:09:33 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:09:34 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:09:34 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:09:34 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:09:34 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1065millis
2016-03-10 19:09:34 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:09:34 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:09:34 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:09:34 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:09:34 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160306.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310190945_a5b3a542-50f1-417c-8fb2-ac9fa834d09e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025746, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025746/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025746
Hadoop job information for Stage-1: number of mappers: 20; number of reducers: 18
2016-03-10 19:10:02,102 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:10:12,677 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 142.33 sec
2016-03-10 19:10:13,732 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 142.91 sec
2016-03-10 19:10:15,853 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 201.9 sec
2016-03-10 19:10:19,008 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 261.58 sec
2016-03-10 19:10:21,118 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 266.68 sec
2016-03-10 19:10:22,174 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 318.04 sec
2016-03-10 19:10:23,225 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 322.38 sec
2016-03-10 19:10:24,289 Stage-1 map = 62%,  reduce = 13%, Cumulative CPU 328.88 sec
2016-03-10 19:10:25,345 Stage-1 map = 69%,  reduce = 13%, Cumulative CPU 362.19 sec
2016-03-10 19:10:26,406 Stage-1 map = 72%,  reduce = 13%, Cumulative CPU 363.71 sec
2016-03-10 19:10:27,467 Stage-1 map = 77%,  reduce = 18%, Cumulative CPU 380.78 sec
2016-03-10 19:10:28,527 Stage-1 map = 78%,  reduce = 18%, Cumulative CPU 392.63 sec
2016-03-10 19:10:30,624 Stage-1 map = 83%,  reduce = 20%, Cumulative CPU 421.59 sec
2016-03-10 19:10:31,672 Stage-1 map = 85%,  reduce = 20%, Cumulative CPU 422.8 sec
2016-03-10 19:10:32,723 Stage-1 map = 90%,  reduce = 23%, Cumulative CPU 427.71 sec
2016-03-10 19:10:33,780 Stage-1 map = 95%,  reduce = 27%, Cumulative CPU 441.15 sec
2016-03-10 19:10:34,825 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 443.58 sec
2016-03-10 19:10:35,879 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 466.17 sec
2016-03-10 19:10:36,925 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 474.39 sec
2016-03-10 19:10:37,976 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 477.32 sec
MapReduce Total cumulative CPU time: 7 minutes 57 seconds 320 msec
Ended Job = job_1453192496319_1025746
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 20  Reduce: 18   Cumulative CPU: 477.32 sec   HDFS Read: 3099903084 HDFS Write: 249 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 57 seconds 320 msec
OK
Time taken: 53.919 seconds, Fetched: 4 row(s)
./data/mz_pv_20160306.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310191050_524decb1-7f45-4ac0-9610-54cf6f454986
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025765, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025765/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025765
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 8
2016-03-10 19:11:07,191 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:11:17,744 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 57.83 sec
2016-03-10 19:11:20,920 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 83.04 sec
2016-03-10 19:11:24,079 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 108.32 sec
2016-03-10 19:11:26,183 Stage-1 map = 49%,  reduce = 0%, Cumulative CPU 111.27 sec
2016-03-10 19:11:27,240 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 126.98 sec
2016-03-10 19:11:29,343 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 129.96 sec
2016-03-10 19:11:30,396 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 141.03 sec
2016-03-10 19:11:32,501 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 157.28 sec
2016-03-10 19:11:33,548 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 158.41 sec
2016-03-10 19:11:34,613 Stage-1 map = 81%,  reduce = 13%, Cumulative CPU 163.17 sec
2016-03-10 19:11:35,676 Stage-1 map = 90%,  reduce = 13%, Cumulative CPU 171.8 sec
2016-03-10 19:11:36,722 Stage-1 map = 100%,  reduce = 13%, Cumulative CPU 174.27 sec
2016-03-10 19:11:37,771 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 175.89 sec
2016-03-10 19:11:38,824 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 190.59 sec
2016-03-10 19:11:39,874 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 192.54 sec
MapReduce Total cumulative CPU time: 3 minutes 12 seconds 540 msec
Ended Job = job_1453192496319_1025765
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 8   Cumulative CPU: 192.54 sec   HDFS Read: 1193268326 HDFS Write: 131 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 12 seconds 540 msec
OK
Time taken: 50.567 seconds, Fetched: 3 row(s)
2016-03-10 19:11:41 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:11:41 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160306
2016-03-10 19:11:41 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:11:41 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:11:42 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:11:42 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:11:42 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:11:42 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:11:43 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:11:43 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:11:43 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:11:43 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1068millis
2016-03-10 19:11:43 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:11:43 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:11:43 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:11:43 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:11:43 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160307.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310191154_a90675a0-c48c-4ca8-93a9-2bd729cd357e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 16
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025775, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025775/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025775
Hadoop job information for Stage-1: number of mappers: 19; number of reducers: 16
2016-03-10 19:12:11,860 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:12:22,434 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 133.18 sec
2016-03-10 19:12:25,599 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 201.36 sec
2016-03-10 19:12:28,759 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 265.97 sec
2016-03-10 19:12:31,910 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 323.95 sec
2016-03-10 19:12:32,956 Stage-1 map = 56%,  reduce = 0%, Cumulative CPU 332.62 sec
2016-03-10 19:12:34,069 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 349.02 sec
2016-03-10 19:12:35,140 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 380.6 sec
2016-03-10 19:12:36,197 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 386.44 sec
2016-03-10 19:12:38,309 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 408.5 sec
2016-03-10 19:12:39,359 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 409.91 sec
2016-03-10 19:12:40,412 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 422.79 sec
2016-03-10 19:12:41,463 Stage-1 map = 89%,  reduce = 0%, Cumulative CPU 426.64 sec
2016-03-10 19:12:42,516 Stage-1 map = 92%,  reduce = 2%, Cumulative CPU 429.03 sec
2016-03-10 19:12:43,566 Stage-1 map = 94%,  reduce = 25%, Cumulative CPU 444.96 sec
2016-03-10 19:12:45,672 Stage-1 map = 97%,  reduce = 26%, Cumulative CPU 451.17 sec
2016-03-10 19:12:46,723 Stage-1 map = 100%,  reduce = 28%, Cumulative CPU 452.51 sec
2016-03-10 19:12:47,782 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 455.99 sec
2016-03-10 19:12:48,837 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 484.14 sec
2016-03-10 19:12:49,888 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 487.83 sec
MapReduce Total cumulative CPU time: 8 minutes 7 seconds 830 msec
Ended Job = job_1453192496319_1025775
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 19  Reduce: 16   Cumulative CPU: 487.83 sec   HDFS Read: 2747223449 HDFS Write: 232 SUCCESS
Total MapReduce CPU Time Spent: 8 minutes 7 seconds 830 msec
OK
Time taken: 56.4 seconds, Fetched: 4 row(s)
./data/mz_pv_20160307.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310191302_42110b3a-ddd1-48a9-a8b6-c292f61c3745
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025791, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025791/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025791
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 6
2016-03-10 19:13:19,273 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:13:30,487 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 33.95 sec
2016-03-10 19:13:31,552 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 47.78 sec
2016-03-10 19:13:33,680 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 85.0 sec
2016-03-10 19:13:34,991 Stage-1 map = 15%,  reduce = 0%, Cumulative CPU 93.84 sec
2016-03-10 19:13:36,077 Stage-1 map = 16%,  reduce = 0%, Cumulative CPU 96.54 sec
2016-03-10 19:13:37,136 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 106.96 sec
2016-03-10 19:13:38,191 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 114.06 sec
2016-03-10 19:13:40,417 Stage-1 map = 35%,  reduce = 0%, Cumulative CPU 124.84 sec
2016-03-10 19:13:41,849 Stage-1 map = 37%,  reduce = 0%, Cumulative CPU 130.48 sec
2016-03-10 19:13:42,962 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 142.66 sec
2016-03-10 19:13:44,018 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 147.66 sec
2016-03-10 19:13:45,854 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 154.54 sec
2016-03-10 19:13:46,908 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 160.13 sec
2016-03-10 19:13:49,020 Stage-1 map = 68%,  reduce = 0%, Cumulative CPU 165.8 sec
2016-03-10 19:13:50,079 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 171.65 sec
2016-03-10 19:13:51,145 Stage-1 map = 76%,  reduce = 2%, Cumulative CPU 172.75 sec
2016-03-10 19:13:52,221 Stage-1 map = 78%,  reduce = 2%, Cumulative CPU 179.29 sec
2016-03-10 19:13:53,280 Stage-1 map = 84%,  reduce = 9%, Cumulative CPU 184.18 sec
2016-03-10 19:13:54,340 Stage-1 map = 84%,  reduce = 10%, Cumulative CPU 184.31 sec
2016-03-10 19:13:55,393 Stage-1 map = 85%,  reduce = 14%, Cumulative CPU 187.14 sec
2016-03-10 19:13:56,457 Stage-1 map = 85%,  reduce = 20%, Cumulative CPU 191.0 sec
2016-03-10 19:13:57,519 Stage-1 map = 85%,  reduce = 24%, Cumulative CPU 192.44 sec
2016-03-10 19:13:59,639 Stage-1 map = 86%,  reduce = 24%, Cumulative CPU 197.83 sec
2016-03-10 19:14:01,741 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 201.01 sec
2016-03-10 19:14:05,172 Stage-1 map = 94%,  reduce = 24%, Cumulative CPU 208.75 sec
2016-03-10 19:14:06,231 Stage-1 map = 100%,  reduce = 24%, Cumulative CPU 210.79 sec
2016-03-10 19:14:07,282 Stage-1 map = 100%,  reduce = 26%, Cumulative CPU 211.56 sec
2016-03-10 19:14:10,391 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 226.09 sec
2016-03-10 19:14:12,477 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 229.22 sec
2016-03-10 19:14:13,532 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 232.71 sec
MapReduce Total cumulative CPU time: 3 minutes 52 seconds 710 msec
Ended Job = job_1453192496319_1025791
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 6   Cumulative CPU: 233.11 sec   HDFS Read: 892720900 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 53 seconds 110 msec
OK
Time taken: 73.544 seconds, Fetched: 3 row(s)
2016-03-10 19:14:16 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:14:16 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160307
2016-03-10 19:14:16 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:14:16 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:14:16 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:14:16 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:14:16 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:14:16 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:14:18 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:14:18 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:14:18 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:14:18 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1098millis
2016-03-10 19:14:18 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:14:18 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:14:18 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:14:18 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:14:18 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
./data/pc_pv_20160308.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310191429_b87af45b-c527-4f3c-91a2-05e2060837aa
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 16
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025796, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025796/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025796
Hadoop job information for Stage-1: number of mappers: 18; number of reducers: 16
2016-03-10 19:14:47,126 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:14:57,713 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 12.86 sec
2016-03-10 19:14:58,774 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 55.66 sec
2016-03-10 19:15:00,879 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 113.09 sec
2016-03-10 19:15:01,937 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 135.57 sec
2016-03-10 19:15:02,996 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 153.49 sec
2016-03-10 19:15:04,053 Stage-1 map = 11%,  reduce = 0%, Cumulative CPU 182.22 sec
2016-03-10 19:15:05,107 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 201.56 sec
2016-03-10 19:15:06,167 Stage-1 map = 19%,  reduce = 0%, Cumulative CPU 211.17 sec
2016-03-10 19:15:07,218 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 231.1 sec
2016-03-10 19:15:08,270 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 244.13 sec
2016-03-10 19:15:09,325 Stage-1 map = 24%,  reduce = 0%, Cumulative CPU 256.66 sec
2016-03-10 19:15:10,386 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 277.06 sec
2016-03-10 19:15:11,438 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 289.08 sec
2016-03-10 19:15:12,487 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 304.37 sec
2016-03-10 19:15:13,546 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 320.33 sec
2016-03-10 19:15:14,597 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 334.14 sec
2016-03-10 19:15:16,686 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 359.24 sec
2016-03-10 19:15:17,752 Stage-1 map = 50%,  reduce = 5%, Cumulative CPU 382.41 sec
2016-03-10 19:15:18,803 Stage-1 map = 55%,  reduce = 6%, Cumulative CPU 393.83 sec
2016-03-10 19:15:19,854 Stage-1 map = 59%,  reduce = 6%, Cumulative CPU 407.4 sec
2016-03-10 19:15:20,907 Stage-1 map = 60%,  reduce = 10%, Cumulative CPU 423.25 sec
2016-03-10 19:15:21,957 Stage-1 map = 64%,  reduce = 11%, Cumulative CPU 433.87 sec
2016-03-10 19:15:23,017 Stage-1 map = 70%,  reduce = 12%, Cumulative CPU 447.34 sec
2016-03-10 19:15:24,068 Stage-1 map = 71%,  reduce = 15%, Cumulative CPU 460.92 sec
2016-03-10 19:15:25,117 Stage-1 map = 78%,  reduce = 16%, Cumulative CPU 471.09 sec
2016-03-10 19:15:26,173 Stage-1 map = 84%,  reduce = 17%, Cumulative CPU 485.84 sec
2016-03-10 19:15:27,229 Stage-1 map = 84%,  reduce = 22%, Cumulative CPU 490.5 sec
2016-03-10 19:15:28,278 Stage-1 map = 84%,  reduce = 24%, Cumulative CPU 494.04 sec
2016-03-10 19:15:29,326 Stage-1 map = 85%,  reduce = 24%, Cumulative CPU 501.27 sec
2016-03-10 19:15:30,378 Stage-1 map = 88%,  reduce = 24%, Cumulative CPU 507.87 sec
2016-03-10 19:15:31,428 Stage-1 map = 88%,  reduce = 25%, Cumulative CPU 509.04 sec
2016-03-10 19:15:33,531 Stage-1 map = 88%,  reduce = 26%, Cumulative CPU 519.19 sec
2016-03-10 19:15:34,585 Stage-1 map = 91%,  reduce = 26%, Cumulative CPU 523.15 sec
2016-03-10 19:15:35,640 Stage-1 map = 91%,  reduce = 27%, Cumulative CPU 526.65 sec
2016-03-10 19:15:36,686 Stage-1 map = 97%,  reduce = 28%, Cumulative CPU 532.53 sec
2016-03-10 19:15:37,735 Stage-1 map = 98%,  reduce = 29%, Cumulative CPU 534.56 sec
2016-03-10 19:15:38,783 Stage-1 map = 100%,  reduce = 48%, Cumulative CPU 541.94 sec
2016-03-10 19:15:39,828 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 559.1 sec
2016-03-10 19:15:40,876 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 569.92 sec
2016-03-10 19:15:41,924 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 571.33 sec
2016-03-10 19:15:42,968 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 575.63 sec
MapReduce Total cumulative CPU time: 9 minutes 35 seconds 630 msec
Ended Job = job_1453192496319_1025796
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 18  Reduce: 16   Cumulative CPU: 575.63 sec   HDFS Read: 2830873995 HDFS Write: 237 SUCCESS
Total MapReduce CPU Time Spent: 9 minutes 35 seconds 630 msec
OK
Time taken: 74.951 seconds, Fetched: 4 row(s)
./data/mz_pv_20160308.log

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160310191555_179b98bf-beae-428b-87d2-19c52064efb9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1453192496319_1025809, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1453192496319_1025809/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1453192496319_1025809
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 6
2016-03-10 19:16:12,142 Stage-1 map = 0%,  reduce = 0%
2016-03-10 19:16:22,713 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 22.4 sec
2016-03-10 19:16:23,767 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 37.5 sec
2016-03-10 19:16:25,875 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 49.97 sec
2016-03-10 19:16:29,014 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 68.51 sec
2016-03-10 19:16:30,056 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 78.98 sec
2016-03-10 19:16:31,106 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 80.92 sec
2016-03-10 19:16:32,156 Stage-1 map = 38%,  reduce = 0%, Cumulative CPU 101.44 sec
2016-03-10 19:16:33,225 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 104.3 sec
2016-03-10 19:16:35,334 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 119.17 sec
2016-03-10 19:16:36,386 Stage-1 map = 54%,  reduce = 0%, Cumulative CPU 123.97 sec
2016-03-10 19:16:38,505 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 136.39 sec
2016-03-10 19:16:39,554 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 139.36 sec
2016-03-10 19:16:40,601 Stage-1 map = 66%,  reduce = 0%, Cumulative CPU 141.52 sec
2016-03-10 19:16:41,670 Stage-1 map = 72%,  reduce = 5%, Cumulative CPU 154.08 sec
2016-03-10 19:16:42,718 Stage-1 map = 78%,  reduce = 7%, Cumulative CPU 156.34 sec
2016-03-10 19:16:43,769 Stage-1 map = 84%,  reduce = 10%, Cumulative CPU 159.49 sec
2016-03-10 19:16:44,819 Stage-1 map = 91%,  reduce = 21%, Cumulative CPU 165.29 sec
2016-03-10 19:16:45,880 Stage-1 map = 93%,  reduce = 23%, Cumulative CPU 169.39 sec
2016-03-10 19:16:46,928 Stage-1 map = 93%,  reduce = 25%, Cumulative CPU 169.5 sec
2016-03-10 19:16:47,983 Stage-1 map = 93%,  reduce = 29%, Cumulative CPU 170.0 sec
2016-03-10 19:16:49,030 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 173.84 sec
2016-03-10 19:16:50,084 Stage-1 map = 100%,  reduce = 35%, Cumulative CPU 173.98 sec
2016-03-10 19:16:51,134 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 184.57 sec
2016-03-10 19:16:52,178 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 187.64 sec
MapReduce Total cumulative CPU time: 3 minutes 7 seconds 640 msec
Ended Job = job_1453192496319_1025809
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 6   Cumulative CPU: 187.64 sec   HDFS Read: 952783734 HDFS Write: 128 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 7 seconds 640 msec
OK
Time taken: 57.905 seconds, Fetched: 3 row(s)
2016-03-10 19:16:54 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-cashier-pvuv.xml
2016-03-10 19:16:54 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20160308
2016-03-10 19:16:54 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:16:54 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2016-03-10 19:16:54 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-cashier-pvuv.xml
2016-03-10 19:16:54 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2016-03-10 19:16:54 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台流量
2016-03-10 19:16:54 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台流量，防止重复导入
2016-03-10 19:16:55 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:12
2016-03-10 19:16:55 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2016-03-10 19:16:55 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:12
2016-03-10 19:16:55 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1086millis
2016-03-10 19:16:55 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2016-03-10 19:16:55 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：12
2016-03-10 19:16:55 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台流量执行结束。
2016-03-10 19:16:55 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-cashier-pvuv.xml
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2016-03-10 19:16:55 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
