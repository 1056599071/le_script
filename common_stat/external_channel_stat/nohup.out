
Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151125095759_ee971bc2-73e2-4ded-9252-e248c81b9194
Total jobs = 6
Stage-11 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298583, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298583/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298583
Hadoop job information for Stage-3: number of mappers: 17; number of reducers: 3
2015-11-25 09:58:26,853 Stage-3 map = 0%,  reduce = 0%
2015-11-25 09:58:42,064 Stage-3 map = 8%,  reduce = 0%, Cumulative CPU 30.1 sec
2015-11-25 09:58:43,127 Stage-3 map = 14%,  reduce = 0%, Cumulative CPU 66.2 sec
2015-11-25 09:58:44,190 Stage-3 map = 41%,  reduce = 0%, Cumulative CPU 139.97 sec
2015-11-25 09:58:45,259 Stage-3 map = 56%,  reduce = 0%, Cumulative CPU 177.85 sec
2015-11-25 09:58:46,328 Stage-3 map = 66%,  reduce = 0%, Cumulative CPU 181.53 sec
2015-11-25 09:58:47,383 Stage-3 map = 74%,  reduce = 0%, Cumulative CPU 228.63 sec
2015-11-25 09:58:48,441 Stage-3 map = 86%,  reduce = 0%, Cumulative CPU 261.94 sec
2015-11-25 09:58:50,562 Stage-3 map = 96%,  reduce = 0%, Cumulative CPU 269.87 sec
2015-11-25 09:58:51,630 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 271.87 sec
2015-11-25 09:58:55,896 Stage-3 map = 100%,  reduce = 25%, Cumulative CPU 276.57 sec
2015-11-25 09:58:56,959 Stage-3 map = 100%,  reduce = 48%, Cumulative CPU 283.37 sec
2015-11-25 09:58:58,028 Stage-3 map = 100%,  reduce = 84%, Cumulative CPU 293.36 sec
2015-11-25 09:58:59,095 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 297.92 sec
MapReduce Total cumulative CPU time: 4 minutes 57 seconds 920 msec
Ended Job = job_1445566335713_298583
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298596, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298596/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298596
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2015-11-25 09:59:09,226 Stage-4 map = 0%,  reduce = 0%
2015-11-25 09:59:14,495 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 0.93 sec
2015-11-25 09:59:20,860 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.36 sec
MapReduce Total cumulative CPU time: 2 seconds 360 msec
Ended Job = job_1445566335713_298596
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298602, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298602/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298602
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2015-11-25 09:59:28,681 Stage-1 map = 0%,  reduce = 0%
2015-11-25 09:59:36,430 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 49.91 sec
2015-11-25 09:59:42,778 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 49.91 sec
MapReduce Total cumulative CPU time: 49 seconds 910 msec
Ended Job = job_1445566335713_298602
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_298603, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_298603/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_298603
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2015-11-25 09:59:50,650 Stage-2 map = 0%,  reduce = 0%
2015-11-25 09:59:56,008 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec
2015-11-25 10:00:05,890 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.94 sec
MapReduce Total cumulative CPU time: 4 seconds 940 msec
Ended Job = job_1445566335713_298603
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 17  Reduce: 3   Cumulative CPU: 297.92 sec   HDFS Read: 1632533692 HDFS Write: 288 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 2.36 sec   HDFS Read: 5574 HDFS Write: 96 SUCCESS
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 52.82 sec   HDFS Read: 53252074 HDFS Write: 96 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.94 sec   HDFS Read: 8220 HDFS Write: 8 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 58 seconds 40 msec
OK
Time taken: 127.231 seconds

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151125103110_680cbd6d-e8e0-43b0-b8d8-466b64983af6
Total jobs = 5
Stage-3 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_299147, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_299147/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_299147
Hadoop job information for Stage-3: number of mappers: 19; number of reducers: 4
2015-11-25 10:31:34,493 Stage-3 map = 0%,  reduce = 0%
2015-11-25 10:31:55,955 Stage-3 map = 14%,  reduce = 0%, Cumulative CPU 69.78 sec
2015-11-25 10:31:57,010 Stage-3 map = 36%,  reduce = 0%, Cumulative CPU 113.47 sec
2015-11-25 10:31:58,070 Stage-3 map = 52%,  reduce = 0%, Cumulative CPU 124.9 sec
2015-11-25 10:31:59,139 Stage-3 map = 63%,  reduce = 0%, Cumulative CPU 156.3 sec
2015-11-25 10:32:00,204 Stage-3 map = 70%,  reduce = 0%, Cumulative CPU 176.34 sec
2015-11-25 10:32:01,270 Stage-3 map = 84%,  reduce = 0%, Cumulative CPU 181.7 sec
2015-11-25 10:32:02,329 Stage-3 map = 89%,  reduce = 0%, Cumulative CPU 191.95 sec
2015-11-25 10:32:05,519 Stage-3 map = 93%,  reduce = 0%, Cumulative CPU 200.87 sec
2015-11-25 10:32:07,658 Stage-3 map = 95%,  reduce = 30%, Cumulative CPU 209.3 sec
2015-11-25 10:32:09,792 Stage-3 map = 95%,  reduce = 31%, Cumulative CPU 214.54 sec
2015-11-25 10:32:10,862 Stage-3 map = 95%,  reduce = 32%, Cumulative CPU 214.81 sec
2015-11-25 10:32:15,119 Stage-3 map = 98%,  reduce = 32%, Cumulative CPU 221.54 sec
2015-11-25 10:32:18,487 Stage-3 map = 99%,  reduce = 32%, Cumulative CPU 225.32 sec
2015-11-25 10:32:19,552 Stage-3 map = 100%,  reduce = 32%, Cumulative CPU 226.96 sec
2015-11-25 10:32:22,722 Stage-3 map = 100%,  reduce = 48%, Cumulative CPU 234.12 sec
2015-11-25 10:32:24,847 Stage-3 map = 100%,  reduce = 57%, Cumulative CPU 239.0 sec
2015-11-25 10:32:25,901 Stage-3 map = 100%,  reduce = 64%, Cumulative CPU 247.89 sec
2015-11-25 10:32:29,082 Stage-3 map = 100%,  reduce = 80%, Cumulative CPU 267.62 sec
2015-11-25 10:32:30,147 Stage-3 map = 100%,  reduce = 84%, Cumulative CPU 274.36 sec
2015-11-25 10:32:31,221 Stage-3 map = 100%,  reduce = 87%, Cumulative CPU 277.46 sec
2015-11-25 10:32:32,274 Stage-3 map = 100%,  reduce = 88%, Cumulative CPU 281.54 sec
2015-11-25 10:32:34,405 Stage-3 map = 100%,  reduce = 92%, Cumulative CPU 286.97 sec
2015-11-25 10:32:35,468 Stage-3 map = 100%,  reduce = 96%, Cumulative CPU 289.82 sec
2015-11-25 10:32:37,769 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 293.65 sec
MapReduce Total cumulative CPU time: 4 minutes 53 seconds 650 msec
Ended Job = job_1445566335713_299147
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_299165, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_299165/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_299165
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2015-11-25 10:32:47,830 Stage-4 map = 0%,  reduce = 0%
2015-11-25 10:33:00,517 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 7.09 sec
2015-11-25 10:33:14,256 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 13.23 sec
MapReduce Total cumulative CPU time: 13 seconds 230 msec
Ended Job = job_1445566335713_299165
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_299179, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_299179/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_299179
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2015-11-25 10:33:20,115 Stage-1 map = 0%,  reduce = 0%
2015-11-25 10:33:26,429 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.93 sec
2015-11-25 10:33:29,608 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 11.51 sec
2015-11-25 10:33:36,992 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 21.39 sec
MapReduce Total cumulative CPU time: 21 seconds 390 msec
Ended Job = job_1445566335713_299179
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_299187, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_299187/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_299187
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2015-11-25 10:33:45,554 Stage-2 map = 0%,  reduce = 0%
2015-11-25 10:33:58,410 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 12.96 sec
2015-11-25 10:34:09,019 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 20.35 sec
MapReduce Total cumulative CPU time: 20 seconds 350 msec
Ended Job = job_1445566335713_299187
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 19  Reduce: 4   Cumulative CPU: 294.97 sec   HDFS Read: 2277133303 HDFS Write: 60081803 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 13.23 sec   HDFS Read: 60087350 HDFS Write: 60081835 SUCCESS
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 21.39 sec   HDFS Read: 113333813 HDFS Write: 69114109 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 20.35 sec   HDFS Read: 69122233 HDFS Write: 2884 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 49 seconds 940 msec
OK
Time taken: 180.183 seconds, Fetched: 329 row(s)
2015-11-25 10:34:10 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-external-channels-income.xml
2015-11-25 10:34:10 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20151124
2015-11-25 10:34:11 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-25 10:34:11 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-25 10:34:11 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-external-channels-income.xml
2015-11-25 10:34:11 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2015-11-25 10:34:11 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台付费率
2015-11-25 10:34:11 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台相关数据，防止重复导入
2015-11-25 10:34:12 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:0
2015-11-25 10:34:12 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2015-11-25 10:34:12 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:0
2015-11-25 10:34:12 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1230millis
2015-11-25 10:34:14 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2015-11-25 10:34:14 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：329
2015-11-25 10:34:14 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台付费率执行结束。
2015-11-25 10:34:14 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-external-channels-income.xml
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2015-11-25 10:34:14 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151126104042_4dd6bf50-a8da-450a-97b4-4c66e934f3f7
Total jobs = 5
Stage-3 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_316428, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_316428/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_316428
Hadoop job information for Stage-3: number of mappers: 20; number of reducers: 4
2015-11-26 10:41:04,199 Stage-3 map = 0%,  reduce = 0%
2015-11-26 10:41:11,730 Stage-3 map = 5%,  reduce = 0%, Cumulative CPU 4.53 sec
2015-11-26 10:41:13,846 Stage-3 map = 18%,  reduce = 0%, Cumulative CPU 149.08 sec
2015-11-26 10:41:14,897 Stage-3 map = 28%,  reduce = 0%, Cumulative CPU 246.98 sec
2015-11-26 10:41:15,954 Stage-3 map = 46%,  reduce = 0%, Cumulative CPU 277.19 sec
2015-11-26 10:41:17,031 Stage-3 map = 57%,  reduce = 0%, Cumulative CPU 353.23 sec
2015-11-26 10:41:18,085 Stage-3 map = 62%,  reduce = 0%, Cumulative CPU 357.19 sec
2015-11-26 10:41:19,142 Stage-3 map = 67%,  reduce = 0%, Cumulative CPU 387.78 sec
2015-11-26 10:41:20,203 Stage-3 map = 76%,  reduce = 0%, Cumulative CPU 406.1 sec
2015-11-26 10:41:21,257 Stage-3 map = 81%,  reduce = 0%, Cumulative CPU 407.81 sec
2015-11-26 10:41:22,328 Stage-3 map = 81%,  reduce = 13%, Cumulative CPU 415.48 sec
2015-11-26 10:41:23,627 Stage-3 map = 86%,  reduce = 20%, Cumulative CPU 424.95 sec
2015-11-26 10:41:25,769 Stage-3 map = 86%,  reduce = 27%, Cumulative CPU 433.54 sec
2015-11-26 10:41:28,981 Stage-3 map = 87%,  reduce = 27%, Cumulative CPU 447.79 sec
2015-11-26 10:41:32,161 Stage-3 map = 91%,  reduce = 28%, Cumulative CPU 455.98 sec
2015-11-26 10:41:35,346 Stage-3 map = 96%,  reduce = 28%, Cumulative CPU 465.61 sec
2015-11-26 10:41:37,473 Stage-3 map = 100%,  reduce = 29%, Cumulative CPU 471.85 sec
2015-11-26 10:41:38,543 Stage-3 map = 100%,  reduce = 31%, Cumulative CPU 472.97 sec
2015-11-26 10:41:39,609 Stage-3 map = 100%,  reduce = 32%, Cumulative CPU 473.83 sec
2015-11-26 10:41:40,663 Stage-3 map = 100%,  reduce = 43%, Cumulative CPU 477.96 sec
2015-11-26 10:41:41,726 Stage-3 map = 100%,  reduce = 51%, Cumulative CPU 481.31 sec
2015-11-26 10:41:42,784 Stage-3 map = 100%,  reduce = 60%, Cumulative CPU 485.85 sec
2015-11-26 10:41:43,853 Stage-3 map = 100%,  reduce = 64%, Cumulative CPU 490.09 sec
2015-11-26 10:41:44,923 Stage-3 map = 100%,  reduce = 73%, Cumulative CPU 499.67 sec
2015-11-26 10:41:46,029 Stage-3 map = 100%,  reduce = 75%, Cumulative CPU 503.74 sec
2015-11-26 10:41:47,091 Stage-3 map = 100%,  reduce = 84%, Cumulative CPU 510.06 sec
2015-11-26 10:41:48,154 Stage-3 map = 100%,  reduce = 88%, Cumulative CPU 513.27 sec
2015-11-26 10:41:49,214 Stage-3 map = 100%,  reduce = 94%, Cumulative CPU 520.92 sec
2015-11-26 10:41:50,273 Stage-3 map = 100%,  reduce = 98%, Cumulative CPU 538.94 sec
2015-11-26 10:41:51,535 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 542.45 sec
MapReduce Total cumulative CPU time: 9 minutes 2 seconds 450 msec
Ended Job = job_1445566335713_316428
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_316437, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_316437/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_316437
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2015-11-26 10:42:19,659 Stage-4 map = 0%,  reduce = 0%
2015-11-26 10:42:33,031 Stage-4 map = 67%,  reduce = 0%, Cumulative CPU 14.6 sec
2015-11-26 10:42:35,214 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 35.05 sec
2015-11-26 10:42:51,274 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 45.18 sec
MapReduce Total cumulative CPU time: 45 seconds 180 msec
Ended Job = job_1445566335713_316437
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_316449, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_316449/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_316449
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2015-11-26 10:43:00,527 Stage-1 map = 0%,  reduce = 0%
2015-11-26 10:43:09,027 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.64 sec
2015-11-26 10:43:13,493 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.34 sec
2015-11-26 10:43:20,887 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 36.95 sec
MapReduce Total cumulative CPU time: 36 seconds 950 msec
Ended Job = job_1445566335713_316449
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_316456, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_316456/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_316456
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2015-11-26 10:43:26,613 Stage-2 map = 0%,  reduce = 0%
2015-11-26 10:43:43,636 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 37.62 sec
2015-11-26 10:43:47,060 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 40.01 sec
2015-11-26 10:43:56,561 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 47.2 sec
MapReduce Total cumulative CPU time: 47 seconds 200 msec
Ended Job = job_1445566335713_316456
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 20  Reduce: 4   Cumulative CPU: 542.45 sec   HDFS Read: 2200041228 HDFS Write: 58584462 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 45.18 sec   HDFS Read: 58590009 HDFS Write: 58584414 SUCCESS
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 36.95 sec   HDFS Read: 102210777 HDFS Write: 66972683 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 47.2 sec   HDFS Read: 66980807 HDFS Write: 3041 SUCCESS
Total MapReduce CPU Time Spent: 11 minutes 11 seconds 780 msec
OK
Time taken: 195.398 seconds, Fetched: 352 row(s)
2015-11-26 10:43:58 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-external-channels-income.xml
2015-11-26 10:43:58 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20151125
2015-11-26 10:43:58 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-26 10:43:58 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-26 10:43:58 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-external-channels-income.xml
2015-11-26 10:43:58 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2015-11-26 10:43:58 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台付费率
2015-11-26 10:43:58 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台相关数据，防止重复导入
2015-11-26 10:43:59 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:0
2015-11-26 10:43:59 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2015-11-26 10:43:59 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:0
2015-11-26 10:43:59 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1153millis
2015-11-26 10:44:02 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2015-11-26 10:44:02 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：352
2015-11-26 10:44:02 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台付费率执行结束。
2015-11-26 10:44:02 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-external-channels-income.xml
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2015-11-26 10:44:02 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/letv/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/letv/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/letv/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/letv/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20151127102910_52d5123b-cb46-47cd-9e84-2f6f2e24ceb1
Total jobs = 5
Stage-3 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_333476, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_333476/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_333476
Hadoop job information for Stage-3: number of mappers: 17; number of reducers: 4
2015-11-27 10:29:48,608 Stage-3 map = 0%,  reduce = 0%
2015-11-27 10:29:59,238 Stage-3 map = 6%,  reduce = 0%, Cumulative CPU 57.81 sec
2015-11-27 10:30:00,297 Stage-3 map = 12%,  reduce = 0%, Cumulative CPU 106.75 sec
2015-11-27 10:30:01,365 Stage-3 map = 18%,  reduce = 0%, Cumulative CPU 189.36 sec
2015-11-27 10:30:02,427 Stage-3 map = 21%,  reduce = 0%, Cumulative CPU 326.43 sec
2015-11-27 10:30:03,479 Stage-3 map = 26%,  reduce = 0%, Cumulative CPU 380.07 sec
2015-11-27 10:30:04,546 Stage-3 map = 39%,  reduce = 0%, Cumulative CPU 402.69 sec
2015-11-27 10:30:05,611 Stage-3 map = 45%,  reduce = 0%, Cumulative CPU 416.39 sec
2015-11-27 10:30:06,670 Stage-3 map = 48%,  reduce = 0%, Cumulative CPU 428.45 sec
2015-11-27 10:30:07,739 Stage-3 map = 52%,  reduce = 0%, Cumulative CPU 441.2 sec
2015-11-27 10:30:08,840 Stage-3 map = 58%,  reduce = 0%, Cumulative CPU 455.62 sec
2015-11-27 10:30:09,914 Stage-3 map = 71%,  reduce = 0%, Cumulative CPU 467.41 sec
2015-11-27 10:30:10,990 Stage-3 map = 72%,  reduce = 0%, Cumulative CPU 476.53 sec
2015-11-27 10:30:12,065 Stage-3 map = 78%,  reduce = 0%, Cumulative CPU 493.23 sec
2015-11-27 10:30:13,128 Stage-3 map = 86%,  reduce = 0%, Cumulative CPU 502.16 sec
2015-11-27 10:30:14,437 Stage-3 map = 94%,  reduce = 13%, Cumulative CPU 510.67 sec
2015-11-27 10:30:16,561 Stage-3 map = 94%,  reduce = 23%, Cumulative CPU 518.24 sec
2015-11-27 10:30:17,615 Stage-3 map = 94%,  reduce = 31%, Cumulative CPU 520.33 sec
2015-11-27 10:30:37,813 Stage-3 map = 98%,  reduce = 31%, Cumulative CPU 549.28 sec
2015-11-27 10:30:40,979 Stage-3 map = 99%,  reduce = 31%, Cumulative CPU 553.02 sec
2015-11-27 10:30:42,042 Stage-3 map = 100%,  reduce = 31%, Cumulative CPU 554.42 sec
2015-11-27 10:30:44,349 Stage-3 map = 100%,  reduce = 39%, Cumulative CPU 558.23 sec
2015-11-27 10:30:45,405 Stage-3 map = 100%,  reduce = 44%, Cumulative CPU 560.94 sec
2015-11-27 10:30:46,456 Stage-3 map = 100%,  reduce = 53%, Cumulative CPU 564.49 sec
2015-11-27 10:30:47,503 Stage-3 map = 100%,  reduce = 58%, Cumulative CPU 582.38 sec
2015-11-27 10:30:48,566 Stage-3 map = 100%,  reduce = 62%, Cumulative CPU 587.2 sec
2015-11-27 10:30:49,618 Stage-3 map = 100%,  reduce = 71%, Cumulative CPU 596.47 sec
2015-11-27 10:30:50,681 Stage-3 map = 100%,  reduce = 75%, Cumulative CPU 599.56 sec
2015-11-27 10:30:51,737 Stage-3 map = 100%,  reduce = 79%, Cumulative CPU 602.67 sec
2015-11-27 10:30:53,849 Stage-3 map = 100%,  reduce = 86%, Cumulative CPU 623.0 sec
2015-11-27 10:30:54,901 Stage-3 map = 100%,  reduce = 92%, Cumulative CPU 636.05 sec
2015-11-27 10:30:57,000 Stage-3 map = 100%,  reduce = 93%, Cumulative CPU 639.13 sec
2015-11-27 10:30:58,046 Stage-3 map = 100%,  reduce = 95%, Cumulative CPU 641.47 sec
2015-11-27 10:31:02,264 Stage-3 map = 100%,  reduce = 98%, Cumulative CPU 645.23 sec
2015-11-27 10:31:05,452 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 650.09 sec
MapReduce Total cumulative CPU time: 10 minutes 50 seconds 90 msec
Ended Job = job_1445566335713_333476
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_333493, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_333493/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_333493
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2015-11-27 10:31:17,528 Stage-4 map = 0%,  reduce = 0%
2015-11-27 10:31:36,738 Stage-4 map = 67%,  reduce = 0%, Cumulative CPU 11.62 sec
2015-11-27 10:31:37,806 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 13.58 sec
2015-11-27 10:31:53,597 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 21.56 sec
MapReduce Total cumulative CPU time: 21 seconds 560 msec
Ended Job = job_1445566335713_333493
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_333503, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_333503/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_333503
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2015-11-27 10:32:04,384 Stage-1 map = 0%,  reduce = 0%
2015-11-27 10:32:16,645 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 32.55 sec
2015-11-27 10:32:17,697 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 52.73 sec
2015-11-27 10:32:38,785 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 58.53 sec
2015-11-27 10:32:41,984 Stage-1 map = 100%,  reduce = 72%, Cumulative CPU 67.44 sec
2015-11-27 10:32:45,368 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 71.86 sec
2015-11-27 10:32:48,556 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 76.21 sec
2015-11-27 10:32:49,606 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 76.51 sec
MapReduce Total cumulative CPU time: 1 minutes 16 seconds 510 msec
Ended Job = job_1445566335713_333503
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1445566335713_333521, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1445566335713_333521/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1445566335713_333521
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2015-11-27 10:33:08,416 Stage-2 map = 0%,  reduce = 0%
2015-11-27 10:33:27,392 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 24.85 sec
2015-11-27 10:33:30,530 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 28.56 sec
2015-11-27 10:33:43,697 Stage-2 map = 100%,  reduce = 83%, Cumulative CPU 36.91 sec
2015-11-27 10:33:44,755 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 38.96 sec
MapReduce Total cumulative CPU time: 38 seconds 960 msec
Ended Job = job_1445566335713_333521
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 17  Reduce: 4   Cumulative CPU: 650.09 sec   HDFS Read: 2203764721 HDFS Write: 58089655 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 21.56 sec   HDFS Read: 58095196 HDFS Write: 58089547 SUCCESS
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 76.51 sec   HDFS Read: 119039843 HDFS Write: 66236805 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 38.96 sec   HDFS Read: 66244923 HDFS Write: 3053 SUCCESS
Total MapReduce CPU Time Spent: 13 minutes 7 seconds 120 msec
OK
Time taken: 275.376 seconds, Fetched: 351 row(s)
2015-11-27 10:33:46 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:file=stat-sql-external-channels-income.xml
2015-11-27 10:33:46 [INFO ] com.celery.stat.core.Context {Context.java:15} - 往作用域添加变量:sdate=20151126
2015-11-27 10:33:47 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-27 10:33:47 [INFO ] org.logicalcobwebs.proxool.ProxoolFacade {ProxoolFacade.java:86} - Proxool 0.9.1 (23-Aug-2008 11:10)
2015-11-27 10:33:47 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:40} - 执行传入参数文件:stat-sql-external-channels-income.xml
2015-11-27 10:33:47 [INFO ] com.celery.stat.core.Executor {Executor.java:18} - 异常节点将跳过，继续执行命令.
2015-11-27 10:33:47 [INFO ] com.celery.stat.core.Executor {Executor.java:22} - 开始执行文件命令:计算收银台付费率
2015-11-27 10:33:47 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:64} - 开始USqlBean:删除收银台相关数据，防止重复导入
2015-11-27 10:33:49 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:386} - 执行影响条数:351
2015-11-27 10:33:49 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:388} - SQL语句执行结果:false
2015-11-27 10:33:49 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:68} - 执行USqlBean影响的数目:351
2015-11-27 10:33:49 [INFO ] com.celery.stat.function.USqlBean {USqlBean.java:69} - 执行完成，耗时:1959millis
2015-11-27 10:33:51 [INFO ] com.celery.stat.core.DBHelper {DBHelper.java:279} - 插入数据成功
2015-11-27 10:33:51 [INFO ] com.celery.stat.function.CsvToDBBean {CsvToDBBean.java:152} - 插入数据rows：351
2015-11-27 10:33:51 [INFO ] com.celery.stat.core.Executor {Executor.java:36} - 命令:计算收银台付费率执行结束。
2015-11-27 10:33:51 [INFO ] com.celery.stat.main.CeleryMain {CeleryMain.java:42} - 执行传入参数文件结束:stat-sql-external-channels-income.xml
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.letv_boss_online {ConnectionPool.java:484} - Shutting down 'letv_boss_online' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.letv_order_test {ConnectionPool.java:484} - Shutting down 'letv_order_test' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.boss_online_activity {ConnectionPool.java:484} - Shutting down 'boss_online_activity' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.letv_boss_test {ConnectionPool.java:484} - Shutting down 'letv_boss_test' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.boss_online_rec {ConnectionPool.java:484} - Shutting down 'boss_online_rec' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.boss_online_rec_test {ConnectionPool.java:484} - Shutting down 'boss_online_rec_test' pool immediately [Shutdown Hook]
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.PrototyperController {PrototyperController.java:100} - Stopping Prototyper thread
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread
2015-11-27 10:33:51 [INFO ] org.logicalcobwebs.proxool.HouseKeeperController {HouseKeeperController.java:107} - Stopping HouseKeeper thread

Logging initialized using configuration in jar:file:/letv/usr/local/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Query ID = zhaochunlong_20160104165238_b1ee9e39-4d82-4b82-b126-eb88dc9bf2b9
Total jobs = 5
Stage-3 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 66
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1451376659790_91409, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1451376659790_91409/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1451376659790_91409
Hadoop job information for Stage-3: number of mappers: 78; number of reducers: 66
2016-01-04 16:52:56,118 Stage-3 map = 0%,  reduce = 0%
2016-01-04 16:53:05,671 Stage-3 map = 7%,  reduce = 0%, Cumulative CPU 448.57 sec
2016-01-04 16:53:06,730 Stage-3 map = 13%,  reduce = 0%, Cumulative CPU 534.66 sec
2016-01-04 16:53:07,788 Stage-3 map = 19%,  reduce = 0%, Cumulative CPU 602.43 sec
2016-01-04 16:53:08,850 Stage-3 map = 22%,  reduce = 0%, Cumulative CPU 787.47 sec
2016-01-04 16:53:09,910 Stage-3 map = 24%,  reduce = 0%, Cumulative CPU 819.46 sec
2016-01-04 16:53:10,961 Stage-3 map = 30%,  reduce = 0%, Cumulative CPU 855.56 sec
2016-01-04 16:53:12,016 Stage-3 map = 37%,  reduce = 0%, Cumulative CPU 1012.94 sec
2016-01-04 16:53:13,072 Stage-3 map = 38%,  reduce = 0%, Cumulative CPU 1036.83 sec
2016-01-04 16:53:14,120 Stage-3 map = 44%,  reduce = 0%, Cumulative CPU 1066.91 sec
2016-01-04 16:53:15,166 Stage-3 map = 55%,  reduce = 0%, Cumulative CPU 1181.22 sec
2016-01-04 16:53:16,213 Stage-3 map = 59%,  reduce = 0%, Cumulative CPU 1208.09 sec
2016-01-04 16:53:17,268 Stage-3 map = 62%,  reduce = 17%, Cumulative CPU 1259.64 sec
2016-01-04 16:53:18,316 Stage-3 map = 66%,  reduce = 18%, Cumulative CPU 1339.51 sec
2016-01-04 16:53:19,366 Stage-3 map = 68%,  reduce = 18%, Cumulative CPU 1356.19 sec
2016-01-04 16:53:20,427 Stage-3 map = 68%,  reduce = 22%, Cumulative CPU 1373.47 sec
2016-01-04 16:53:23,597 Stage-3 map = 71%,  reduce = 23%, Cumulative CPU 1510.72 sec
2016-01-04 16:53:24,655 Stage-3 map = 83%,  reduce = 23%, Cumulative CPU 1578.68 sec
2016-01-04 16:53:25,709 Stage-3 map = 85%,  reduce = 23%, Cumulative CPU 1590.42 sec
2016-01-04 16:53:26,760 Stage-3 map = 90%,  reduce = 23%, Cumulative CPU 1640.32 sec
2016-01-04 16:53:27,816 Stage-3 map = 93%,  reduce = 23%, Cumulative CPU 1701.72 sec
2016-01-04 16:53:28,867 Stage-3 map = 96%,  reduce = 23%, Cumulative CPU 1722.78 sec
2016-01-04 16:53:29,917 Stage-3 map = 97%,  reduce = 27%, Cumulative CPU 1770.16 sec
2016-01-04 16:53:30,966 Stage-3 map = 98%,  reduce = 28%, Cumulative CPU 1801.86 sec
2016-01-04 16:53:32,026 Stage-3 map = 99%,  reduce = 29%, Cumulative CPU 1823.48 sec
2016-01-04 16:53:33,081 Stage-3 map = 99%,  reduce = 31%, Cumulative CPU 1855.36 sec
2016-01-04 16:53:34,131 Stage-3 map = 100%,  reduce = 32%, Cumulative CPU 1874.79 sec
2016-01-04 16:53:35,176 Stage-3 map = 100%,  reduce = 44%, Cumulative CPU 1938.76 sec
2016-01-04 16:53:36,227 Stage-3 map = 100%,  reduce = 58%, Cumulative CPU 2000.81 sec
2016-01-04 16:53:37,278 Stage-3 map = 100%,  reduce = 64%, Cumulative CPU 2065.15 sec
2016-01-04 16:53:38,329 Stage-3 map = 100%,  reduce = 75%, Cumulative CPU 2221.97 sec
2016-01-04 16:53:39,382 Stage-3 map = 100%,  reduce = 83%, Cumulative CPU 2310.52 sec
2016-01-04 16:53:40,436 Stage-3 map = 100%,  reduce = 88%, Cumulative CPU 2405.08 sec
2016-01-04 16:53:41,485 Stage-3 map = 100%,  reduce = 97%, Cumulative CPU 2595.8 sec
2016-01-04 16:53:42,548 Stage-3 map = 100%,  reduce = 99%, Cumulative CPU 2651.08 sec
2016-01-04 16:53:49,886 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2687.37 sec
MapReduce Total cumulative CPU time: 44 minutes 47 seconds 370 msec
Ended Job = job_1451376659790_91409
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1451376659790_91426, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1451376659790_91426/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1451376659790_91426
Hadoop job information for Stage-4: number of mappers: 4; number of reducers: 4
2016-01-04 16:54:01,386 Stage-4 map = 0%,  reduce = 0%
2016-01-04 16:54:12,901 Stage-4 map = 4%,  reduce = 0%, Cumulative CPU 14.44 sec
2016-01-04 16:54:14,995 Stage-4 map = 6%,  reduce = 0%, Cumulative CPU 28.82 sec
2016-01-04 16:54:16,045 Stage-4 map = 14%,  reduce = 0%, Cumulative CPU 42.17 sec
2016-01-04 16:54:18,137 Stage-4 map = 19%,  reduce = 0%, Cumulative CPU 45.77 sec
2016-01-04 16:54:19,188 Stage-4 map = 24%,  reduce = 0%, Cumulative CPU 60.1 sec
2016-01-04 16:54:21,283 Stage-4 map = 25%,  reduce = 0%, Cumulative CPU 72.21 sec
2016-01-04 16:54:22,336 Stage-4 map = 27%,  reduce = 0%, Cumulative CPU 89.36 sec
2016-01-04 16:54:24,435 Stage-4 map = 29%,  reduce = 0%, Cumulative CPU 92.19 sec
2016-01-04 16:54:25,485 Stage-4 map = 33%,  reduce = 0%, Cumulative CPU 102.82 sec
2016-01-04 16:54:28,624 Stage-4 map = 38%,  reduce = 0%, Cumulative CPU 117.26 sec
2016-01-04 16:54:31,765 Stage-4 map = 48%,  reduce = 0%, Cumulative CPU 145.87 sec
2016-01-04 16:54:33,964 Stage-4 map = 51%,  reduce = 0%, Cumulative CPU 153.3 sec
2016-01-04 16:54:35,013 Stage-4 map = 56%,  reduce = 0%, Cumulative CPU 163.06 sec
2016-01-04 16:54:37,115 Stage-4 map = 58%,  reduce = 0%, Cumulative CPU 169.61 sec
2016-01-04 16:54:40,258 Stage-4 map = 65%,  reduce = 0%, Cumulative CPU 182.84 sec
2016-01-04 16:54:41,313 Stage-4 map = 69%,  reduce = 0%, Cumulative CPU 189.4 sec
2016-01-04 16:54:42,355 Stage-4 map = 72%,  reduce = 0%, Cumulative CPU 191.31 sec
2016-01-04 16:54:43,401 Stage-4 map = 74%,  reduce = 0%, Cumulative CPU 194.71 sec
2016-01-04 16:54:44,449 Stage-4 map = 83%,  reduce = 0%, Cumulative CPU 202.55 sec
2016-01-04 16:54:46,546 Stage-4 map = 87%,  reduce = 0%, Cumulative CPU 205.68 sec
2016-01-04 16:54:47,585 Stage-4 map = 89%,  reduce = 0%, Cumulative CPU 209.76 sec
2016-01-04 16:54:50,724 Stage-4 map = 92%,  reduce = 0%, Cumulative CPU 212.93 sec
2016-01-04 16:54:52,826 Stage-4 map = 92%,  reduce = 25%, Cumulative CPU 283.53 sec
2016-01-04 16:54:55,965 Stage-4 map = 94%,  reduce = 25%, Cumulative CPU 287.32 sec
2016-01-04 16:54:59,110 Stage-4 map = 97%,  reduce = 25%, Cumulative CPU 290.8 sec
2016-01-04 16:55:02,268 Stage-4 map = 100%,  reduce = 25%, Cumulative CPU 294.07 sec
2016-01-04 16:55:05,413 Stage-4 map = 100%,  reduce = 50%, Cumulative CPU 304.24 sec
2016-01-04 16:55:08,562 Stage-4 map = 100%,  reduce = 67%, Cumulative CPU 324.44 sec
2016-01-04 16:55:10,648 Stage-4 map = 100%,  reduce = 71%, Cumulative CPU 331.8 sec
2016-01-04 16:55:11,698 Stage-4 map = 100%,  reduce = 75%, Cumulative CPU 339.26 sec
2016-01-04 16:55:13,799 Stage-4 map = 100%,  reduce = 80%, Cumulative CPU 350.98 sec
2016-01-04 16:55:14,844 Stage-4 map = 100%,  reduce = 83%, Cumulative CPU 355.05 sec
2016-01-04 16:55:16,925 Stage-4 map = 100%,  reduce = 89%, Cumulative CPU 365.97 sec
2016-01-04 16:55:17,978 Stage-4 map = 100%,  reduce = 92%, Cumulative CPU 369.6 sec
2016-01-04 16:55:19,023 Stage-4 map = 100%,  reduce = 95%, Cumulative CPU 374.52 sec
2016-01-04 16:55:20,065 Stage-4 map = 100%,  reduce = 99%, Cumulative CPU 381.82 sec
2016-01-04 16:55:22,153 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 384.02 sec
MapReduce Total cumulative CPU time: 6 minutes 24 seconds 20 msec
Ended Job = job_1451376659790_91426
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1451376659790_91438, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1451376659790_91438/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1451376659790_91438
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 4
2016-01-04 16:55:26,971 Stage-1 map = 0%,  reduce = 0%
2016-01-04 16:55:35,316 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 4.75 sec
2016-01-04 16:55:45,775 Stage-1 map = 20%,  reduce = 7%, Cumulative CPU 71.28 sec
2016-01-04 16:55:49,968 Stage-1 map = 75%,  reduce = 7%, Cumulative CPU 104.03 sec
2016-01-04 16:55:53,131 Stage-1 map = 83%,  reduce = 7%, Cumulative CPU 117.68 sec
2016-01-04 16:55:56,297 Stage-1 map = 89%,  reduce = 7%, Cumulative CPU 131.95 sec
2016-01-04 16:55:57,363 Stage-1 map = 96%,  reduce = 8%, Cumulative CPU 136.04 sec
2016-01-04 16:55:58,457 Stage-1 map = 100%,  reduce = 12%, Cumulative CPU 141.6 sec
2016-01-04 16:56:00,553 Stage-1 map = 100%,  reduce = 25%, Cumulative CPU 143.67 sec
2016-01-04 16:56:01,597 Stage-1 map = 100%,  reduce = 66%, Cumulative CPU 156.15 sec
2016-01-04 16:56:02,664 Stage-1 map = 100%,  reduce = 74%, Cumulative CPU 159.41 sec
2016-01-04 16:56:03,714 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 163.91 sec
2016-01-04 16:56:04,764 Stage-1 map = 100%,  reduce = 92%, Cumulative CPU 174.31 sec
2016-01-04 16:56:24,673 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 208.58 sec
2016-01-04 16:56:45,639 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 239.23 sec
2016-01-04 16:56:58,235 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 252.8 sec
2016-01-04 16:57:12,835 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 269.06 sec
2016-01-04 16:57:28,482 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 286.06 sec
2016-01-04 16:57:49,711 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 307.28 sec
2016-01-04 16:58:08,491 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 325.78 sec
2016-01-04 16:58:28,262 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 347.36 sec
MapReduce Total cumulative CPU time: 5 minutes 47 seconds 360 msec
Ended Job = job_1451376659790_91438
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1451376659790_91484, Tracking URL = http://sdf-resourcemanager1:50030/proxy/application_1451376659790_91484/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1451376659790_91484
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 4
2016-01-04 16:58:36,426 Stage-2 map = 0%,  reduce = 0%
2016-01-04 16:58:46,854 Stage-2 map = 9%,  reduce = 0%, Cumulative CPU 35.54 sec
2016-01-04 16:58:49,994 Stage-2 map = 20%,  reduce = 0%, Cumulative CPU 59.12 sec
2016-01-04 16:59:01,470 Stage-2 map = 20%,  reduce = 3%, Cumulative CPU 130.38 sec
2016-01-04 16:59:03,561 Stage-2 map = 20%,  reduce = 5%, Cumulative CPU 145.98 sec
2016-01-04 16:59:04,608 Stage-2 map = 20%,  reduce = 7%, Cumulative CPU 146.51 sec
2016-01-04 16:59:08,784 Stage-2 map = 33%,  reduce = 7%, Cumulative CPU 176.87 sec
2016-01-04 16:59:11,916 Stage-2 map = 34%,  reduce = 7%, Cumulative CPU 191.02 sec
2016-01-04 16:59:15,057 Stage-2 map = 38%,  reduce = 7%, Cumulative CPU 205.83 sec
2016-01-04 16:59:17,160 Stage-2 map = 40%,  reduce = 7%, Cumulative CPU 212.32 sec
2016-01-04 16:59:18,204 Stage-2 map = 40%,  reduce = 8%, Cumulative CPU 221.2 sec
2016-01-04 16:59:19,255 Stage-2 map = 40%,  reduce = 13%, Cumulative CPU 230.3 sec
2016-01-04 16:59:29,677 Stage-2 map = 53%,  reduce = 13%, Cumulative CPU 290.94 sec
2016-01-04 16:59:30,721 Stage-2 map = 67%,  reduce = 13%, Cumulative CPU 297.97 sec
2016-01-04 16:59:32,803 Stage-2 map = 81%,  reduce = 13%, Cumulative CPU 304.69 sec
2016-01-04 16:59:35,923 Stage-2 map = 85%,  reduce = 13%, Cumulative CPU 315.55 sec
2016-01-04 16:59:39,046 Stage-2 map = 90%,  reduce = 13%, Cumulative CPU 325.54 sec
2016-01-04 16:59:40,084 Stage-2 map = 91%,  reduce = 13%, Cumulative CPU 329.53 sec
2016-01-04 16:59:42,169 Stage-2 map = 93%,  reduce = 13%, Cumulative CPU 335.76 sec
2016-01-04 16:59:43,214 Stage-2 map = 95%,  reduce = 13%, Cumulative CPU 338.9 sec
2016-01-04 16:59:45,301 Stage-2 map = 97%,  reduce = 13%, Cumulative CPU 346.05 sec
2016-01-04 16:59:46,343 Stage-2 map = 99%,  reduce = 15%, Cumulative CPU 351.4 sec
2016-01-04 16:59:47,387 Stage-2 map = 99%,  reduce = 17%, Cumulative CPU 352.09 sec
2016-01-04 16:59:48,427 Stage-2 map = 99%,  reduce = 18%, Cumulative CPU 354.56 sec
2016-01-04 16:59:49,479 Stage-2 map = 99%,  reduce = 20%, Cumulative CPU 385.0 sec
2016-01-04 16:59:50,521 Stage-2 map = 99%,  reduce = 25%, Cumulative CPU 387.5 sec
2016-01-04 16:59:51,563 Stage-2 map = 100%,  reduce = 27%, Cumulative CPU 390.69 sec
2016-01-04 16:59:52,602 Stage-2 map = 100%,  reduce = 29%, Cumulative CPU 391.83 sec
2016-01-04 16:59:53,643 Stage-2 map = 100%,  reduce = 36%, Cumulative CPU 395.15 sec
2016-01-04 16:59:54,682 Stage-2 map = 100%,  reduce = 38%, Cumulative CPU 396.88 sec
2016-01-04 16:59:55,721 Stage-2 map = 100%,  reduce = 54%, Cumulative CPU 404.14 sec
2016-01-04 16:59:56,762 Stage-2 map = 100%,  reduce = 59%, Cumulative CPU 411.33 sec
2016-01-04 16:59:58,837 Stage-2 map = 100%,  reduce = 61%, Cumulative CPU 418.63 sec
2016-01-04 16:59:59,878 Stage-2 map = 100%,  reduce = 66%, Cumulative CPU 421.67 sec
2016-01-04 17:00:01,962 Stage-2 map = 100%,  reduce = 71%, Cumulative CPU 429.43 sec
2016-01-04 17:00:03,006 Stage-2 map = 100%,  reduce = 73%, Cumulative CPU 432.61 sec
2016-01-04 17:00:04,040 Stage-2 map = 100%,  reduce = 77%, Cumulative CPU 441.54 sec
2016-01-04 17:00:08,174 Stage-2 map = 100%,  reduce = 80%, Cumulative CPU 558.5 sec
2016-01-04 17:00:11,311 Stage-2 map = 100%,  reduce = 83%, Cumulative CPU 564.0 sec
2016-01-04 17:00:14,433 Stage-2 map = 100%,  reduce = 84%, Cumulative CPU 571.0 sec
2016-01-04 17:00:16,522 Stage-2 map = 100%,  reduce = 87%, Cumulative CPU 574.72 sec
2016-01-04 17:00:17,562 Stage-2 map = 100%,  reduce = 89%, Cumulative CPU 577.79 sec
2016-01-04 17:00:19,645 Stage-2 map = 100%,  reduce = 90%, Cumulative CPU 580.85 sec
2016-01-04 17:00:20,687 Stage-2 map = 100%,  reduce = 91%, Cumulative CPU 584.27 sec
2016-01-04 17:00:22,764 Stage-2 map = 100%,  reduce = 92%, Cumulative CPU 587.45 sec
2016-01-04 17:00:23,797 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 590.51 sec
2016-01-04 17:00:25,877 Stage-2 map = 100%,  reduce = 95%, Cumulative CPU 593.55 sec
2016-01-04 17:00:26,919 Stage-2 map = 100%,  reduce = 97%, Cumulative CPU 596.69 sec
2016-01-04 17:00:27,962 Stage-2 map = 100%,  reduce = 99%, Cumulative CPU 601.66 sec
2016-01-04 17:00:33,160 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 606.27 sec
MapReduce Total cumulative CPU time: 10 minutes 6 seconds 270 msec
Ended Job = job_1451376659790_91484
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 78  Reduce: 66   Cumulative CPU: 2687.37 sec   HDFS Read: 22597238431 HDFS Write: 1653622830 SUCCESS
Stage-Stage-4: Map: 4  Reduce: 4   Cumulative CPU: 384.02 sec   HDFS Read: 1653658068 HDFS Write: 1653624858 SUCCESS
Stage-Stage-1: Map: 5  Reduce: 4   Cumulative CPU: 347.36 sec   HDFS Read: 1870075118 HDFS Write: 1927447807 SUCCESS
Stage-Stage-2: Map: 5  Reduce: 4   Cumulative CPU: 606.27 sec   HDFS Read: 1927489125 HDFS Write: 14708 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 7 minutes 5 seconds 20 msec
OK
Time taken: 475.713 seconds, Fetched: 1702 row(s)
